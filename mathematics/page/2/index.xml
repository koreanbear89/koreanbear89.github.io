<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mathematics on Lab.Koreanbear|한국곰연구소</title>
    <link>https://koreanbear89.github.io/mathematics/</link>
    <description>Recent content in Mathematics on Lab.Koreanbear|한국곰연구소</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 13 Sep 2022 09:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://koreanbear89.github.io/mathematics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>CheatSheet | Spark</title>
      <link>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-spark/</link>
      <pubDate>Fri, 11 Dec 2020 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-spark/</guid>
      <description>1. Introduction 1.1 Terminologies
Job : A piece of code that reads some input (HDFS or local), performs some computation and writes output.
Stages : Jobs are divided into stages. Stages are classified as a Map or reduce stages. Stages are divided based on computational boundaries, all computations cannot be Updated in a single Stage. It happens over many stages.
Tasks : Each stage has some tasks. One task is executed on one partition of data on one executor (machine).</description>
    </item>
    
    <item>
      <title>CheatSheet | Docker</title>
      <link>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-docker/</link>
      <pubDate>Tue, 10 Nov 2020 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-docker/</guid>
      <description>Introduction Contents
Install docker in centos
Frequently used docker commands
FIle sharing in docker (bind, volume)
Dockerfile instruction
Docker compose
Docker swarm
Glossary
swarm : almost same with word &amp;ldquo;cluster&amp;rdquo; node (manager/worker) : A unit of server in a cluster. You can run swarm commands only on the manager node. service : A unit of modules in project, a basic distribution unit, stack : You can think of it as a unit of a project, and containers grouped into one stack basically belong to the same overlay network.</description>
    </item>
    
    <item>
      <title>CheatSheet | Git</title>
      <link>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-git/</link>
      <pubDate>Tue, 29 Sep 2020 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-git/</guid>
      <description>Introduction Pull Request Summary
Fork repo to your own repo
clone, set remote
git remote add &amp;lt;remote_name&amp;gt; &amp;lt;URL&amp;gt; Generate branch
git checkout -b &amp;lt;feature/issue_number&amp;gt; Add, Commit, Push
git push &amp;lt;remote_name&amp;gt; &amp;lt;branch_name&amp;gt; Pull Request
Code Review
Merge
Squash and Merge : squash multiple commits into a new commit
Rebase and Merge : multiple commits will be merged
1. Setup Git Environment # (1) create new git $ git init # in the working dir, generates .</description>
    </item>
    
    <item>
      <title>CheatSheet | HDFS</title>
      <link>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-hdfs/</link>
      <pubDate>Tue, 03 Mar 2020 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-hdfs/</guid>
      <description>
1. HDFS $ hdfs dfs -ls $ hdfs dfs -put [LOCAL_FILE_NAME] [DEST] $ hdfs dfs -get [FILE_NAME or FOLDER_PATH] $ hdfs dfs -rm [-f] [-r|-R] [-skipTrash] URI [URI ...] </description>
    </item>
    
    <item>
      <title>Cheat Sheet | VIM</title>
      <link>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-vim/</link>
      <pubDate>Sun, 26 Jan 2020 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-vim/</guid>
      <description>1. Setup vim Install Vundle
git clone [GitHub - VundleVim/Vundle.vim: Vundle, the plug-in manager for Vim](https://github.com/VundleVim/Vundle.vim.git) ~/.vim/bundle/Vundle.vim color scheme setup
Jellybean color scheme official install
And simply add line color jellybean in .vimrc
mkdir -p ~/.vim/colors cd ~/.vim/colors curl -O https://raw.githubusercontent.com/nanotech/jellybeans.vim/master/colors/jellybeans.vim write .vimrc
wget https://raw.githubusercontent.com/jjeaby/jscript/master/.vimrc
Added Plugin &#39;preservim/nerdcommenter&#39;
install plugins in vimrc
:PluginInstall Shell
# install Vundle git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim # setup jellybean color scheme mkdir -p ~/.vim/colors cd ~/.</description>
    </item>
    
    <item>
      <title>Linear Algebra for ML #5 | Singular Value Decomposition</title>
      <link>https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec5/</link>
      <pubDate>Mon, 29 Jul 2019 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec5/</guid>
      <description>5.1 Singular Value decomposition (SVD) singular value decomposition (SVD) : is a factorization of a real or complex matrix that generalizes the eigendecomposition (EVD) of a square normal matrix to any $m \times n$ matrix via an extension of the polar decomposition. $$ A = U \Sigma V^T$$ $A \in \mathbb{R}^{m \times n}$ : A given rectangular matrix $U \in \mathbb{R}^{m\times m} $ : matrices with orthonormal columns, providing an</description>
    </item>
    
    <item>
      <title>Linear Algebra for ML #4 | Eigen Decomposition </title>
      <link>https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec4/</link>
      <pubDate>Sun, 07 Jul 2019 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec4/</guid>
      <description>4.0 Introduction Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource. The above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$ $D$ is a diagonal matrix with eigenvalues in diagonal entries $V$ is a matrix whose column vectors</description>
    </item>
    
    <item>
      <title>Linear Algebra for ML #3 | Least Square </title>
      <link>https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec3/</link>
      <pubDate>Mon, 01 Jul 2019 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec3/</guid>
      <description>3.0 Least Square Inner Product : Given $ \mathbf{u,v} \in \mathbb{R}^n$, we can consider $ \mathbf{u,v} $ as $n \times 1$ matrices. The number $\mathbf{u^Tv}$ is called inner product or dot product, and it is written as $ \mathbf{u \cdot v} $. Vector Norm : The length or magnitude of $\mathbf{v}$, can be calculated as $ \sqrt{ \mathbf{v} \cdot \mathbf{v} }$ $L_p$ Norm : $ \Vert \mathbf{x} \Vert_p = (</description>
    </item>
    
    <item>
      <title>Linear Algebra for ML #2 | Linear System &amp; Linear Transform </title>
      <link>https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec2/</link>
      <pubDate>Wed, 08 May 2019 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec2/</guid>
      <description>2.1 Linear Equation and Linear System Linear Equation is an equation that can be written in the form $$ a_1x_1 + &amp;hellip;. a_nx_n = b $$ The above equation can be written as $ \textbf{a}^T \textbf{x} = b $. Linear System is a collection of one or more linear equations Identity Matrix : $I$ is a square matrix whose diagonal entries are all 1&amp;rsquo;s and all the other entries are</description>
    </item>
    
    <item>
      <title>Linear Algebra for ML #1 | Introductions </title>
      <link>https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec1/</link>
      <pubDate>Tue, 07 May 2019 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec1/</guid>
      <description>1.1 Scalars, Vectors, Matrices, and Tensors Scalars : just a single number, italics in this book, lower-case variable name, such as $x$ (loswercase)
Vectors : an array of numbers, in order, bold lower case name, such as $\bf x$ (bold lowercase)
Matrices : 2-D array of numbers, with two indices, bold uppercase variable name, such as $A$ (uppercase)
Tensors : an array of numbers arranged on a regular grid with a variable number of axes.</description>
    </item>
    
    <item>
      <title>Statistics #6 | Models, Statistical Inference and Learning </title>
      <link>https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch6/</link>
      <pubDate>Sat, 02 Feb 2019 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch6/</guid>
      <description>6.1 Introduction Statistical Inference, or &amp;ldquo;learning&amp;rdquo; as it is called in computer science, is the process of using data to infer distribution that generated the data. 6.2 Parametric and Nonparametric Models A statistical model $\Im$ is a set of distributions (or densities or regression functions)
Parametric model : is a set of $\Im$ that can be parameterized by a finite number of parameters
If we assume that the data come from a Normal distribution, then It would be two-prarmeter model.</description>
    </item>
    
    <item>
      <title>Statistics #5 | Convergence of Random Variable </title>
      <link>https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch5/</link>
      <pubDate>Wed, 30 Jan 2019 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch5/</guid>
      <description>5.1 Introduction The law of large numbers says that the sample average converges in proability to the expectation $ \mu = \mathbb{E}(X_i)$
The central limit theorem says that $ \sqrt{n} (\overline{X - \mu})$ converges in distribution to a Normal distribution.
5.2 Types of Convergence $X_n$ converges to $X$ in probability, written $X_n \xrightarrow{P}{} X$ if for every $\epsilon &amp;gt; 0$ $$ \mathbb{P}(|X_n - X| &amp;gt; \epsilon) \rightarrow 0$$
$X_n$ converges to $X$ in distribution, written $X_n \rightsquigarrow X$ if $$ \lim_{n\rightarrow \infty} F_n(t) = F(t) $$</description>
    </item>
    
    <item>
      <title>Statistics #3 | Expectation </title>
      <link>https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch3/</link>
      <pubDate>Mon, 28 Jan 2019 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch3/</guid>
      <description>3.1 Expectation of a Random Variable The expected value, or mean, or first moment of $X$ is defined to be $$ \mathbb{E}(X) = \int xdF(x) = \begin{cases} \sum_x xf(x) &amp;amp; (\text{if X is discrete})\ \int xf(x)dx &amp;amp; ( \text{if X is continuous}) \end{cases}$$
3.3 Variance and Covariance The variance measures the spread of a distribution $$ \sigma^2 = \mathbb{E}(X-\mu)^2 = \int (x-\mu)^2 dF(x) $$
The covariance and correlation between $X$ and $Y$ measure how strong the linear relationship is between $X$ and $Y$ $$\text{Cov}(X,Y) = \mathbb{E}((X-\mu_X)(Y- \mu_Y ))$$</description>
    </item>
    
    <item>
      <title>Statistics #2 | Random Variables </title>
      <link>https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch2/</link>
      <pubDate>Sun, 20 Jan 2019 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch2/</guid>
      <description>2.1 Introduction How do we link sample spaces and events to data? Random Variable!
A random variable is a mapping $ X : \Omega \rightarrow \mathbb{R} $ that assigns a real number $X(\omega)$ to each outcome $\omega$.
2.2 Distribuition Functions and Probability Functions Cumulative Distribution Function (CDF) : is the function $F_x : \mathbb{R} \rightarrow [0,1] $ defined by $$F_X(x) = \mathbb{P}(X \leq x)$$
Probability Mass Function : A Random Variable $X$ is discrete if it takes countably many values ${x_1, x_2, &amp;hellip;}$.</description>
    </item>
    
    <item>
      <title>Statistics #1 | Probability </title>
      <link>https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch1/</link>
      <pubDate>Wed, 16 Jan 2019 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch1/</guid>
      <description>1.1 Introduction Probability is a mathematical language for quantifying uncertatinty 1.2 Sample Spaces and Events Sample Space $\Omega$ : is the set of possible outcomes of an experiment Events : Subsets of Ω are called Events 1.3 Probability A function $\mathbb{P}$ that assigns a real number $ \mathbb{P}(A) $ to every event $A$ is a probability distribution or a probability measure. 1.4 Probability on Finite Sample Spaces If $\Omega$ is</description>
    </item>
    
    <item>
      <title>CheatSheet | Linux</title>
      <link>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-linux/</link>
      <pubDate>Thu, 03 Mar 2016 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-linux/</guid>
      <description>chmod $ chmod -R [777] FOLDER_NAME conda # show the list of envs $ conda info --envs # Create conda ENV using specific python version $ conda create --name myenv python=3.5 # Create conda ENV using the existing ENV $ conda create --name myclone --clone myenv # Remove activated #!/usr/bin/env $ conda remove --name myenv --all # Backup conda env $ conda env export &amp;gt; [filename].yaml # create conda env from yaml file $ conda env create -f [filename].</description>
    </item>
    
    <item>
      <title>Python | Snippet</title>
      <link>https://koreanbear89.github.io/engineering/2.-languages/python-snippet/</link>
      <pubDate>Sat, 02 Jan 2016 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/engineering/2.-languages/python-snippet/</guid>
      <description>argparse import argparse if __name__ == &amp;quot;__main__&amp;quot;: parser = argparse.ArgumentParser() parser.add_argument(&amp;quot;--name&amp;quot;, type=str, required=True, help=&amp;quot;help&amp;quot;) args = parser.parse_args() print(args.name) counter from collections import Counter Counter([&#39;apple&#39;,&#39;red&#39;,&#39;apple&#39;,&#39;red&#39;,&#39;red&#39;,&#39;pear&#39;]) &amp;gt;&amp;gt;&amp;gt; Counter({&#39;red&#39;: 3, &#39;apple&#39;: 2, &#39;pear&#39;: 1}) datetime from datetime import datetime datetime.today().strftime(&amp;quot;%Y%m%d%H%M%S&amp;quot;) # YYYYmmddHHMMSS 형태의 시간 출력 flask from flask import jsonify, make_response @application.route(&#39;/inference&#39;, methods=[&amp;quot;GET&amp;quot;]) def infer(): summary = {&#39;class&#39; : &#39;cat&#39;, &#39;score&#39;:&#39;0.92&#39;} # make response data res = make_response(jsonify(summary), 200)</description>
    </item>
    
    <item>
      <title>LaTeX | Snippet</title>
      <link>https://koreanbear89.github.io/engineering/2.-languages/latex-snippet/</link>
      <pubDate>Fri, 01 Jan 2016 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/engineering/2.-languages/latex-snippet/</guid>
      <description>1. Matrix $$ \begin{pmatrix}1 &amp;amp; 2 &amp;amp; 3 \4 &amp;amp; 5 &amp;amp; 6 \7 &amp;amp; 8 &amp;amp; 9 \end{pmatrix} \begin{bmatrix}a &amp;amp; b &amp;amp; c \d &amp;amp; e &amp;amp; f \g &amp;amp; h &amp;amp; i \end{bmatrix} \begin{pmatrix} a_{1,1} &amp;amp; a_{1,2} &amp;amp; \cdots &amp;amp; a_{1,n} \ a_{2,1} &amp;amp; a_{2,2} &amp;amp; \cdots &amp;amp; a_{2,n} \ \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \ a_{m,1} &amp;amp; a_{m,2} &amp;amp; \cdots &amp;amp; a_{m,n} \end{pmatrix} $$</description>
    </item>
    
    <item>
      <title></title>
      <link>https://koreanbear89.github.io/engineering/9.-others/set-up-mac/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/engineering/9.-others/set-up-mac/</guid>
      <description>title: &amp;quot;Cheat Sheet | Mac OS Setup&amp;quot; date: 2022-05-22 09:00:13 categories: [2. Linux, Favorites] Introduction 어플리케이션 별 단축키 설정 시스템 환경설정 &amp;gt; 키보드 &amp;gt; 단축키 &amp;gt; 앱단축키 Automount 해제 diskutil info -all 로 Volume UUID 와 type 확인 sudo vi /etc/fstab 맨 아래에 아래와</description>
    </item>
    
    <item>
      <title></title>
      <link>https://koreanbear89.github.io/research/2.-machine-learning/ml11-multimodal-representation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/2.-machine-learning/ml11-multimodal-representation/</guid>
      <description>title: &amp;quot;MLCV #13 | Multimodal Representation&amp;quot; date: 2021-12-07 09:00:13 categories: [2. Machine Learning] Learing Transferable Visual Models, CLIP (Contrastive Language Image Pretraining Introduction Traditional CV-DL models are trained to predict a fixed set of pre-determined object categories =&amp;gt; limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much</description>
    </item>
    
  </channel>
</rss>
