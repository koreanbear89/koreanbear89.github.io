<!DOCTYPE html>
<html lang="en">
  <head>
    <title>
        Mathematcs for ML #10 | Dimensionality Reduction with PCA - Lab.Koreanbear|한국곰연구소
      </title>
        <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
    <meta name="renderer" content="webkit">
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    
    <meta name="theme-color" content="#000000" />
    
    <meta http-equiv="window-target" content="_top" />
    
    
    <meta name="description" content="10. Dimensionality Reduction with PCA Introduction : Working directly with high-dimensional data comes with some difficulties It is hard to analyze, interpretation is difficult, visualization is nearly impossible, and storage of the data vectors can be expensive In this chapter, we derive PCA from first principles, drawing on our understanding of basis and basis change (Sections 2.6.1 and 2.7.2), projections (Section 3.8), eigenvalues (Section 4.2), Gaussian distributions (Section 6.5), and constrained optimization (Section 7." />
    <meta name="generator" content="Hugo 0.101.0 with theme pure" />
    <title>Mathematcs for ML #10 | Dimensionality Reduction with PCA - Lab.Koreanbear|한국곰연구소</title>
    
    
    <link rel="stylesheet" href="https://koreanbear89.github.io/css/style.min.be627ecd35738958a04c60fe5c31d5410949a5e53a29e404dda965a1eb8160c8.css">
    
    <link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/9.15.10/styles/github.min.css" async>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css" async>
    <meta property="og:title" content="Mathematcs for ML #10 | Dimensionality Reduction with PCA" />
<meta property="og:description" content="10. Dimensionality Reduction with PCA Introduction : Working directly with high-dimensional data comes with some difficulties It is hard to analyze, interpretation is difficult, visualization is nearly impossible, and storage of the data vectors can be expensive In this chapter, we derive PCA from first principles, drawing on our understanding of basis and basis change (Sections 2.6.1 and 2.7.2), projections (Section 3.8), eigenvalues (Section 4.2), Gaussian distributions (Section 6.5), and constrained optimization (Section 7." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml10-dimensionality-reduction-and-pca/" /><meta property="article:section" content="Mathematics" />
<meta property="article:published_time" content="2022-10-13T09:00:00+00:00" />
<meta property="article:modified_time" content="2022-10-13T09:00:00+00:00" />

<meta itemprop="name" content="Mathematcs for ML #10 | Dimensionality Reduction with PCA">
<meta itemprop="description" content="10. Dimensionality Reduction with PCA Introduction : Working directly with high-dimensional data comes with some difficulties It is hard to analyze, interpretation is difficult, visualization is nearly impossible, and storage of the data vectors can be expensive In this chapter, we derive PCA from first principles, drawing on our understanding of basis and basis change (Sections 2.6.1 and 2.7.2), projections (Section 3.8), eigenvalues (Section 4.2), Gaussian distributions (Section 6.5), and constrained optimization (Section 7."><meta itemprop="datePublished" content="2022-10-13T09:00:00+00:00" />
<meta itemprop="dateModified" content="2022-10-13T09:00:00+00:00" />
<meta itemprop="wordCount" content="1638">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Mathematcs for ML #10 | Dimensionality Reduction with PCA"/>
<meta name="twitter:description" content="10. Dimensionality Reduction with PCA Introduction : Working directly with high-dimensional data comes with some difficulties It is hard to analyze, interpretation is difficult, visualization is nearly impossible, and storage of the data vectors can be expensive In this chapter, we derive PCA from first principles, drawing on our understanding of basis and basis change (Sections 2.6.1 and 2.7.2), projections (Section 3.8), eigenvalues (Section 4.2), Gaussian distributions (Section 6.5), and constrained optimization (Section 7."/>

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    
    
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" rel="stylesheet">

    
    <!--[if lte IE 9]>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
      <![endif]-->

    <!--[if lt IE 9]>
        <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
      <![endif]-->



  </head>

  
  

  <body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader">
    <div class="slimContent">
      <div class="navbar-header">
        <div class="profile-block text-center">
          
           
          
          <a id="avatar" href="/" target="_self">
            <img class=" img-rotate" src="https://koreanbear89.github.io/fa-igloo.png" width="200" height="200">
          </a>
          <a href="/" target="_self">
            <h2 id="name" class="hidden-xs hidden-sm">Lab.Koreanbear</h2>
          </a>
          
          <h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3>
          <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i>Seoul, Korea</small>
        </div><div class="search" id="search-form-wrap">
    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i
                        class="icon icon-search"></i></button>
            </span>
        </div>
        <div class="ins-search">
            <div class="ins-search-mask"></div>
            <div class="ins-search-container">
                <div class="ins-input-wrapper">
                    <input type="text" class="ins-search-input" placeholder="Type something..."
                        x-webkit-speech />
                    <button type="button" class="close ins-close ins-selectable" data-dismiss="modal"
                        aria-label="Close"><span aria-hidden="true">×</span></button>
                </div>
                <div class="ins-section-wrapper">
                    <div class="ins-section-container"></div>
                </div>
            </div>
        </div>
    </form>
</div>
        <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>


      <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
        <ul class="nav navbar-nav main-nav">
            <li class="menu-item menu-item-home">
                <a href="/">

                    
                    

                    
                    <i class=" fas fa-home"></i>
                  <span class="menu-title">Home</span>
                </a>
            </li>
            <li class="menu-item menu-item-about">
                <a href="/about/">

                    
                    

                    
                    <i class=" fas fa-user-alt"></i>
                  <span class="menu-title">About</span>
                </a>
            </li>
            <li class="menu-item menu-item-mathematics">
                <a href="/mathematics/">

                    
                    

                    
                    <i class=" fas fa-square-root-alt"></i>
                  <span class="menu-title">Mathematics</span>
                </a>
            </li>
            <li class="menu-item menu-item-research">
                <a href="/research/">

                    
                    

                    
                    <i class=" fas fa-book-open"></i>
                  <span class="menu-title">Research</span>
                </a>
            </li>
            <li class="menu-item menu-item-engineering">
                <a href="/engineering/">

                    
                    

                    
                    <i class=" fas fa-cog"></i>
                  <span class="menu-title">Engineering</span>
                </a>
            </li>
        </ul>
      </nav>
    </div>
  </header>

    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    
    <div class="widget-body">



        <ul style="margin-bottom:25px;"  class="category-list"><h5>Mathematics</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-linear-algebra">1. Linear Algebra</a>
                  <span class="category-list-count">6</span></li>
                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-statistics">2. Statistics</a>
                  <span class="category-list-count">11</span></li>
                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-mathematics-for-ml">3. Mathematics for ML</a>
                  <span class="category-list-count">12</span></li>
                  
              
        </ul><hr>



        <ul style="margin-bottom:25px;"  class="category-list"><h5>Research</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-computer-science">1. Computer Science</a>
                  <span class="category-list-count">8</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-machine-learning">2. Machine Learning</a>
                  <span class="category-list-count">12</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-computer-vision">3. Computer Vision</a>
                  <span class="category-list-count">13</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/4.-image-processing">4. Image Processing</a>
                  <span class="category-list-count">4</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/5.-natural-language">5. Natural Language</a>
                  <span class="category-list-count">6</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/6.-recommendation-system">6. Recommendation System</a>
                  <span class="category-list-count">2</span></li>

                  
              
        </ul><hr>



        <ul style="margin-bottom:25px;"  class="category-list"><h5>Engineering</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-cheatsheets">1. CheatSheets</a>
                  <span class="category-list-count">13</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-languages">2. Languages</a>
                  <span class="category-list-count">10</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-system-design">3. System Design</a>
                  <span class="category-list-count">5</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/9.-others">9. Others</a>
                  <span class="category-list-count">9</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/9.-study">9. Study</a>
                  <span class="category-list-count">6</span></li>

                  
              
        </ul><hr>









    </div>
</div>

  </div>
</aside>

    
    


  
  <div class="sidebar-toc-all">
  <aside class="sidebar sidebar-toc show" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  
    <div class="slimContent">
      <h4 class="toc-title">Contents</h4>
      <nav id="toc" class="js-toc toc" >
      </nav>
    </div>
  </aside>
</div>

<main class="main" role="main"><div class="content">
  <article id="-" class="article article-type-" itemscope
    itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      <h1 itemprop="name">
  <a
    class="article-title"
    href="/mathematics/3.-mathematics-for-ml/mml10-dimensionality-reduction-and-pca/"
    >Mathematcs for ML #10 | Dimensionality Reduction with PCA</a
  >
</h1>

      <div class="article-meta">
        
<span class="article-date">
  <i class="icon icon-calendar-check"></i>&nbsp;
<a href="https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml10-dimensionality-reduction-and-pca/" class="article-date">
  <time datetime="2022-10-13 09:00:00 &#43;0000 UTC" itemprop="datePublished">2022-10-13</time>
</a>
</span>
<span class="article-category">
  <i class="icon icon-folder"></i>&nbsp;
  <a href="/categories/3.-mathematics-for-ml/"> 3. Mathematics for ML </a>
</span>


        <span class="post-comment"><i class="icon icon-comment"></i>&nbsp;<a href="/mathematics/3.-mathematics-for-ml/mml10-dimensionality-reduction-and-pca/#comments"
            class="article-comment-link">Comments</a></span>
      </div>
    </div>
    <div class="article-entry marked-body js-toc-content" itemprop="articleBody">
      <br>
<h2 id="10-dimensionality-reduction-with-pca">10. Dimensionality Reduction with PCA</h2>
<ul>
<li><strong>Introduction</strong> : Working directly with high-dimensional data comes with some difficulties
<ul>
<li>It is hard to analyze,</li>
<li>interpretation is difficult,</li>
<li>visualization is nearly impossible,</li>
<li>and storage of the data vectors can be expensive</li>
</ul>
</li>
<li><strong>In this chapter</strong>, we derive PCA from first principles, drawing on our understanding of
<ul>
<li>basis and basis change (Sections 2.6.1 and 2.7.2),</li>
<li>projections (Section 3.8),</li>
<li>eigenvalues (Section 4.2),</li>
<li>Gaussian distributions (Section 6.5),</li>
<li>and constrained optimization (Section 7.2)</li>
</ul>
</li>
</ul>
<br>
<hr>
<h3 id="101-problem-setting">10.1 Problem Setting</h3>
<ul>
<li>
<p><strong>Objective</strong> : is to find projections $\hat{x_n}$ of data points $x_n$ that are as similar to the original data points as possible, but which have a significantly lower intrinsic dimensionality.</p>
</li>
<li>
<p><strong>Dataset</strong> : we consider an i.i.d. dataset $X = [x_1 , . . . , x_N], x_n ∈ \mathbb{R}^D,$ with mean 0</p>
<ul>
<li>
<p>possesses the data covariance matrix $S$ (6.42)</p>
</li>
<li>
<p>a low-dimensional compressesd representation $z_n$ with the projection matrix $B$</p>
</li>
</ul>
</li>
</ul>
<p>$$
S = \frac{1}{N} \sum{x_n}{x_n^⊤}
$$</p>
<p>$$
z_n = B^⊤x_n ∈ \mathbb{R}^M
$$</p>
<p>$$
B := [b_1,&hellip;,b_M] ∈ \mathbb{R}^{D×M}
$$</p>
<ul>
<li>
<p><strong>Overview</strong></p>
<ul>
<li>
<p>We assume that the columns of $B$ are orthonormal (Definition 3.7) so that $b^⊤_i b_j =0 $ if and only if $i \neq j$ and $b^⊤_i b_i =1$.</p>
</li>
<li>
<p>We seek an M-dimensional subspace $U ⊆ \mathbb{R}^D,$ $dim(U)=M &lt; D $ onto which we project the data.</p>
</li>
<li>
<p>We denote the projected data by $\hat{x}_n ∈ U,$ and their coordinates (with respect to the basis vectors $b_1,&hellip;,b_M$ of $U$ by $z_n. $</p>
</li>
<li>
<p>Our aim is to find projections $ \hat{x_n} ∈ \mathbb{R}^D $ (or equivalently the codes $z_n$ and the basis vectors $b_1, . . . , b_M $) so that they are as similar to the original data $x_n$ and minimize the loss due to compression.</p>
</li>
</ul>
</li>
</ul>
<br>
<hr>
<h3 id="102-maximum-variance-perspective">10.2 Maximum Variance Perspective</h3>
<ul>
<li><strong>Introduction :</strong>
<ul>
<li>We know that the variance is an indicator of the spread of the data, (Section 6.4.1),</li>
<li>And we can derive PCA as a dimensionality reduction algorithm that maximizes the variance in the low-dimensional representation of the data to retain as much information as possible.</li>
<li>Centered Data : the variance of the low-dimensional code does not depend on the
mean of the data (eq 10.6) Therefore, we assume without loss of generality that the
data has mean $0$.</li>
</ul>
</li>
</ul>
<br>
<h4 id="1021-direction-with-maximal-variance">10.2.1 Direction with Maximal Variance</h4>
<ul>
<li>
<p>(1) Objective : maximize the variance $V_1$ of dataset projected onto single vector $b_1$</p>
</li>
<li>
<p>(2) Variance $V_1$ : expectation of the squared deviation</p>
<ul>
<li>
<p>Deviation $z_{1n}$ : $b_1^⊤ x_n$ (section 3.8)</p>
<p>$$
V_1=\frac{1}{N} \sum z_{1n}^2 = \frac{1}{N} \sum b^⊤_1 x_n x_n^⊤ b_1 = b^⊤_1 (\frac{1}{N} \sum x_nx_n^⊤)b_1 = b^⊤_1 S b_1
$$</p>
</li>
</ul>
</li>
<li>
<p>(3) Maximize Variance :  solve the constrained optimization problem (section 7.2)</p>
<ul>
<li>by Setting the partial derivatives to 0, we see that $b_1$ should be an eigenvector of the data covariance matrix $S$</li>
</ul>
<p>$$
Sb_1 = λ_1 b_1
$$</p>
</li>
<li>
<p><strong>(4) Eigenvalue of Covariance Matrix</strong> : is same with the variance ($V_1$) of the data projected onto a 1D subspace spanned by $b_1$</p>
</li>
</ul>
<p>$$
V_1 = b^⊤_1 S b_1 = b^⊤_1 λ_1 b_1 = λ_1
$$</p>
<ul>
<li>
<p><strong>(5) Principal Component</strong> : the basis vector associated with the largest eigenvalue of the data covariance matrix</p>
<ul>
<li>Therefore, to maximize the variance, we should find the basis vector associated with the largest eigenvalue (Principal Components), and then project given data onto them.</li>
</ul>
</li>
</ul>
<br>
<h4 id="1022-m-dimensional-subspace-with-maximal-variance">10.2.2 M-dimensional Subspace with Maximal Variance</h4>
<ul>
<li>
<p>Extending to $M$dimensional subspace:</p>
<ul>
<li>The variance of the data, when projected onto an $M$ dimensional subspace, equals the sum of the eigenvalues that are associated with the corresponding eigenvectors of the data covariance matrix.</li>
</ul>
<p>$$
V_m = b_m^⊤ S b_m = λ_m b_m^⊤ b_m = λ_m .
$$</p>
</li>
</ul>
<br>
<hr>
<h3 id="103-projection-perspective">10.3 Projection Perspective</h3>
<ul>
<li>
<p>Introduction :Not something new, we will just derive PCA in another view point</p>
<ul>
<li>
<p>In the previous section, we derived PCA by maximizing the variance in the projected space to retain as much information as possible.</p>
</li>
<li>
<p>In the following, we will look at the difference vectors between the original data $x_n$ and their projection $\tilde{x}_n$ and minimize this distance so that $x_n$ and $\tilde{x}_n$ are as close as possible.</p>
</li>
</ul>
</li>
</ul>
<br>
<h4 id="1031-setting-and-objective">10.3.1 Setting and Objective</h4>
<ul>
<li>Objective : minimizing the average squared Euclidean distance between $x$ and $\tilde{x}_n$ (projection error)</li>
</ul>
<p>$$
J_M := \frac{1}{N} \sum ∥ x_n − \tilde{x}_n∥^2
$$</p>
<ul>
<li>Solution : To find the coordinates $z_n$ and the OrthoNormalBasis of the principal subspace, we follow a two-step approach
<ul>
<li>(1) we optimize the coordinates $z_n$ for a given ONB (10.3.2)</li>
<li>(2) second, we find the optimal ONB. (10.3.3)</li>
</ul>
</li>
</ul>
<br>
<h4 id="1032-finding-optimal-coordinates">10.3.2 Finding Optimal Coordinates</h4>
<ul>
<li>
<p>Introduction : Let us start by finding the optimal coordinates $z_{1n} , . . . , z_{Mn}$ of the projections $\tilde{x}_n$ for $n = 1,&hellip;,N$ that minimizes the distance between  $\tilde{x} − x$.</p>
</li>
<li>
<p>Consequence :</p>
<ul>
<li>The optimal linear projection $\tilde{x}_n$ of $x_n$ is an orthogonal projection.</li>
<li>The coordinates of $\tilde{x}_n$ with respect to the basis $(b_1,&hellip;,b_M) $ are the coordinates of the orthogonal projection of $x_n$ onto the principal subspace.</li>
<li>An orthogonal projection is the best linear mapping given the objective.</li>
</ul>
</li>
<li>
<p>Summary :</p>
<ul>
<li>
<p>So far we have shown that for a given ONB we can find the optimal coordinates by an orthogonal projection onto the principal subspace.</p>
</li>
<li>
<p>In the following, we will determine what the best basis is.</p>
</li>
</ul>
</li>
</ul>
<br>
<h4 id="1033-finding-the-basis-of-the-principal-subspace">10.3.3 Finding the Basis of the Principal Subspace</h4>
<ul>
<li>
<p>Consequence: after solving the equation from (10.35) to (10.43b), we finally get the consequences below.</p>
<ul>
<li>Minimizing the average squared reconstruction (projection) error is equivalent to minimizing the variance of the data when projected onto the subspace we ignore, i.e., the orthogonal complement of the principal subspace</li>
<li>The basis of the principal subspace comprises the eigenvectors $b_1, &hellip; b_M$ that are associated with the largest M eigenvalues of the data covariance matrix.</li>
</ul>
</li>
</ul>
<br>
<hr>
<h3 id="104-eigenvector-computation-and-low-rank-approximations">10.4 Eigenvector Computation and Low-Rank Approximations</h3>
<ul>
<li>
<p><strong>Introduction</strong></p>
<ul>
<li>
<p>In the previous sections, we obtained the basis of the principal subspace as the eigenvectors that are associated with the largest eigenvalues of the data covariance matrix </p>
</li>
<li>
<p>To get the eigenvalues (and the corresponding eigenvectors) of covmat</p>
<ul>
<li>
<p>We perform an eigendecomposition (see Section 4.2)</p>
</li>
<li>
<p>We use a singular value decomposition (see Section 4.5)</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Results</strong> : The relationship between the eigenvalues of $S$ and the singular values of $X$ provides the connection between the maximum variance view (Section 10.2) and the singular value decomposition.</p>
</li>
</ul>
<br>
<h4 id="1041-pca-using-low-rank-matrix-approximations">10.4.1. PCA using Low-Rank Matrix Approximations</h4>
<ul>
<li>
<p>Eckart-Young theorem: states that $\tilde{X}_M$, is given by truncating the SVD at the top-M singular value.</p>
<ul>
<li>Diagonal entries of diagonal matrix $\Sigma_m$ are the $M$ largest singular values of $X$</li>
</ul>
<p>$$
\tilde{X}_M = U_M \Sigma_M V^T_M
$$</p>
</li>
</ul>
<br>
<h4 id="1042-practical-aspects">10.4.2. Practical Aspects</h4>
<ul>
<li>
<p><strong>Find eigenvalues and eigenvectors</strong> :</p>
<ul>
<li>
<p>(1) Roots of the characteristic polynomial  (section 4.2) : practically impossible for large matrix.</p>
</li>
<li>
<p>(2) eigendecomposition (or SVD) : we only require a few eigenvectors, so It would be wasteful to compute the full decomposition</p>
</li>
<li>
<p>(3) Iterative methods : If we are interested in only the first few eigenvectors, then iterative processes, which directly optimize these eigenvectors, are computationally efficient</p>
<ul>
<li>Power Iteration (Google PageRank) : is very efficient In the extreme case of only needing the first eigenvector.</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>
<hr>
<h3 id="105-pca-in-high-dimensions">10.5 PCA in High Dimensions</h3>
<ul>
<li>
<p><strong>Introduction</strong> : In the following, we provide a solution to the problem for the case that we have substantially fewer data points than dimensions</p>
<ul>
<li>The number of data $N$ is smaller than the dimension of the data $D$.</li>
<li>The rank of the covmat is $N$, $S$ has $D-N+1$ eigenvalues are zero.</li>
<li>We will exploit this and turn the $D×D$ covariance matrix into an $N × N$ covariance matrix whose eigenvalues are all positive.</li>
</ul>
</li>
<li>
<p><strong>Consequences</strong> : we can compute the eigenvalues and eigenvectors much more efficiently than for the original $D \times D$ data covmat</p>
</li>
</ul>
<br>
<hr>
<h3 id="106-key-steps-of-pca-in-practice">10.6 Key Steps of PCA in Practice</h3>
<ul>
<li><strong>Introduction</strong> : n the following, we will go through the individual steps of PCA using a running example, which is summarized in Figure 10.11</li>
<li><strong>(1) Mean Subtraction</strong> : centering the data by computing the mean and subtracting it from every single data point =&gt; dataset has zero-mean</li>
<li><strong>(2) Standardization</strong> : Divide the datapoints by the standard deviation $σ_d$ of the dataset for every dimension.</li>
<li><strong>(3) Eigendecomposition of the covariance matrix</strong> : Compute the data covariance matrix and its eigenvalues and corresponding eigenvectors</li>
<li><strong>(4) Projection</strong> : We can project any data point $x_∗ ∈ R^D$ onto the principal subspace</li>
</ul>
<p style="text-align: center;">
<img class="post-fig" width="80%" align="center" src="/figures/2022-07-15-fig1.png"></p>
<center><i>Fig10.11 Steps of PCA</i></center>
<br>
<hr>
<h3 id="107-latent-variable-perspective">10.7 Latent Variable Perspective</h3>
<ul>
<li><strong>Introduction</strong> : In the previous sections, we derived PCA without any notion of a probabilistic model</li>
<li><strong>Probabilistic PCA</strong> : By introducing a continuous-valued latent variable $z ∈ R^M$ it is possible to phrase PCA as a probabilistic latent-variable model (skip details)</li>
</ul>
<br>
<hr>
<h3 id="summary">Summary</h3>
<ul>
<li>
<p>In this chapter, we will discuss principal component analysis, an algorithm for linear dimensionality reduction</p>
</li>
<li>
<p>Section 10.1, Problem Setting</p>
<ul>
<li>Our objective is to find projections $\hat{x_n}$ of data points $x_n$ that are as similar to the original data points as possible, but which have a significantly lower intrinsic dimensionality.</li>
</ul>
</li>
<li>
<p>Section 10.2, Maximum Variance Perspective</p>
<ul>
<li>
<p>We saw PCA as a way of dim-reduction that results in the largest variance after projection</p>
<ul>
<li>
<p>the variance of projected vectors are same with the eigenvalue of covariance matrix</p>
</li>
<li>
<p>So we can find the best basis vector to project by choose the vector whose eigenvalue is the largest</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Section 10.3, Projection Perspective</p>
<ul>
<li>
<p>We looked at the difference vectors between the original data $x_n$ and their projection $\tilde{x}_n$ and minimize this distance so that $x_n$ and $\tilde{x}_n$ are as close as possible.</p>
<ul>
<li>
<p>Objective : minimizing the average squared Euclidean distance between $x$ and $\tilde{x}_n$ (projection error)</p>
</li>
<li>
<p>An orthogonal projection is the best linear mapping given the objective.</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Section 10.4, Eigenvector Computation and Low-Rank Approximations</p>
<ul>
<li>We saw computational efficient ways to find Eigenvector in practical aspect.</li>
</ul>
</li>
<li>
<p>Section 10.5, PCA in high dimensions</p>
<ul>
<li>We provided a solution to the problem for the case that we have substantially fewer data points than dimensions</li>
</ul>
</li>
<li>
<p>Section 10.6, Key steps of PCA in practice</p>
<ul>
<li>
<p>(1) Mean Subtraction</p>
</li>
<li>
<p>(2) Standardization</p>
</li>
<li>
<p>(3) Eigendecomposition of the covariance matrix</p>
</li>
<li>
<p>(4) Projection</p>
</li>
</ul>
</li>
<li>
<p>section 10.7, Latent Variable Perspective</p>
<ul>
<li>We saw PCA as probabilistic latent-variable model</li>
</ul>
</li>
</ul>

    </div>
    <div class="article-footer">

    </div>
  </article>

</div><nav class="bar bar-footer clearfix" data-stick-bottom>
    <div class="bar-inner">
        <ul class="pager pull-right">
            
            
            
            
        </ul>
        <div class="bar-right">
        </div>
    </div>
</nav>


</main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
<ul class="social-links">
    <li><a href="https://github.com/koreanbear89" target="_blank" title="github" data-toggle=tooltip data-placement=top >
            <i class="icon icon-github"></i></a></li>
    <li><a href="https://www.linkedin.com/in/hanwoong-kim-95b5a7130/" target="_blank" title="linkedin" data-toggle=tooltip data-placement=top >
            <i class="icon icon-linkedin"></i></a></li>
    <li><a href="https://koreanbear89.github.io/index.xml" target="_blank" title="rss" data-toggle=tooltip data-placement=top >
            <i class="icon icon-rss"></i></a></li>
</ul>
  
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>
<script>
    window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/highlight.min.js"></script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/languages/python.min.js" defer></script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/languages/javascript.min.js" defer></script><script>
    hljs.configure({
        tabReplace: '    ', 
        classPrefix: ''     
        
    })
    hljs.initHighlightingOnLoad();
</script>
<script src="https://koreanbear89.github.io/js/application.min.c181e6b0c036798c7731cfb85b41b44c80689fd48fee546b73d449386ce6ccfb.js"></script>
<script src="https://koreanbear89.github.io/js/plugin.min.46930345227de54c034f39c80841463c3b879632feb482e9f2734d4b616ae3be.js"></script>

<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            ROOT_URL: 'https:\/\/koreanbear89.github.io\/',
            CONTENT_URL: 'https:\/\/koreanbear89.github.io\/\/searchindex.json ',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script type="text/javascript" src="https://koreanbear89.github.io/js/insight.min.07c7d73e2ec5e960e55f8b98577ada0bd78b36e7ef9d6dd65b85d4b1d443e7c1a1ca7cd1d98e7105db5fd72583cbc6fbb5e386062d3bf3c2568588fbb38c0c06.js" defer></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
<script>
    tocbot.init({
        
        tocSelector: '.js-toc',
        
        contentSelector: '.js-toc-content',
        
        headingSelector: 'h1, h2, h3',
        
        hasInnerContainers: true,
    });
</script>



  </body>
</html>
