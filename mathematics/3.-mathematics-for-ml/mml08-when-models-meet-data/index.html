<!DOCTYPE html>
<html lang="en">
  <head>
    <title>
        Mathematics for ML #8 | Introduction Part.II - Lab.Koreanbear|한국곰연구소
      </title>
        <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
    <meta name="renderer" content="webkit">
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    
    <meta name="theme-color" content="#000000" />
    
    <meta http-equiv="window-target" content="_top" />
    
    
    <meta name="description" content="8. When Models Meet Data In the first part of the book, we introduced the mathematics that form the foundations of many machine learning methods
The second part of the book introduces four pillars of machine learning:
Regression (Chapter 9) Dimensionality reduction (Chapter 10) Density estimation (Chapter 11) Classification (Chapter 12) 8.1 Data, Models, and Learning Three major components of a machine learning system: data, models, and learning. Good models : should perform well on unseen data." />
    <meta name="generator" content="Hugo 0.101.0 with theme pure" />
    <title>Mathematics for ML #8 | Introduction Part.II - Lab.Koreanbear|한국곰연구소</title>
    
    
    <link rel="stylesheet" href="https://koreanbear89.github.io/css/style.min.be627ecd35738958a04c60fe5c31d5410949a5e53a29e404dda965a1eb8160c8.css">
    
    <link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/9.15.10/styles/github.min.css" async>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css" async>
    <meta property="og:title" content="Mathematics for ML #8 | Introduction Part.II" />
<meta property="og:description" content="8. When Models Meet Data In the first part of the book, we introduced the mathematics that form the foundations of many machine learning methods
The second part of the book introduces four pillars of machine learning:
Regression (Chapter 9) Dimensionality reduction (Chapter 10) Density estimation (Chapter 11) Classification (Chapter 12) 8.1 Data, Models, and Learning Three major components of a machine learning system: data, models, and learning. Good models : should perform well on unseen data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml08-when-models-meet-data/" /><meta property="article:section" content="Mathematics" />
<meta property="article:published_time" content="2022-08-13T09:00:00+00:00" />
<meta property="article:modified_time" content="2022-08-13T09:00:00+00:00" />

<meta itemprop="name" content="Mathematics for ML #8 | Introduction Part.II">
<meta itemprop="description" content="8. When Models Meet Data In the first part of the book, we introduced the mathematics that form the foundations of many machine learning methods
The second part of the book introduces four pillars of machine learning:
Regression (Chapter 9) Dimensionality reduction (Chapter 10) Density estimation (Chapter 11) Classification (Chapter 12) 8.1 Data, Models, and Learning Three major components of a machine learning system: data, models, and learning. Good models : should perform well on unseen data."><meta itemprop="datePublished" content="2022-08-13T09:00:00+00:00" />
<meta itemprop="dateModified" content="2022-08-13T09:00:00+00:00" />
<meta itemprop="wordCount" content="1581">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Mathematics for ML #8 | Introduction Part.II"/>
<meta name="twitter:description" content="8. When Models Meet Data In the first part of the book, we introduced the mathematics that form the foundations of many machine learning methods
The second part of the book introduces four pillars of machine learning:
Regression (Chapter 9) Dimensionality reduction (Chapter 10) Density estimation (Chapter 11) Classification (Chapter 12) 8.1 Data, Models, and Learning Three major components of a machine learning system: data, models, and learning. Good models : should perform well on unseen data."/>

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    
    
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" rel="stylesheet">

    
    <!--[if lte IE 9]>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
      <![endif]-->

    <!--[if lt IE 9]>
        <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
      <![endif]-->



  </head>

  
  

  <body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader">
    <div class="slimContent">
      <div class="navbar-header">
        <div class="profile-block text-center">
          
           
          
          <a id="avatar" href="/" target="_self">
            <img class=" img-rotate" src="https://koreanbear89.github.io/fa-igloo.png" width="200" height="200">
          </a>
          <a href="/" target="_self">
            <h2 id="name" class="hidden-xs hidden-sm">Lab.Koreanbear</h2>
          </a>
          
          <h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3>
          <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i>Seoul, Korea</small>
        </div><div class="search" id="search-form-wrap">
    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i
                        class="icon icon-search"></i></button>
            </span>
        </div>
        <div class="ins-search">
            <div class="ins-search-mask"></div>
            <div class="ins-search-container">
                <div class="ins-input-wrapper">
                    <input type="text" class="ins-search-input" placeholder="Type something..."
                        x-webkit-speech />
                    <button type="button" class="close ins-close ins-selectable" data-dismiss="modal"
                        aria-label="Close"><span aria-hidden="true">×</span></button>
                </div>
                <div class="ins-section-wrapper">
                    <div class="ins-section-container"></div>
                </div>
            </div>
        </div>
    </form>
</div>
        <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>


      <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
        <ul class="nav navbar-nav main-nav">
            <li class="menu-item menu-item-home">
                <a href="/">

                    
                    

                    
                    <i class=" fas fa-home"></i>
                  <span class="menu-title">Home</span>
                </a>
            </li>
            <li class="menu-item menu-item-about">
                <a href="/about/">

                    
                    

                    
                    <i class=" fas fa-user-alt"></i>
                  <span class="menu-title">About</span>
                </a>
            </li>
            <li class="menu-item menu-item-mathematics">
                <a href="/mathematics/">

                    
                    

                    
                    <i class=" fas fa-square-root-alt"></i>
                  <span class="menu-title">Mathematics</span>
                </a>
            </li>
            <li class="menu-item menu-item-research">
                <a href="/research/">

                    
                    

                    
                    <i class=" fas fa-book-open"></i>
                  <span class="menu-title">Research</span>
                </a>
            </li>
            <li class="menu-item menu-item-engineering">
                <a href="/engineering/">

                    
                    

                    
                    <i class=" fas fa-cog"></i>
                  <span class="menu-title">Engineering</span>
                </a>
            </li>
        </ul>
      </nav>
    </div>
  </header>

    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    
    <div class="widget-body">



        <ul style="margin-bottom:25px;"  class="category-list"><h5>Mathematics</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-linear-algebra">1. Linear Algebra</a>
                  <span class="category-list-count">6</span></li>
                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-statistics">2. Statistics</a>
                  <span class="category-list-count">11</span></li>
                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-mathematics-for-ml">3. Mathematics for ML</a>
                  <span class="category-list-count">12</span></li>
                  
              
        </ul><hr>



        <ul style="margin-bottom:25px;"  class="category-list"><h5>Research</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-computer-science">1. Computer Science</a>
                  <span class="category-list-count">8</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-machine-learning">2. Machine Learning</a>
                  <span class="category-list-count">12</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-computer-vision">3. Computer Vision</a>
                  <span class="category-list-count">13</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/4.-image-processing">4. Image Processing</a>
                  <span class="category-list-count">4</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/5.-natural-language">5. Natural Language</a>
                  <span class="category-list-count">6</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/6.-recommendation-system">6. Recommendation System</a>
                  <span class="category-list-count">2</span></li>

                  
              
        </ul><hr>



        <ul style="margin-bottom:25px;"  class="category-list"><h5>Engineering</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-cheatsheets">1. CheatSheets</a>
                  <span class="category-list-count">13</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-languages">2. Languages</a>
                  <span class="category-list-count">10</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-system-design">3. System Design</a>
                  <span class="category-list-count">5</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/9.-others">9. Others</a>
                  <span class="category-list-count">9</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/9.-study">9. Study</a>
                  <span class="category-list-count">6</span></li>

                  
              
        </ul><hr>









    </div>
</div>

  </div>
</aside>

    
    


  
  <div class="sidebar-toc-all">
  <aside class="sidebar sidebar-toc show" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  
    <div class="slimContent">
      <h4 class="toc-title">Contents</h4>
      <nav id="toc" class="js-toc toc" >
      </nav>
    </div>
  </aside>
</div>

<main class="main" role="main"><div class="content">
  <article id="-" class="article article-type-" itemscope
    itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      <h1 itemprop="name">
  <a
    class="article-title"
    href="/mathematics/3.-mathematics-for-ml/mml08-when-models-meet-data/"
    >Mathematics for ML #8 | Introduction Part.II</a
  >
</h1>

      <div class="article-meta">
        
<span class="article-date">
  <i class="icon icon-calendar-check"></i>&nbsp;
<a href="https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml08-when-models-meet-data/" class="article-date">
  <time datetime="2022-08-13 09:00:00 &#43;0000 UTC" itemprop="datePublished">2022-08-13</time>
</a>
</span>
<span class="article-category">
  <i class="icon icon-folder"></i>&nbsp;
  <a href="/categories/3.-mathematics-for-ml/"> 3. Mathematics for ML </a>
</span>


        <span class="post-comment"><i class="icon icon-comment"></i>&nbsp;<a href="/mathematics/3.-mathematics-for-ml/mml08-when-models-meet-data/#comments"
            class="article-comment-link">Comments</a></span>
      </div>
    </div>
    <div class="article-entry marked-body js-toc-content" itemprop="articleBody">
      <br>
<h2 id="8-when-models-meet-data">8. When Models Meet Data</h2>
<ul>
<li>
<p>In the first part of the book, we introduced the mathematics that form the foundations of many machine learning methods</p>
</li>
<li>
<p>The second part of the book introduces four pillars of machine learning:</p>
<ul>
<li>Regression (Chapter 9)</li>
<li>Dimensionality reduction (Chapter 10)</li>
<li>Density estimation (Chapter 11)</li>
<li>Classification (Chapter 12)</li>
</ul>
</li>
</ul>
<br>
<hr>
<h3 id="81-data-models-and-learning">8.1 Data, Models, and Learning</h3>
<ul>
<li>Three major components of a machine learning system: data, models, and learning.</li>
<li>Good models : should perform well on unseen data.</li>
<li>There are two different senses in which we use the phrase “machine learning algorithm”: training and prediction.</li>
</ul>
<p><strong>8.1.1 Data as Vectors</strong></p>
<ul>
<li>We assume that a domain expert already converted data appropriately, i.e., each input $x_n$ is a D-dimensional vector of real numbers, which are called features, attributes, or covariates by following steps:
<ul>
<li>Raw Data format : In recent years, machine learning has been applied to many types of data that do not obviously come in the tabular numerical format</li>
<li>Numerical Representation : Even when we have data in tabular format, there are still choices to be made to obtain a numerical representation.</li>
<li>Normalization : Even numerical data that could potentially be directly read into a machine learning algorithm should be carefully considered for units, scaling, and constraint</li>
</ul>
</li>
</ul>
<p><strong>8.1.2 Models as Functions</strong></p>
<ul>
<li>A predictor is a function that, when given a particular input example (in our case, a vector of features), produces an output</li>
</ul>
<p><strong>8.1.3 Models as Probability Distributions</strong></p>
<ul>
<li>Instead of considering a predictor as a single function, we could consider predictors to be probabilistic models,</li>
</ul>
<p><strong>8.1.4 Learning is Finding Parameters</strong></p>
<ul>
<li>The goal of learning is to find a model and its corresponding parameters such that the resulting predictor will perform well on unseen data.</li>
<li>we will consider two schools of machine learning in this book, corresponding to whether the predictor is a function or a probabilistic model.</li>
<li>We would like to find good predictors for given training data, with two main strategies :
<ul>
<li>based on some measure of quality (sometimes called finding a point estimate),</li>
<li>using Bayesian inference. (requires probabilistic models)</li>
</ul>
</li>
</ul>
<br>
<hr>
<h3 id="82-empirical-risk-minimization">8.2 Empirical Risk Minimization</h3>
<ul>
<li>In this section, we consider the case of a predictor that is a function</li>
</ul>
<p><strong>8.2.1 Hypothesis Class of Functions</strong></p>
<ul>
<li>
<p>For given data, we would like to estimate a predictor $f(·,θ):RD →R$, parametrized by $θ$. We hope to be able to find a good parameter $θ^∗$ such that we fit the data well, that is,</p>
<p>$$
f(x_n,θ^∗) ≈ y_n \text{ for all } n = 1,&hellip;,N.
$$</p>
<ul>
<li>Affine functions (linear functions) are used as predictors for conventional ML methods</li>
<li>Instead of a linear function, we may wish to consider non-linear functions as predictors.</li>
</ul>
</li>
<li>
<p>Given the class of functions, we want to search for a good predictor. We now move on to the second ingredient of empirical risk minimization: how to measure how well the predictor fits the training data.</p>
</li>
</ul>
<p><strong>8.2.2 Loss Function for Training</strong></p>
<ul>
<li>Equation below is called the empirical risk and depends on three arguments, the predictor $f$ and the data $X, y$.</li>
</ul>
<p>$$
R_{emp}(f,X,y) = \frac{1}{N} \sum_{n}^{N} l(y_n, \hat{y}_n)
$$</p>
<ul>
<li>Empirical risk minimization : This general strategy for learning is called empirical risk minimization</li>
</ul>
<p><strong>8.2.3 Regularization to Reduce Overfitting</strong></p>
<ul>
<li>The aim of training a ML predictor is so that we can perform well on unseen data</li>
<li>Overfitting : the predictor fits too closely to the training data and does not generalize well to new data</li>
<li>Regularization : introduce a penalty term that makes it harder for the optimizer to return an overly flexible predictor</li>
</ul>
<p>$$
\text{min}_θ \frac{1}{N}∥y − Xθ∥^2  \rightarrow  \text{min}_θ \frac{1}{N}∥y − Xθ∥^2 + λ∥θ∥^2
$$</p>
<p><strong>8.2.4 Cross-Validation to Assess the Generalization Performance</strong></p>
<ul>
<li>partitions the data into K chunks, K-1 of which form the trainig set, and the last chunk serves as the validation set.</li>
<li>We cycle through all possible partitionings of validation and training sets and compute the average generalization error of the predictor</li>
</ul>
<br>
<hr>
<h3 id="83-parameter-estimation">8.3 Parameter Estimation</h3>
<ul>
<li>In this section, we will see how to use probability distributions to model our uncertainty due to the observation process and our uncertainty in the parameters of our predictors.</li>
</ul>
<br>
<p><strong>8.3.1 Maximum Likelihood Estimation</strong></p>
<ul>
<li>
<p>The idea behind MLE is to define a function of the parameters that enables us to find a model that fits the data well.</p>
</li>
<li>
<p>Negative log-likelihood for data represented by a random variable $x$ and a family of probability densities $p(x|θ)$ is given by :</p>
<ul>
<li>The notation $L_x(θ)$ emphasizes the fact that the parameter $θ$ is varying and the data $x$ is fixed.</li>
<li>to find a good parameter vector $θ$ that explains the data well, minimize the negative log-likelihood $L(θ)$ with respect to $θ$.</li>
</ul>
<p>$$
L_x(\theta) = −logp(x|θ)
$$</p>
</li>
<li>
<p>Minimizing NLL $L(θ)$ of Gaussian, corresponds to solving the least-squares problem (example 8.5)</p>
</li>
</ul>
<blockquote>
<p>Likelihood detail in section 6.3</p>
</blockquote>
<br>
<p><strong>8.3.2 Maximum A Posteriori Estimaion</strong></p>
<ul>
<li>If we have prior knowledge about the distribution of the parameters θ, we can multiply an additional term to the likelihood.
<ul>
<li>using Bayes&rsquo; theorem (6.3) we can compute a posterior distbution $p(θ|x)$, with prior distribution $p(\theta)$, and likelihood $p(x|θ)$</li>
</ul>
</li>
</ul>
<p>$$
p(θ|x) = p(x|θ)p(θ) / p(x)
$$</p>
<br>
<p><strong>8.3.3 Model Fitting</strong></p>
<ul>
<li>Overfitting : refers to the situation where the parametrized model class is too rich to model the dataset</li>
<li>Underfitting : we encounter the opposite problem where the model class is not rich enough</li>
</ul>
<br>
<hr>
<h3 id="84-probabilistic-modeling-and-inference">8.4 Probabilistic Modeling and Inference</h3>
<ul>
<li>We often build models that describe the generative process that generates the observed data.
<ul>
<li>A coin-flip experiment can be described in two steps,
<ul>
<li>(1), we define a parameter μ, which describes the probability of “heads” as the parameter of a Bernoulli distribution</li>
<li>(2) we can sample an outcome x ∈ {head, tail} from the Bernoulli distribution $p(x | μ) = Ber(μ)$</li>
</ul>
</li>
</ul>
</li>
<li>In this chapter, we will discuss how probabilistic modeling can be used for this purpose.</li>
</ul>
<br>
<p><strong>8.4.1 Probabilistic Models</strong></p>
<ul>
<li>Probabilistic models represent the uncertain aspects of an experiment as probability distributions.</li>
<li>Only the joint distribution has this property. Therefore, a probabilistic model is specified by the joint distribution of all its random variables
<ul>
<li>The prior and the likelihood (product rule, Section 6.3)</li>
<li>The marginal likelihood p(x), which will play an important role in model selection (Section 8.6), can be computed by taking the joint distribution and integrating out the parameters (sum rule, Section 6.3)</li>
<li>The posterior, which can be obtained by dividing the joint by the marginal likelihood.</li>
</ul>
</li>
</ul>
<br>
<p><strong>8.4.2 Bayesian Inference</strong></p>
<ul>
<li>
<p>In Section 8.3.1, we already discussed two ways for estimating model parameters θ using maximum likelihood or maximum a posteriori estimation.</p>
</li>
<li>
<p>Once these point estimates θ* are known, we use them to make predictions. More specifically, the predictive distribution will be p(x | θ*), where we use θ* in the likelihood function.</p>
</li>
<li>
<p>Having the full posterior distribution around can be extremely useful and leads to more robust decisions.</p>
<p>=&gt; Bayesian inference is about finding this posterior distribution</p>
</li>
<li>
<p>The key idea is to exploit Bayes’ theorem to invert the relationship between the parameters θ and the data X (given by the likelihood) to obtain the posterior distribution p(θ | X ).</p>
</li>
</ul>
<br>
<p><strong>8.4.3 Latent Variable Models</strong></p>
<ul>
<li>In practice, it is sometimes useful to have additional latent variables z (besides the model parameters θ) as part of the model.</li>
<li>Latent variables may describe the data-generating process, thereby contributing to the interpretability of the model.</li>
<li>The conditional distribution $p(x | θ,z)$ allows us to generate data for any model parameters and latent variables.</li>
</ul>
<br>
<hr>
<h3 id="85-directed-graphical-models">8.5 Directed Graphical Models</h3>
<ul>
<li>In this section, we introduce a graphical language for specifying a probabilistic model, called the directed graphical model
<ul>
<li>provides a compact and succinct way to specify probabilistic models,</li>
<li>allows the reader to visually parse dependencies between random variables</li>
</ul>
</li>
<li>This section relies on the concepts of independence and conditional independence, as described in Section 6.4.5.</li>
<li>Probabilistic graphical models have some convenient properties:
<ul>
<li>They are a simple way to visualize the structure of a probabilistic model.</li>
<li>They can be used to design or motivate new kinds of statistical models.</li>
<li>Inspection of the graph alone gives us insight into properties, e.g., conditional independence.</li>
<li>Complex computations for inference and learning in statistical models can be expressed in terms of graphical manipulations.</li>
</ul>
</li>
</ul>
<br>
<hr>
<h3 id="86-model-selection">8.6 Model Selection</h3>
<ul>
<li>In machine learning, we often need to make high-level modeling decisions that critically influence the performance of the model</li>
<li>More complex models are more flexible in the sense that they can be used to describe more datasets
<ul>
<li>A polynomial of degree 1 (a line) can only be used to describe linear relations between inputs x and observations y.</li>
<li>A polynomial of degree 2 can additionally describe quadratic relationships between inputs and observations.</li>
</ul>
</li>
<li>However, we also need some mechanisms for assessing how a model generalizes to unseen test data. =&gt; Model selection is concerned with exactly this problem.</li>
</ul>
<br>
<hr>
<h3 id="summary">Summary</h3>
<ul>
<li>In this chapter, We will see how ML algorithms work from a mathematical point of view.</li>
<li>Section 8.1, Data, Model, Learning
<ul>
<li>We saw the three main components of ML algorithm : Data, Model, Learning</li>
</ul>
</li>
<li>Section 8.2, Empirical Risk Minimization
<ul>
<li>We learned about the perspective of seeing ML model as a functional optimization
<ul>
<li>Loss Function (ex. MSE =&gt; Least Square Problem)</li>
<li>Regularization, additional penalty term in loss function</li>
</ul>
</li>
</ul>
</li>
<li>Section 8.3, Parameter Estimation
<ul>
<li>We learned about the parameter estimation of data distribution in terms of ML model training.
<ul>
<li>Likelihood (loss function)</li>
<li>prior (regularization term)</li>
</ul>
</li>
</ul>
</li>
<li>Section 8.4 Probabilistic Modeling and Inference
<ul>
<li>We learned about the Bayesian inference in terms of ML Model inference</li>
</ul>
</li>
<li>Section 8.5 Directed Graphical Models
<ul>
<li>Directed Graphical Models can be used to specify a probabilistic model.</li>
</ul>
</li>
<li>Section 8.6 Model Selection
<ul>
<li>How can we choose the model that works best for unseen data among trained models.</li>
</ul>
</li>
</ul>
<br>

    </div>
    <div class="article-footer">

    </div>
  </article>

</div><nav class="bar bar-footer clearfix" data-stick-bottom>
    <div class="bar-inner">
        <ul class="pager pull-right">
            
            
            
            
        </ul>
        <div class="bar-right">
        </div>
    </div>
</nav>


</main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
<ul class="social-links">
    <li><a href="https://github.com/koreanbear89" target="_blank" title="github" data-toggle=tooltip data-placement=top >
            <i class="icon icon-github"></i></a></li>
    <li><a href="https://www.linkedin.com/in/hanwoong-kim-95b5a7130/" target="_blank" title="linkedin" data-toggle=tooltip data-placement=top >
            <i class="icon icon-linkedin"></i></a></li>
    <li><a href="https://koreanbear89.github.io/index.xml" target="_blank" title="rss" data-toggle=tooltip data-placement=top >
            <i class="icon icon-rss"></i></a></li>
</ul>
  
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>
<script>
    window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/highlight.min.js"></script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/languages/python.min.js" defer></script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/languages/javascript.min.js" defer></script><script>
    hljs.configure({
        tabReplace: '    ', 
        classPrefix: ''     
        
    })
    hljs.initHighlightingOnLoad();
</script>
<script src="https://koreanbear89.github.io/js/application.min.c181e6b0c036798c7731cfb85b41b44c80689fd48fee546b73d449386ce6ccfb.js"></script>
<script src="https://koreanbear89.github.io/js/plugin.min.46930345227de54c034f39c80841463c3b879632feb482e9f2734d4b616ae3be.js"></script>

<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            ROOT_URL: 'https:\/\/koreanbear89.github.io\/',
            CONTENT_URL: 'https:\/\/koreanbear89.github.io\/\/searchindex.json ',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script type="text/javascript" src="https://koreanbear89.github.io/js/insight.min.07c7d73e2ec5e960e55f8b98577ada0bd78b36e7ef9d6dd65b85d4b1d443e7c1a1ca7cd1d98e7105db5fd72583cbc6fbb5e386062d3bf3c2568588fbb38c0c06.js" defer></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
<script>
    tocbot.init({
        
        tocSelector: '.js-toc',
        
        contentSelector: '.js-toc-content',
        
        headingSelector: 'h1, h2, h3',
        
        hasInnerContainers: true,
    });
</script>



  </body>
</html>
