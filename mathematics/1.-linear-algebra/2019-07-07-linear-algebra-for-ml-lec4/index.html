<!DOCTYPE html>
<html lang="en">
  <head>
    <title>
        Linear Algebra for ML #4 | Eigen Decomposition  - Lab.Koreanbear|한국곰연구소
      </title>
        <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
    <meta name="renderer" content="webkit">
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    
    <meta name="theme-color" content="#000000" />
    
    <meta http-equiv="window-target" content="_top" />
    
    
    <meta name="description" content="4.0 Introduction Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource. The above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$ $D$ is a diagonal matrix with eigenvalues in diagonal entries $V$ is a matrix whose column vectors" />
    <meta name="generator" content="Hugo 0.99.1 with theme pure" />
    <title>Linear Algebra for ML #4 | Eigen Decomposition  - Lab.Koreanbear|한국곰연구소</title>
    
    
    <link rel="stylesheet" href="https://koreanbear89.github.io/css/style.min.d1216b260cd13e7182354139db9968c003b9e5e63805ce6bf4055d0cc1e58362.css">
    
    <link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/9.15.10/styles/github.min.css" async>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css" async>
    <meta property="og:title" content="Linear Algebra for ML #4 | Eigen Decomposition " />
<meta property="og:description" content="4.0 Introduction Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource. The above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$ $D$ is a diagonal matrix with eigenvalues in diagonal entries $V$ is a matrix whose column vectors" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://koreanbear89.github.io/mathematics/1.-linear-algebra/2019-07-07-linear-algebra-for-ml-lec4/" /><meta property="article:section" content="Mathematics" />
<meta property="article:published_time" content="2019-07-07T09:00:13+00:00" />
<meta property="article:modified_time" content="2019-07-07T09:00:13+00:00" />

<meta itemprop="name" content="Linear Algebra for ML #4 | Eigen Decomposition ">
<meta itemprop="description" content="4.0 Introduction Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource. The above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$ $D$ is a diagonal matrix with eigenvalues in diagonal entries $V$ is a matrix whose column vectors"><meta itemprop="datePublished" content="2019-07-07T09:00:13+00:00" />
<meta itemprop="dateModified" content="2019-07-07T09:00:13+00:00" />
<meta itemprop="wordCount" content="2026">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Linear Algebra for ML #4 | Eigen Decomposition "/>
<meta name="twitter:description" content="4.0 Introduction Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource. The above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$ $D$ is a diagonal matrix with eigenvalues in diagonal entries $V$ is a matrix whose column vectors"/>

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    
    
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" rel="stylesheet">

    
    <!--[if lte IE 9]>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
      <![endif]-->

    <!--[if lt IE 9]>
        <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
      <![endif]-->



  </head>

  
  

  <body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader">
    <div class="slimContent">
      <div class="navbar-header">
        <div class="profile-block text-center">
          
           
          
          <a id="avatar" href="/" target="_self">
            <img class=" img-rotate" src="https://koreanbear89.github.io/fa-igloo.png" width="200" height="200">
          </a>
          <a href="/" target="_self">
            <h2 id="name" class="hidden-xs hidden-sm">Lab.Koreanbear</h2>
          </a>
          
          <h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3>
          <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i>Seoul, Korea</small>
        </div><div class="search" id="search-form-wrap">
    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i
                        class="icon icon-search"></i></button>
            </span>
        </div>
        <div class="ins-search">
            <div class="ins-search-mask"></div>
            <div class="ins-search-container">
                <div class="ins-input-wrapper">
                    <input type="text" class="ins-search-input" placeholder="Type something..."
                        x-webkit-speech />
                    <button type="button" class="close ins-close ins-selectable" data-dismiss="modal"
                        aria-label="Close"><span aria-hidden="true">×</span></button>
                </div>
                <div class="ins-section-wrapper">
                    <div class="ins-section-container"></div>
                </div>
            </div>
        </div>
    </form>
</div>
        <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>


      <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
        <ul class="nav navbar-nav main-nav">
            <li class="menu-item menu-item-home">
                <a href="/">

                    
                    

                    
                    <i class=" fas fa-home"></i>
                  <span class="menu-title">Home</span>
                </a>
            </li>
            <li class="menu-item menu-item-about">
                <a href="/about/">

                    
                    

                    
                    <i class=" fas fa-user-alt"></i>
                  <span class="menu-title">About</span>
                </a>
            </li>
            <li class="menu-item menu-item-mathematics">
                <a href="/mathematics/">

                    
                    

                    
                    <i class=" fas fa-square-root-alt"></i>
                  <span class="menu-title">Mathematics</span>
                </a>
            </li>
            <li class="menu-item menu-item-research">
                <a href="/research/">

                    
                    

                    
                    <i class=" fas fa-book-open"></i>
                  <span class="menu-title">Research</span>
                </a>
            </li>
            <li class="menu-item menu-item-engineering">
                <a href="/engineering/">

                    
                    

                    
                    <i class=" fas fa-cog"></i>
                  <span class="menu-title">Engineering</span>
                </a>
            </li>
        </ul>
      </nav>
    </div>
  </header>

    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    
    <div class="widget-body">



        <ul style="margin-bottom:25px;"  class="category-list"><h5>Mathematics</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-linear-algebra">1. Linear Algebra</a>
                  <span class="category-list-count">6</span></li>
                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-statistics">2. Statistics</a>
                  <span class="category-list-count">11</span></li>
                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-mathematics-for-ml">3. Mathematics for ML</a>
                  <span class="category-list-count">12</span></li>
                  
              
        </ul><hr>



        <ul style="margin-bottom:25px;"  class="category-list"><h5>Research</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-computer-science">1. Computer Science</a>
                  <span class="category-list-count">8</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-machine-learning">2. Machine Learning</a>
                  <span class="category-list-count">12</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-computer-vision">3. Computer Vision</a>
                  <span class="category-list-count">13</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/4.-image-processing">4. Image Processing</a>
                  <span class="category-list-count">4</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/5.-natural-language">5. Natural Language</a>
                  <span class="category-list-count">5</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/6.-recommendation-system">6. Recommendation System</a>
                  <span class="category-list-count">2</span></li>

                  
              
        </ul><hr>




        <ul style="margin-bottom:25px;"  class="category-list"><h5>Engineering</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-tools">1. Tools</a>
                  <span class="category-list-count">15</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-linux">2. Linux</a>
                  <span class="category-list-count">8</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-spark">3. Spark</a>
                  <span class="category-list-count">4</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/4.-web">4. Web</a>
                  <span class="category-list-count">3</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/5.-study">5. Study</a>
                  <span class="category-list-count">6</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/6.-mac">6. Mac</a>
                  <span class="category-list-count">1</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/8.-leet-code">8. Leet Code</a>
                  <span class="category-list-count">1</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/9.-others">9. Others</a>
                  <span class="category-list-count">1</span></li>

                  
              
        </ul><hr>









    </div>
</div>

  </div>
</aside>

    
    


  
  <div class="sidebar-toc-all">
  <aside class="sidebar sidebar-toc show" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  
    <div class="slimContent">
      <h4 class="toc-title">Contents</h4>
      <nav id="toc" class="js-toc toc" >
      </nav>
    </div>
  </aside>
</div>

<main class="main" role="main"><div class="content">
  <article id="-" class="article article-type-" itemscope
    itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      <h1 itemprop="name">
  <a
    class="article-title"
    href="/mathematics/1.-linear-algebra/2019-07-07-linear-algebra-for-ml-lec4/"
    >Linear Algebra for ML #4 | Eigen Decomposition </a
  >
</h1>

      <div class="article-meta">
        
<span class="article-date">
  <i class="icon icon-calendar-check"></i>&nbsp;
<a href="https://koreanbear89.github.io/mathematics/1.-linear-algebra/2019-07-07-linear-algebra-for-ml-lec4/" class="article-date">
  <time datetime="2019-07-07 09:00:13 &#43;0000 UTC" itemprop="datePublished">2019-07-07</time>
</a>
</span>
<span class="article-category">
  <i class="icon icon-folder"></i>&nbsp;
  <a href="/categories/1.-linear-algebra/"> 1. Linear Algebra </a>
</span>


        <span class="post-comment"><i class="icon icon-comment"></i>&nbsp;<a href="/mathematics/1.-linear-algebra/2019-07-07-linear-algebra-for-ml-lec4/#comments"
            class="article-comment-link">Comments</a></span>
      </div>
    </div>
    <div class="article-entry marked-body js-toc-content" itemprop="articleBody">
      <h2 id="40-introduction">4.0 Introduction</h2>
<ul>
<li>
<p>Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource.</p>
</li>
<li>
<p>The above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$</p>
<ol>
<li>
<p>$D$ is a diagonal matrix with eigenvalues in diagonal entries</p>
</li>
<li>
<p>$V$ is a matrix whose column vectors are eigenvectors</p>
</li>
<li>
<p>$A$ is a given square matrix $A \in \mathbb{R}^{n \times n}$</p>
</li>
</ol>
</li>
<li>
<p>Consider the linear transformation $T(\mathbf{x}) = A \mathbf{x} = VDV^{-1} \mathbf{x} $, it can be seen to be a series of following transformation. (4.5.1+ 4.5.2 + 4.5.3)</p>
<ol>
<li>
<p>Change of Basis</p>
</li>
<li>
<p>Element-wise Scaling</p>
</li>
<li>
<p>Back to Original Basis</p>
</li>
</ol>
</li>
</ul>
<p><br><br><br></p>
<h2 id="41-eigenvectors-and-eigenvalues">4.1 Eigenvectors and Eigenvalues</h2>
<ul>
<li>
<p>An <strong>eigenvector</strong> of a square matrix $A \in \mathbb{R}^{n \times n}$ is a nonzero vector $\mathbf{x} \in \mathbb{R}^n$ such that $A \mathbf{x} = \lambda \mathbf{x}$ for some scalar $\lambda$. In this case $\mathbf{\lambda}$ is called an eigenvalue of $A$, and such an $\mathbf{x}$ is called an eigenvector.</p>
</li>
<li>
<p>$A \mathbf{x}$ can be considered as a linear transformation $T( \mathbf{x})$. If $\mathbf{x} $ is an eigenvector, then $T(x) = A \mathbf{x} = \lambda \mathbf{x} $, which means <mark> <strong>the output vector has the same direction as $\mathbf{x}$, but the length is scaled by a factor of</strong> $\lambda$ </mark></p>
</li>
</ul>
<p style="text-align: center;">
<img class="post-fig" width="50%" align="center" src="  ">
</p>
<center>
  <img width="65%" src="/figures/2019-07-07-fig1.png"></center>
<center>  
  Fig1. Example of eigenvector and eigenvalue</center>
<br>
<ul>
<li>
<p>And here, $8 \begin{bmatrix} 1 \\ 1 \end{bmatrix} $  is faster than $ \begin{bmatrix} 2\ 6 \\ 5 \ 3 \end{bmatrix}\begin{bmatrix} 1\ 1 \end{bmatrix} $</p>
</li>
<li>
<p>The equation $A \mathbf{x} = \lambda \mathbf{x}$ can be re-written as
$$(A-\lambda I)\mathbf{x}=\mathbf{0} $$</p>
</li>
<li>
<p>$\lambda$ is an eigenvalue of an $n \times n$ matrix $A$ if and only if this equation has a nontrivial solution. (since $\mathbf{x}$ should be a nonzero vector).</p>
</li>
<li>
<p>The set of all solutions of the above equation is the null space of the matrix $A-\lambda I$, which we call the eigenspace of A corresponding to $\lambda$</p>
</li>
<li>
<p>The eigenspace consists of the zero vector and all the eigenvectors corresponding to $\lambda$, satisfying the above equation.</p>
</li>
</ul>
<blockquote>
<ul>
<li>eigenvector는 정방행렬 $A$ 에 대하여 정의되는데 이때 $A$ 는 Linear Transform (scale, rotate, etc)으로 볼 수도 있다.</li>
<li>즉 어떤 선형변환 $A$ 의 eigenvector 는 선형변환 $A$를 적용했을때 방향은 변하지 않고 크기만 변하는 vector들을 의미하며 이 때, eigenvector 크기의 변화량을 eigenvalue라고 한다.</li>
<li>For a given linear Transformation A, we can find vectors with the same direction and scaled length before and after transformation. we call these vectors as eigenvector, and the scaled factor $\lambda$ as eigenvalues.</li>
</ul>
</blockquote>
<p><br><br><br></p>
<h2 id="42-null-space">4.2 Null Space</h2>
<ul>
<li><strong>Null Space</strong> : The null space of a matrix $A \in \mathbb{R}^{m\times n}$ is the <strong>set of all solutions of a homogeneous linear system</strong>, $A \mathbf{x = 0}$</li>
<li>For $A = \begin{bmatrix} \mathbf{a_1^T} \\ &hellip; \\ \mathbf{a_m^T}  \end{bmatrix}$, $\mathbf{x}$ should satify the following $ \mathbf{ a_1^Tx =a_2^Tx= &hellip; =a_m^Tx  }=0$, That is, $\mathbf{x} $ should be orthogonal to every row vector in $A$</li>
<li><strong>Orthogonal Complement</strong> : The set of all vectors $\mathbf{z}$ that are orthogonal to $W$ is called the orthogonal complement of $W$ and is denoted by $W ^ \perp$.</li>
</ul>
<blockquote>
<ul>
<li>Null Space 란 선형방정식 $Ax = 0$ 의 해들이 이루는 공간. 즉, A 의 모든 열벡터에 대하여 orthogonal인 ($A\mathbb{x}=0$) 벡터 $\mathbf{x}$의 집합을 null space 라고 한다.</li>
<li>Orthogonal Complement 는 주어진 subspace와 수직인 벡터들의 공간</li>
</ul>
</blockquote>
<p><br><br><br></p>
<h2 id="43-characteristic-equation">4.3 Characteristic equation</h2>
<p>$$(A - \lambda I) \mathbf{x = 0}$$</p>
<ul>
<li>
<p>The set of all solutions of the above equation = null space of the matrix $(A - \lambda I)$ = eigenspace of $A$ corresponding to $\lambda$</p>
</li>
<li>
<p>The eigensapce consists of the zero vector and all the engienvectors corresponding to $\lambda$</p>
</li>
<li>
<p>How can we find the eigenvalues?</p>
</li>
<li>
<p>If $(A-\lambda I)\mathbf{x = 0}$ has a nontrivial solution, then the columns of $(A - \lambda I)$ should be noninvertible.</p>
</li>
<li>
<p>If it is invertible, $\mathbf{x}$ cannot be a nonzero vector since</p>
</li>
</ul>
<p>$$ (A - \lambda I)^{-1} (A - \lambda I) \mathbf{x} = (A - \lambda I)^{-1} \mathbf{0} \longrightarrow \mathbf{x = 0} $$</p>
<ul>
<li><mark> Thus, we can obtain eignevalues by solving <strong>characteristic equation</strong> :    </mark></li>
</ul>
<p>$$ det(A - \lambda I) = 0$$</p>
<ul>
<li>
<p>Also, the solution is not unique, and thus $A-\lambda I$ has linearly dependent columns.</p>
</li>
<li>
<p>Once obtaining eigenvalues, we compute the eigenvectors for each $\lambda$ by solving :
$$ (A-\lambda I) \mathbf{x = 0}$$</p>
</li>
<li>
<p><strong>Eigenspace</strong> : Note that the dimension of the eigenspace (corresponding to a particular $\lambda$) can be more than one. In this case, any vector in the eigenspace satisfies</p>
</li>
</ul>
<p>$$ T\mathbf{(x)} = A \mathbf{(x)} = \lambda \mathbf{(x)}$$</p>
<p style="text-align: center;">
<img class="post-fig" width="50%" align="center" src=" /figures/2019-07-07-fig2.png ">
</p>
<br>
<ul>
<li>
<p>In summary, we can find all the possible eigenvalues and eignevectors, as follows.</p>
</li>
<li>
<p>First, find all the eigenvalue by solving the characteristic equation : $ det(A-\lambda I) = 0$</p>
</li>
<li>
<p>Second, for each eigenvalue $\lambda$, solve for $(A-\lambda I) \mathbf{x=0}$
and obtain the set of basis vectors of the corresponding eigenspace.</p>
</li>
</ul>
<blockquote>
<ul>
<li>우선 characteristic equation $ det(A-\lambda I) = 0$ 를 풀어 eigenvalue를 찾습니다.</li>
<li>이후 각각의 eigenvalue $\lambda$ 에 대하여 $(A-\lambda I) \mathbf{x=0}$ 를 풀어 eigenvector를 찾습니다.</li>
</ul>
</blockquote>
<blockquote>
<p>즉, eigenvector가 존재하려면 Characteristic Equation이 Linearly Dependent 한 column을 가져야하며 이는 곧 $ det(A-\lambda I) = 0$ 가 역행렬을 갖지 않는다는 말과 같고 다시말해, Determinant가 0이 되어야합니다.</p>
</blockquote>
<br>
<h2 id="44-diagonalization">4.4 Diagonalization</h2>
<ul>
<li>
<p><strong>Diagonal matrix</strong> : matrix in which the entries outside the main diagonal are all zero</p>
</li>
<li>
<p><strong>Diagonalization</strong> : We want to change a given square matrix $A \in \mathbb{R}^{n\times n}$ into a diagonal matrix via the following form : $D = V^{-1} A  V $</p>
</li>
<li>
<p>For $A$ to be diagonalizable, an invertible $V$ should  exist such that $V^{-1}AV$</p>
</li>
<li>
<p><strong>How can we find an invertible $V$ and the resulting diagonal matrix $D$</strong></p>
</li>
<li>
<p>$V = [\mathbf{v_1, v_2 , &hellip; , v_n}]$ where $\mathbb{v_i}$ are column vectors of $V$</p>
</li>
<li>
<p>$D = diag[\lambda_1, \lambda_2 , &hellip; , \lambda_n ]$</p>
</li>
</ul>
<p>$$ D = V^{-1}AV $$</p>
<p>$$ VD = AV$$</p>
<p>$$AV = A[ \mathbf{v_1, v_2 , &hellip; v_n }] = [A\mathbf{v_1}, A\mathbf{v_2},&hellip;, A\mathbf{v_n}], (\text{column vector matmul})$$</p>
<p>$$VD = [\lambda \mathbf{v_1},\lambda \mathbf{v_2}, &hellip; ,\lambda \mathbf{v_n}]$$</p>
<p>$$AV=VD \longleftrightarrow [A\mathbf{v_1}, A\mathbf{v_2},&hellip;, A\mathbf{v_n}] = [\lambda \mathbf{v_1},\lambda \mathbf{v_2}, &hellip; ,\lambda \mathbf{v_n}]$$</p>
<ul>
<li>
<p>From above, we obtain</p>
<p>$$A \mathbf{v_1} = \lambda \mathbf{v_1}, A \mathbf{v_2} = \lambda \mathbf{v_2}, &hellip; , A \mathbf{v_n} = \lambda \mathbf{v_n}$$</p>
</li>
<li>
<p><mark> Thus $\mathbf{v_1, v_2, &hellip; v_n} $ should be eigenvectors and $\lambda_1, \lambda_2, &hellip;. \lambda_n$
should be eigenvalues. </mark></p>
</li>
<li>
<p>Then, For $VD = AV \longrightarrow D = V^{-1}AV$ to be true, $V$ should invertible.</p>
</li>
<li>
<p>In this case, the resulting diagonal matrix D has eigenvalues as diagonal entries</p>
</li>
</ul>
<blockquote>
<p>즉 대각화를 통해 얻어진 대각행렬은 해당행렬의 eigenvalue를 diagonal entries로 갖게됩니다.</p>
</blockquote>
<ul>
<li><strong>Diagonalizable matrix</strong> : For $V$ to be invertible, $V$ should be a square matrix in $\mathbb{R}^{n \times n}$, and $V$ should have $n$ linearly independent columns : Hence, $A$ should have $n$ linearly independent eigenvectors.</li>
</ul>
<blockquote>
<p>즉, 3x3 matrix 가 diagonalizable 하다면 3개의 linearly independent한 eigenvector를 찾을 수 있다.</p>
</blockquote>
<br>
<h2 id="45-eigen-decomposition-and-linear-transfomation">4.5 Eigen-Decomposition and Linear Transfomation</h2>
<ul>
<li>
<p><strong>Eigendecomposition</strong> : If $A$ is diagonalizable, we can write $A = V^{-1}DV$, and we can also write $A = VDV^{-1}$, which we call eigendecomposition of $A$. So, $A$ being diagonalizable is equilvalent to $A$ having Eigendecomposition.</p>
</li>
<li>
<p>Suppose $A$ is diagonalizable, thus having eigendecomposition $ A = VDV^{-1}$,  Consider the linear transformation $T(\mathbf{x}) = A \mathbf{x}$, it can be seen to be a series of following transformation. (4.5.1+ 4.5.2 + 4.5.3)</p>
<p>$$T(\mathbf{x}) = A\mathbf{x} = VDV^{-1}\mathbf{x} = V(D(V^{-1}\mathbf{x}))$$</p>
</li>
</ul>
<blockquote>
<p>Diagonalizable $A$ 는 $VDV^{-1}$ 로 eigendecompostion이 가능하고, 이렇게 분해된 $V^{-1}, D, V$ 와의 matmul은 각각 기하학적으로 change of basis, Scaling, Back to original basis 를 순차적으로 시행하는것과 같다.</p>
</blockquote>
<p><strong>4.5.1 Change of Basis</strong></p>
<ul>
<li>Let $\mathbf{y} = V^{-1}\mathbf{x}$ then $V \mathbf{y=x}$</li>
<li>$\mathbf{y}$ is a new coordinate of $\mathbf{x}$ with respect to a new basis of eigenvectors {$\mathbf{v_1, v_2}$}</li>
<li><a href="https://angeloyeo.github.io/2020/12/07/change_of_basis.html">https://angeloyeo.github.io/2020/12/07/change_of_basis.html</a></li>
</ul>
<br>
<p><strong>4.5.2 Element-wise and Dimension-Wise Scaling</strong></p>
<ul>
<li>
<p>$T(\mathbf{x}) = V(D(V^{-1}\mathbf{x})) = V(D\mathbf{y})$</p>
</li>
<li>
<p>Let $ z=D\mathbf{y}$. This computation is a simple Element-wise scaling of $\mathbf{y}$</p>
</li>
</ul>
<blockquote>
<p>즉 diagonal matrix 와의 곱은 계산상의 이점이 있다.</p>
</blockquote>
<br>
<p><strong>4.5.3 Back to Original Basis</strong></p>
<ul>
<li>
<p>$T(\mathbf{x}) = V(D\mathbf{y}) = Vz$</p>
</li>
<li>
<p>$z$ is still a coordinate based on the new basis $\mathbf{v_1, v_2}$</p>
</li>
<li>
<p>$Vz$ converts $z$ to another coordinates based on the original standard basis.</p>
</li>
<li>
<p>That is, $Vz$ is a linear combination of $\mathbf{v_1}$ and $\mathbf{v2}$ using the coefficient vector $z$.</p>
</li>
<li>
<p>That is,</p>
</li>
</ul>
<p>$$Vz = \begin{bmatrix} \mathbf{v_1} \ \mathbf{v_2} \end{bmatrix}  \begin{bmatrix} z_1 \ z_2 \end{bmatrix} = \mathbf{v_1}z_1 + \mathbf{v_2}z_2$$</p>
<br>
<p style="text-align: center;">
<img class="post-fig" width="80%" align="center" src="/figures/2019-07-07-fig3_2.png"></p>
<center>Fig3. Eigendecomposition as a series of transformation</center>
<br>
<blockquote>
<p>즉, $T(x) = Ax$ 는 $V(D(V^{-1}x))$ 와 같이 다시 쓸 수 있고 이는 아래와 같이 해석할 수 있다</p>
<ol>
<li>$y=V^{-1}x$ : eigenvector들을 basis로 하는 새로운 좌표계를 구하고</li>
<li>$z = Dy = D(V^{-1}x)$ : Element 별로 따로따로 eigenvalue를 곱하여 간단하게 계산하는 scaling 을 해주고</li>
<li>$Vz = V(D(V^{-1}x))$ : 다시 원래 좌표계로 복원함</li>
</ol>
<p>이렇게 복잡한 작업을 굳이 하는 이유는 4.5.4와 같이 계산상의 이점 때문입니다.</p>
</blockquote>
<blockquote>
<p><strong>Example :</strong> $A = VDV^{-1}  = \begin{bmatrix} 3&amp;{-2}\\ 1&amp;1 \end{bmatrix} \begin{bmatrix} -1&amp;0\\ 0&amp;2 \end{bmatrix}\begin{bmatrix} 3&amp;{-2}\\ 1&amp;1 \end{bmatrix}^{-1}, \text{ and } \bf{x}=\begin{bmatrix} 4\\ 3 \end{bmatrix}  $</p>
<p>$$ y =  \begin{bmatrix} 3&amp;{-2}\\ 1&amp;1 \end{bmatrix}^{-1}\begin{bmatrix} 4\\ 3 \end{bmatrix} = \begin{bmatrix} 2\\ 1 \end{bmatrix}  $$</p>
<p>$$ z = \begin{bmatrix} -1 &amp; 0\\ 0 &amp;2 \end{bmatrix}  \begin{bmatrix} 2\\ 1 \end{bmatrix}   = \begin{bmatrix} -2\\ 2 \end{bmatrix}$$</p>
<p>$$ Vz =  \begin{bmatrix} 3&amp;{-2}\\ 1&amp;1 \end{bmatrix}\begin{bmatrix} -2\\ 2 \end{bmatrix} = \begin{bmatrix} -10\\ 0 \end{bmatrix}  $$</p>
</blockquote>
<br>
<p><strong>4.5.4 Linaer Transformation via $A^k$</strong></p>
<ul>
<li>
<p>Now, consider recursive transformation $A \times A \times A &hellip; \times A \mathbf{x} = A^k \mathbf{x} $</p>
</li>
<li>
<p>If $A$ is diagonalizable, $A$ has Eigendecomposition</p>
</li>
</ul>
<p>$$ A = VDV^{-1}$$</p>
<p>$$ A^k = (VDV^{-1})(VDV^{-1})(VDV^{-1})&hellip;(VDV^{-1}) = (VD{^k}V^{-1})$$</p>
<ul>
<li>
<p>Here, $D^k$ is simply computed as $diag[\lambda_1^k, \lambda_2^k&hellip; , \lambda_n^k]$</p>
</li>
<li>
<p>It is much faster to compute $V(D^k(V^{-1}\mathbf{x}))$ than to compute $A^k \mathbf{x}$</p>
</li>
</ul>
<br>
<h2 id="46-summary-so-far">4.6 Summary So Far</h2>
<ul>
<li>
<p>Eigenvalue and Eigenvectors</p>
</li>
<li>
<p>Null space, Column space, and orthogonal complement in $\mathbb{R}^n$</p>
</li>
<li>
<p>Diagonalization and Eigendecomposition</p>
</li>
<li>
<p>Linear transforamtion via eigendecomposition</p>
</li>
</ul>
<br>

    </div>
    <div class="article-footer">

    </div>
  </article>

</div><nav class="bar bar-footer clearfix" data-stick-bottom>
    <div class="bar-inner">
        <ul class="pager pull-right">
            
            
            
            
        </ul>
        <div class="bar-right">
        </div>
    </div>
</nav>


</main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
<ul class="social-links">
    <li><a href="https://github.com/koreanbear89" target="_blank" title="github" data-toggle=tooltip data-placement=top >
            <i class="icon icon-github"></i></a></li>
    <li><a href="https://www.linkedin.com/in/hanwoong-kim-95b5a7130/" target="_blank" title="linkedin" data-toggle=tooltip data-placement=top >
            <i class="icon icon-linkedin"></i></a></li>
    <li><a href="https://koreanbear89.github.io/index.xml" target="_blank" title="rss" data-toggle=tooltip data-placement=top >
            <i class="icon icon-rss"></i></a></li>
</ul>
  
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>
<script>
    window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/highlight.min.js"></script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/languages/python.min.js" defer></script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/languages/javascript.min.js" defer></script><script>
    hljs.configure({
        tabReplace: '    ', 
        classPrefix: ''     
        
    })
    hljs.initHighlightingOnLoad();
</script>
<script src="https://koreanbear89.github.io/js/application.min.c181e6b0c036798c7731cfb85b41b44c80689fd48fee546b73d449386ce6ccfb.js"></script>
<script src="https://koreanbear89.github.io/js/plugin.min.cf6ab047215536635a92ff5defe1e5174ee0acd6c4950757b2732693f7f196f3.js"></script>

<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            ROOT_URL: 'https:\/\/koreanbear89.github.io\/',
            CONTENT_URL: 'https:\/\/koreanbear89.github.io\/\/searchindex.json ',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script type="text/javascript" src="https://koreanbear89.github.io/js/insight.min.853c5d4062a8c262b2490c70df2f6d2b232483f85227b30b28013788b8c4da8ab59c1a735b9b31c9006f2962ef62213fea9d17cb9469c297f201772ce94e8fdf.js" defer></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
<script>
    tocbot.init({
        
        tocSelector: '.js-toc',
        
        contentSelector: '.js-toc-content',
        
        headingSelector: 'h1, h2, h3',
        
        hasInnerContainers: true,
    });
</script>



  </body>
</html>
