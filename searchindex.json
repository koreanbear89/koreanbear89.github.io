{"categories":[{"title":"1. Linear Algebra","uri":"https://koreanbear89.github.io/categories/1.-linear-algebra/"},{"title":"1. Tools","uri":"https://koreanbear89.github.io/categories/1.-tools/"},{"title":"2. Linux","uri":"https://koreanbear89.github.io/categories/2.-linux/"},{"title":"2. Machine Learning","uri":"https://koreanbear89.github.io/categories/2.-machine-learning/"},{"title":"2. Statistics","uri":"https://koreanbear89.github.io/categories/2.-statistics/"},{"title":"3. Computer Vision","uri":"https://koreanbear89.github.io/categories/3.-computer-vision/"},{"title":"3. Mathematics for ML","uri":"https://koreanbear89.github.io/categories/3.-mathematics-for-ml/"},{"title":"3. Spark","uri":"https://koreanbear89.github.io/categories/3.-spark/"},{"title":"5. Natural Language","uri":"https://koreanbear89.github.io/categories/5.-natural-language/"},{"title":"5. Study","uri":"https://koreanbear89.github.io/categories/5.-study/"},{"title":"8. Leetcode","uri":"https://koreanbear89.github.io/categories/8.-leetcode/"},{"title":"9. Others","uri":"https://koreanbear89.github.io/categories/9.-others/"},{"title":"Favorites","uri":"https://koreanbear89.github.io/categories/favorites/"}],"posts":[{"content":"\nTwo Sum  Hash map  class Solution: def twoSum(self, nums: List[int], target: int) -\u0026gt; List[int]: # (1) generate empty hash map hash_map = {} # (2) iterate along the num in nums for i,num in enumerate(nums): # (2.1) get required number for each given num req = target - num # (2.2) if req in hash_map, return if req in hash_map: return [i, hash_map[req]] # (2.3) else, add number in hash map hash_map[num] = i  Maximum Subarray  kadane algorithm : 현재 인덱스 까지의 최대 부분합을 계산해두자  A[n] 까지의 maximum sub-array는 다음 둘 중 하나이다  A[n] A[n-1] 까지의 maximum sub-array + A[n]      class Solution(object): def maxSubArray(self, nums): local_max = global_max = nums[0] for num in nums[1:]: # 이전의 최댓값 + 자기 자신 or 자기 자신중 최댓 값 구하기 local_max = max(num, local_max+num) # 최종 결과 갱신 global_max = max(global_max, local_max) return global_max  ","id":0,"section":"Engineering","summary":"Two Sum Hash map class Solution: def twoSum(self, nums: List[int], target: int) -\u0026gt; List[int]: # (1) generate empty hash map hash_map = {} # (2) iterate along the num in nums for i,num in enumerate(nums): # (2.1) get required number for each given num req = target - num # (2.2) if req in hash_map, return if req in hash_map: return [i, hash_map[req]] # (2.3) else, add number in hash","tags":null,"title":"Leet Code","uri":"https://koreanbear89.github.io/engineering/8.-leet-code/leet-code/","year":"2022"},{"content":"\n1. GlusterFS  GlusterFS : is a Scalable Network Filesystem  Brick : is the basic unit of storage in GlusterFS, Volume : is a logical collection of bricks.  Create volume with bricks from several nodes (peer) And mount volume to the specific location to use volume like virtual Storage     References  Tutorial: Create a Docker Swarm with Persistent Storage Using GlusterFS – The New Stack    # install GlusterFS on CentOS sudo yum install centos-release-gluster sudo yum install glusterfs-server sudo systemctl start glusterd # start daemon sudo systemctl enable glusterd # restart daemon when reboot the machine # Configure Gluster Volume gluster peer probe \u0026lt;HOST_NAME\u0026gt; # connect HOST as peer gluster pool list # show gluster peers # Create Volume sudo mkdir -p /\u0026lt;BRICK_DIR\u0026gt; # run on all machines sudo gluster volume create \u0026lt;VOLUME_NAME\u0026gt; replica \u0026lt;N\u0026gt; \u0026lt;HOST1\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST2\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST3\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; force # run only on the master # ex) sudo gluster volume create gluster_pvs replica 4 pgsca2x0350:/home1/irteam/whome/__oss/gs_pvs pgsca2x0351:/home1/irteam/whome/__oss/gs_pvs pgsca2x0352:/home1/irteam/whome/__oss/gs_pvs pgsca2x0353:/home1/irteam/whome/__oss/gs_pvs force # Start Volume sudo gluster volume start \u0026lt;VOLUME_NAME\u0026gt; # run only on the master sudo mount.glusterfs localhost:/\u0026lt;VOLUME_NAME\u0026gt; \u0026lt;MOUNT_LOCATION\u0026gt; # run on all machines # ex) sudo mount.glusterfs localhost:/gluster_pvs /home1/irteam/mnt_gluster # Manage Gluster Volumes gluster volume info # show information of volume and bricks gluster volume stop \u0026lt;VOLUME_NAME\u0026gt; gluster volume delete \u0026lt;VOLUME_NAME\u0026gt;  ","id":1,"section":"Engineering","summary":"1. GlusterFS  GlusterFS : is a Scalable Network Filesystem  Brick : is the basic unit of storage in GlusterFS, Volume : is a logical collection of bricks.  Create volume with bricks from several nodes (peer) And mount volume to the specific location to use volume like virtual Storage     References  Tutorial: Create a Docker Swarm with Persistent Storage Using GlusterFS – The New Stack    # install GlusterFS on CentOS sudo yum install centos-release-gluster sudo yum install glusterfs-server sudo systemctl start glusterd # start daemon sudo systemctl enable glusterd # restart daemon when reboot the machine # Configure Gluster Volume gluster peer probe \u0026lt;HOST_NAME\u0026gt; # connect HOST as peer gluster pool list # show gluster peers # Create Volume sudo mkdir -p /\u0026lt;BRICK_DIR\u0026gt; # run on all machines sudo gluster volume create \u0026lt;VOLUME_NAME\u0026gt; replica \u0026lt;N\u0026gt; \u0026lt;HOST1\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST2\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST3\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; force # run only on the master # ex) sudo gluster volume create gluster_pvs replica 4 pgsca2x0350:/home1/irteam/whome/__oss/gs_pvs pgsca2x0351:/home1/irteam/whome/__oss/gs_pvs pgsca2x0352:/home1/irteam/whome/__oss/gs_pvs pgsca2x0353:/home1/irteam/whome/__oss/gs_pvs force # Start Volume sudo gluster volume start \u0026lt;VOLUME_NAME\u0026gt; # run only on the master sudo mount.","tags":null,"title":"CheatSheet | GlusterFS","uri":"https://koreanbear89.github.io/engineering/1.-tools/cheatsheet-glusterfs/","year":"2022"},{"content":"\n 1. Introduction and Motivation  Machine learning is about designing algorithms that automatically extract valuable information from data. There are three concepts that are at the core of machine learning : data, a model, and learning.  Data : Since machine learning is inherently data driven, data is at the core of machine learning. Model : would describe a function that maps inputs to real-valued outputs. Learning : can be understood as a way to automatically find patterns and structure in data by optimizing the parameters of the model    \n  1.1 Finding Words for Intuitions  Data as vectors : there are (at least) three different ways to think about vectors:  a vector as an array of numbers (computer science view), a vector as an arrow with a direction and magnitude (physics view), a vector as an object that obeys addition and scaling (a mathematical view)   Model : A good model can be used to predict what would happen in the real world without performing real-world experiments. Learning : We learn from available data by using numerical optimization methods with the aim that the model performs well on unseen data.  \n  1.2 Two Ways to Read This Book  Bottom-up : Building up the concepts from foundational to more ad-vanced. Top-down : Drilling down from practical needs to more basic requirements Part I is about Mathematics :  linear algebra : The study of vectors and matrices analytic geometry : the construction of similarity and distances matrix decomposition : Some operations on matrices are extremely useful in ML probability theory : Quantification of uncertainty vector calculus : details concept of gradients optimization : to find maxima/minima of functions   Part II is about Machine Learning :  linear regression ; to find functions that map inputs $x$ to corresponding observed function values $y$, model fitting via MLE and MAP. dimensionality reduction : to find a compact, lower-dimensional representation of high-dimensional data $x$. density estimation : to find a probability distribution that describes a given dataset. We will focus on Gaussian mixture models for this purpose, and we will discuss an iterative scheme to find the parameters of this model. classification : unlike regression, where the labels were real-valued, the labels in classification are integers, which requires special care.    ","id":2,"section":"Mathematics","summary":"1. Introduction and Motivation  Machine learning is about designing algorithms that automatically extract valuable information from data. There are three concepts that are at the core of machine learning : data, a model, and learning.  Data : Since machine learning is inherently data driven, data is at the core of machine learning. Model : would describe a function that maps inputs to real-valued outputs. Learning : can be understood as a way to automatically find patterns and structure in data by optimizing the parameters of the model","tags":null,"title":"Mathematics for ML #1 | Introduction Part.I ","uri":"https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml01-introduction/","year":"2022"},{"content":"Django # set Django project with config folder django-admin startproject config . # start Django Server python manage.py runserver 127.0.0.1:8080  ","id":3,"section":"Engineering","summary":"Django # set Django project with config folder django-admin startproject config . # start Django Server python manage.py runserver 127.0.0.1:8080  ","tags":null,"title":"CheatSheet | Django","uri":"https://koreanbear89.github.io/engineering/1.-tools/cheatsheet-django/","year":"2021"},{"content":"\nIntroduction   \u0026ldquo;#!\u0026ldquo;는, 스크립트를 실행할 쉘을 지정하는 선언문으로 #!/bin/sh를 주로 볼 수 있음\n  그리고 우분투 6.06이후 /bin/sh 는 더이상 bash가 아닌 dash를 가리키고 있음\n$ ls -al /bin/sh lrwxrwxrwx 1 root root 4 6월 30 19:44 /bin/sh -\u0026gt; dash    특수문자  ~ : 홈 디렉토리를 나타낸다. $ : 변수의 값을 표출할때 변수 이름 앞에 사용한다. \u0026amp; : 백그라운드 작업 실행의 위해 사용한다. # : 주석 * : 와일드 카드 문자 ? : 와일드카드(단일 문자) \\ : 다음 문자 일반 문자화 ( : 하위 쉘 시작 ) : 하위 쉘 끝 ! : 논리적으로 NOT 의미 / : 디렉토리 경로 구분자 ; : 쉘 명령 구분자 \u0026lt; : 입력 리다이렉션 기호 ( 파일에서 입력값을 받음) \u0026gt; : 출력 리다이렉션, 덮어쓰기 \u0026gt;\u0026gt; : 출력 리다이렉션 기호, 추가의 의미 `cmd : 역 따옴표, 명령 내 명령을 실행 | : 파이프 명령의 출력값을 다른 명령의 입력값으로 사용 [ : 문자 집합의 시작을 나타냄 ] : 문자 집합의 끝을 나타냄. ( #ls [ a-d]*.sh )  Redirection  2\u0026gt;\u0026amp;1 : 표준에러를 표준출력으로 redirection  ","id":4,"section":"Engineering","summary":"Introduction \u0026ldquo;#!\u0026ldquo;는, 스크립트를 실행할 쉘을 지정하는 선언문으로 #!/bin/sh를 주로 볼 수 있음 그리고 우분투 6.06이후 /bin/sh 는 더이","tags":null,"title":"Cheat Sheet | Shell Script ","uri":"https://koreanbear89.github.io/engineering/2.-linux/cheatsheet-shell/","year":"2021"},{"content":" The Future of Data Systems   In this final chapter,\n we will shift our perspective toward the future and discuss how things should be: propose some ideas and approaches that may fundamentally improve the ways we design and build applications.    The goal of this book was outlined in Chapter 1 : to explore how to create applications and systems that are reliable, scalable, and maintainable.\n  In this chapter we will bring all of these ideas together, and build on them to envisage the future.\n     12.1 Data Integration  A recurring theme has been that for any given problem, there are several solutions, all of which have different pros, cons, and trade-offs. So you inevitably end up having to cobble together several different pieces of software in order to provide your application’s functionality.  12.1.1 Combining Specialized Tools by Deriving Data   Derived data (log-based) vs distributed transactions\n In the absence of widespread support for a good distributed transaction protocol, I believe that log-based derived data is the most promising approach for integrating different data systems.    The limits of total ordering :\n With small systems, constructing a totally ordered event log is entirely feasible. However, as systems are scaled toward bigger and more complex workloads, limitations begin to emerge : (p.446, 168, 131, 170)    Ordering events to capture causality\n Unfortunately, there does not seem to be a simple answer to this problem Perhaps, over time, patterns for application development will emerge that allow causal dependencies to be captured efficiently, and derived state to be maintained correctly, without forcing all events to go through the bottleneck of total order broadcast.    12.1.2 Batch and Stream Processing  The goal of data integration is to make sure that data ends up in the right form in all the right places Doing so requires consuming inputs, transforming, joining, filtering, aggregating, training models, evaluating, and eventually writing to the appropriate outputs. Batch and stream processors are the tools for achieving this goal.  the main fundamental difference is that stream processors operate on unbounded datasets whereas batch process inputs are of a known, finite size. but these distinctions are beginning to blur Spark performs stream processing on top of a batch processing engine by breaking the stream into microbatches, whereas Apache Flink performs batch processing on top of a stream processing engine   Maintaining derived state :  maintaining derived data =\u0026gt; batch processing (determministic) =\u0026gt; well-defined inputs and outputs =\u0026gt; good for fault tolerance and reasoning about the dataflows   Reprocessing data for application evolution :  batch processing allows large amounts of accumulated historical data to be reprocessed in order to derive new views onto an existing dataset gradul evolution of application   The lambda architecture : how to combine batch process \u0026amp; stream processing  batch processing is used to reprocess historical data, stream processing is used to process recent updates, incoming data should be recorded by appending immutable events to an always-growing dataset, similarly to event sourcing (p.457) the stream processor consumes the events and quickly produces an approximate update to the view; the batch processor later consumes the same set of events and produces a corrected version of the derived view.   Unifying batch and stream processing : More recent work has enabled the benefits of the lambda architecture to be enjoyed without its downsides, by allowing both batch computations (reprocessing historical data) and stream computations (processing events as they arrive) to be implemented in the same system     12.2 Unbundling Databases (Done)  At a most abstract level, databases and operating systems all perform the same functions: they store some data, and they allow you to process and query that data. Unix and relational databases have approached the information management problem with very different philosophies.  Unix is simpler in the sense that it is a fairly thin wrapper around hardware resources relational data‐bases are simpler in the sense that a short declarative query can draw on a lot of powerful infrastructure   In this section I will attempt to reconcile the two philosophies, in the hope that we can combine the best of both worlds.  12.2.1 Composing Data Storage Technologies   We have discussed various features provided by databases and how they work,\n  The meta-database of everything :\n the dataflow across an entire organization starts looking like one huge database Recently, they are provided by various different pieces of software, different machines, different teams. (instead of a single integrated database product,) Where will these developments take us in the future?  Federated databases (unifying reads) : provide a unified query interface to a wide variety of underlying storage engines and processing methods. (PostgreSQL\u0026rsquo;s foreign data wrapper) Unbundled databases (unifying writes) : is like unbundling a database’s index-maintenance features that can synchronize writes across disparate technologies      Making unbundling work : keeping the writes to several storage systems in sync\n traditional approach : distributed transaction =\u0026gt; the lack of a standardized transaction protocol makes integration much harder. log-based integration : An ordered log of events with idempotent consumers =\u0026gt; loose coupling between the various components,    12.2.2 Designing Applications Around Dataflow  In this section, I will explore some ways of building applications around the ideas of unbundled databases and dataflow Application code as a derivation function :  When the function that creates a derived dataset is not a standard cookie-cutter function (like ML, feature extraction) =\u0026gt; custom code is required =\u0026gt; where many databases struggle.   Separation of application code state :  Databases could be deployment environments =\u0026gt; poorly suited it makes sense to have some parts of a system that specialize in durable data storage, and other parts that specialize in running application code. YARN, Docker, Kubernetes   Dataflow: Interplay between state changes and application code :  Stable message ordering and fault-tolerant message processing are quite stringent demands, but they are much less expensive and more operationally robust than distributed transactions. Instead of treating a database as a passive variable that is manipulated by the application, we think much more about the interplay and collaboration between state, state changes, and code that processes them. Unbundling the database means taking this idea and applying it to the creation of derived datasets outside of the primary database: caches, full-text search indexes, machine learning, or analytics systems. We can use stream processing and messaging systems for this purpose.   Stream processors and services  The currently trendy style of application development involves breaking down functionality into a set of services that communicate via synchronous network requests such as REST APIs : Microservices Not only is the dataflow approach faster, but it is also more robust to the failure of another service.    12.2.3 Observing Derived State  The derived dataset is the place where the write path and the read path meet, It represents a trade-off between the amount of work that needs to be done at write time and the amount that needs to be done at read time. Materialized views and caching (fig12-1)  ex1) If you didn’t have an index, a search query would have to scan over all documents (like grep), which would get very expensive =\u0026gt; No index means less work on the write path (no index to update), but a lot more work on the read path. ex2) precomputing the search results for all possible queries =\u0026gt; less work to do on the read path Usually) precompute the search results for only a fixed set of the most common queries, (cache, materialized view) Viewed like this, the role of caches, indexes, and materialized views is simple: they shift the boundary between the read path and the write path   Stateful, offline-capable clients (뒤랑 연결안되면 빼도될듯)  Let\u0026rsquo;s look at the idea in different context changing capabilities have led to a renewed interest in offline-first applications we can think of the on-device state as a cache of state on the server.   Pushing state changes to clients  Typical web page : The state on the device is a stale cache that is not updated unless you explicitly poll for changes. Recent protocols : the server can actively push messages to the browser as long as it remains connected. (subscribe) The same technique works for individual users, where each device is a small subscriber to a small stream of events   End-to-end event streams  Recent tools for developing stateful clients and user interfaces, (Facebook’s toolchain of React, Flux, and Redux) already manage internal client-side state by subscribing to a stream of events representing user input or responses from a server, structured similarly to event sourcing In order to extend the write path all the way to the end user, we would need to fundamentally rethink the way we build many of these systems I think that the advantages of more responsive user interfaces and better offline support would make it worth the effort. If you are designing data systems, I hope that you will keep inmind the option of subscribing to changes, not just querying the current state.       12.3 Aiming for Correctness (Done)  In this section I will suggest some ways of thinking about correctness in the context of dataflow architectures.  12.3.1 The End-to-End Argument for Databases  Let’s look at a more subtle example of data corruption that can occur. Exactly-once execution of an operation  If something goes wrong while processing a message, you can either give up or try again. If you try again =\u0026gt; there is the risk that the message ends up being processed twice (charge a customer twice) =\u0026gt; idempotence (478p) =\u0026gt; the final effect is the same as if no faults had occurred, even if the operation actually was retried due to some fault   Duplicate suppression  The same pattern of duplicates occurs in many other places besides stream processing.   Operation identifiers  To make the operation idempotent, it is not sufficient to rely just on a transaction mechanism you need to consider the end-to-end flow of the request.  For example, you could generate a unique identifier for an operation (such as a UUID) client reques twice =\u0026gt; have the same operation ID     The end-to-end argument  This scenario of suppressing duplicate transactions is just one example of a more general principle called the end-to-end argument   Applying end-to-end thinking in data systems  It would be really nice to wrap up the remaining high-level fault-tolerance machinery in an abstraction so that application code needn’t worry about it But we have not yet found the right abstraction I think it is worth exploring fault-tolerance abstractions that make it easy to provide application-specific end-to-end correctness properties,    12.3.2 Enforcing Constraints  Techniques that enforce uniqueness can often be used for these kinds of constraints as well  Uniqueness constraints (user name, email address, book\u0026hellip;) Account balance never goes negative,   Uniqueness constraints require consensus : there are several concurrent requests with the same value, the system somehow needs to decide which one of the conflicting operations is accepted, and reject the others as violations of the constraint (Chapter9) Uniqueness in log-based messaging : In the unbundled database approach with log-based messaging, we can use a very similar approach to enforce uniqueness constraints : Total Order Broadcast. (p.348)  12.3.3 Timeliness and Integrity  consistency : conflates two different requirements that are worth considering separately  Timelineness : means ensuring that users observe the system in an up-to-date state Integrity : means absence of corruption; i.e., no data loss, and no contradictory orfalse data   In most applications, integrity (error in sum) is much more important than timeliness (not yet appear). : credit card statement Loosely interpreted constraints :  many real applications can actually get away with much weaker notions of uniqueness (overbook) In many business contexts, it is actually acceptable to temporarily violate a constraintand fix it up later by apologizing These applications do require integrity: you would not want to lose a reservation, or have money disappear due to mismatched credits and debits.   Coordination-avoiding data systems  You cannot reduce the number of apologies to zero, but you can aim to find the best trade-off for your needs—the sweet spot where there are neither too many inconsistencies nor too many availability problems.    12.3.4 Trust, but Verify   All of our discussion of correctness, integrity, and fault-tolerance has been under the assumption that certain things might go wrong,\n  But we might also assume that data written to disk is not lost after fsync, that data in memory is not corrupted, and that the multiplication instruction of our CPU always returns the correct result.\n  They are still very rare on modern hardware. I just wantt o point out that they are not beyond the realm of possibility, and so they deserve some attention.\n  Maintaining integrity in the face of software bugs : Despite considerable efforts in careful design, testing, and review, bugs still creep in.\n  Don’t just blindly trust what they promise : Thus, we should at least have a way of finding out if data has been corrupted so that we can fix it and try to track down the source of the error. Don’t just blindly trust that it is all working\n  A culture of verification :\n Systems like HDFS and S3 still have to assume that disks work correctly most of the time—which is a reasonable assumption, but not the same as assuming that they always work correctly. self-validating or self-auditing systems : read back files, compare them to other replicas, and move files from one disk toanother, in order to mitigate the risk of silent corruption    Tools for auditable data systems\n distributed ledger technologies : Bitcoin, Ethereum, Ripple,\u0026hellip; I could imagine integrity-checking and auditing algorithms, like those of certificate transparency and distributed ledgers, becoming more widely used in data systems ingeneral.      12.4 Doing the Right Thing (Done)  Throughout this book we have examined a wide range of different architectures for data systems, Finally, Let\u0026rsquo;s take a step back and examine some ethical aspects of building data intensive applications.  12.4.1 Predictive Analytics  Algorithmic prison : As algorithmic decision-making becomes more widespread, someone who has been labeled as risky by some algorithm may suffer a large number of those “no” decisions (jobs, insurance coverage, rental, ..) Bias and discrimination : If there is a systematic bias in the input to an algorithm, the system will most likely learn and amplify that bias in its output Responsibility and accountability : When a self-driving car causes an accident, who is responsible? Feedback loops : When services become good at predicting what content users want to see, they may end up showing people only opinions they already agree with, leading to echochambers in which stereotypes, misinformation, and polarization can breed.  12.4.2 Privacy and Tracking   Besides the problems of predictive analytics, there are ethical problems with data collection itself. =\u0026gt; surveillance, privacy, etc.\n  As software and data are having such a large impact on the world, we engineers must remember that we carry a responsibility to work toward the kind of world that we want to live in: a world that treats people with humanity and respect.\n  ","id":5,"section":"Engineering","summary":"The Future of Data Systems In this final chapter, we will shift our perspective toward the future and discuss how things should be: propose some ideas and approaches that may fundamentally improve the ways we design and build applications. The goal of this book was outlined in Chapter 1 : to explore how to create applications and systems that are reliable, scalable, and maintainable. In this chapter we will bring","tags":null,"title":"Data Intensive App #12 | The Future of Data Systems","uri":"https://koreanbear89.github.io/engineering/5.-study/data-intensive-app-12/","year":"2021"},{"content":" 8.0 Introduction  In the last few chapters =\u0026gt; how systems handle things going wrong However, even though we have talked a lot about faults, the last few chapters have still been too optimistic  The reality is even darker we will now turn our pessimism to the maximum and assume that anything that can go wrong will go wrong   In this chapter  we will get a taste of the problems that arise in practice and understanding of the things we can and cannot rely on we must understand what challenges we are up against   Next chapter,  In spite of everthing going wrong, our task as engineers is to build systems that do their job (i.e, meet the guarantees that users are expecting) We will look at some examples of algorithms that can provide such guarantees in a distributed systems    \n8.1 Faults and Partial Failures  When you are writing a program on a single computer, it normally behaves in a fairly predictable way : either it works or it doesn’t When you are writing software that runs on several computers, connected by a network, the situation is fundamentally different. Partial failure : In a distributed system, there may well be some parts of the system that are broken in some unpredictable way, even though other parts of the system are working fine  Nondeterministic : if you try to do anything involving multiple nodes and the network, it may sometimes work and sometimes unpredictably fail.   This non-determinism and possibility of partial failures is what makes distributed systems hard to work with.  8.1.1 Cloud Computing and Supercomputing  philosophies on how to build large-scale computing systems:  Cloud Computing : commodity computers connected with and IP network HPC (high-performance computing) : super computers with thousands of CPUs   In a super-computer : If one node fails, a common solution is to simply stop the entire cluster workload. After the faulty node is repaired, the computation is restarted from the last checkpoint. =\u0026gt; A supercomputer is more like a single-node computer than a distributed system. we must accept the possibility of partial failure and build fault-tolerance mechanisms into the software. In other words, we need to build a reliable system from unreliable components.  \n8.2 Unreliable Networks   The distributed systems we focus on in this book are shared-nothing systems: i.e., a bunch of machines connected by a network\n  The internet and most internal networks in datacenters are asynchronous packet networks.\n=\u0026gt; one node can send a message (a packet) to another node, but the network gives no guarantees as to when it will arrive or whether it will arrive at all.\n=\u0026gt; If you send a request and expect a response, many things could go wrong.\n  The sender can’t even tell whether the packet was delivered: the only option is for the recipient to send a response message, which may in turn be lost or delayed.\n  The usual way of handling this issue is a timeout: after some time you give up waiting and assume that the response is not going to arrive.\n  8.2.1 Network Faults in Practice  We have been building computer networks for decades—one might hope that by now we would have figured out how to make them reliable. =\u0026gt; However, it seems that we have not yet succeeded. network problems can be surprisingly common =\u0026gt; medium-sized datacenter found about 12 faults/month Even if network faults are rare in your environment, the fact that faults can occur means that your software needs to be able to handle them  8.2.2 Detecting Faults  Many systems need to automatically detect faulty nodes  ex) In a distributed DB with single-leader replication, if the leader fails, one of the followers needs to be promoted to be the new leader   the uncertainty about the network makes it difficult to tell whether a node is working or not  8.2.3 Timeouts and Unbounded Delays   If a timeout is the only sure way of detecting a fault, then how long should the time‐out be? : \u0026ldquo;Failure detection delay\u0026rdquo; vs \u0026ldquo;risk of premature timeouts\u0026rdquo;\n A long timeout : a long wait until a node is declared dead A short timeout : detects faults faster, but carries a higher risk of incorrectly declaring a node dead  When a node is declared dead, its responsibilities need to be transferred to other nodes, =\u0026gt; places additional load on other nodes and the network. =\u0026gt; If the system is already struggling with high load, declaring nodes dead prematurely can make the problem worse.      A fictious system with maximum delay $d$, guarantee that a non-failed node always handles a request within some time $r$, =\u0026gt; $2d+r$ would be a reasonable timeout to use\n  Unfortunately, most systems we work with have neither of those guarantees: asynchronous networks have unbounded delays (that is, they try to deliver packets as quickly as possible, but there is no upper limit on the time it may take for a packet to arrive),\n  rather than using configured constant timeouts, systems can continually measure response times and their variability (jitter), and automatically adjust time‐outs according to the observed response time distribution. (Phi Accural failure detector used in Akka and Cassandra)\n  \n 8.3 Unreliable Clocks (287p)   Clocks and time are important. Applications depend on clocks in various ways to answer questions like the following:\n  Has this request timed out yet?\n  What’s the 99th percentile response time of this service?\n  How many queries per second did this service handle on average in the last fiveminutes?\n  How long did the user spend on our site?\n    Each machine on the network has its own clock, which is an actual hardware device: usually a quartz crystal oscillator. These devices are not perfectly accurate, so each machine has its own notion of time, which may be slightly faster or slower than on other machines.\n  8.3.1 Time-of-Day Clocks vs Monotonic   Time of Day clocks : does what you intuitively expect of a clock: it returns the current date and time according to some calendar\n clock_gettime(CLOCK_REALTIME) on Linux System.currentTimeMillis() in Java return the number of seconds since the epoch: midnight UTC on January 1, 1970 usually synchronized with NTP (Network Time Protocol), which means that a time‐stamp from one machine (ideally) means the same as a timestamp on another machine. if the local clock is too far ahead of the NTP server, it maybe forcibly reset and appear to jump back to a previous point in time =\u0026gt; make time-of-day clocks unsuitable for measuring elapsed time    Monotonic clocks : are guaranteed to always move forward\n A monotonic clock is suitable for measuring a duration, such as a timeout or a service’s response time clock_gettime(CLOCK_MONOTONIC) on Linux and System.nanoTime() in Java time.monotonic() in python By default, NTP allows the clock rate to be speeded up or slowed down by up to 0.05%, but NTP cannot cause the monotonic clock to jump forward or backward.    8.3.2 Clock Synchronization and Accuracy  Monotonic clocks don’t need synchronization, but time-of-day clocks need to be set according to an NTP server Unfortunately, our methods for getting a clock to tell the correct time aren’t nearly as reliable or accurate as you might hope It is possible to achieve very good clock accuracy if you care about it sufficiently to invest significant resources. (e.g. financial institutions, trading funds) Such accuracy can be achieved using GPS receivers, and careful deployment and monitoring. However, it requires significant effort and expertise, and there are plenty of ways clock synchronization can go wrong.  8.3.3 Relying on Synchronized Clocks  Although clocks work quite well most of the time, robust software needs to be prepared to deal with incorrect clocks However incorrect clockes easily go unnoticed. If its quartz clock is defective or its NTP client is misconfigured, most things will seem to work fine, even though its clock gradually drifts further and further away from reality. Timestamps for ordering events : (Figure 8-3) Clock readings have a confidence interval  Thus, it doesn’t make sense to think of a clock reading as a point in time - it is more like a range of times, Google’s TrueTime API (Spanner) explicitly reports the confidence interval on the local clock. =\u0026gt; returns [earliest, latest]    8.3.4 Process Pauses  Let’s consider another example of dangerous clock use in a distributed system. What if there is an unexpected pause in the execution of the program?  leader paused =\u0026gt; another leader take over =\u0026gt; previous leader resumed   Is it crazy to assume that a thread might be paused for so long? Unfortunately not. There are various reasons why this could happen:  Garbage collector (GC) that occasionally needs to stop all running threads. In virtualized environments, a virtual machine can be suspended and resumed If the application performs synchronous disk access, a thread may be paused waiting for a slow disk I/O operation to complete A Unix process can be paused by sending it the SIGSTOP signal, for example bypressing Ctrl-Z in a shell   When writing multi-threaded code on a single machine, we have fairly good tools for making it thread-safe: mutexes, semaphores, atomic counters, lock-free data structures, blocking queues, and so on. Unfortunately, these tools don’t directly translate to distributed systems A node in a distributed system must assume that its execution can be paused for a significant length of time at any point, even in the middle of a function. During the pause, the rest of the world keeps moving and may even declare the paused node dead because it’s not responding. Response time guarantees : requires a large amount of additional work and severely restricts the range of programming languages, libraries, and tools that can be used Limiting the impact of garbage collection : The negative effects of process pauses can be mitigated without resorting to expensive real-time scheduling guarantees.   GC 같은 이유로 process가 멈추면 문제가 될 수 있다. 이를 해결하기 위해, response time 을 guarantee 하도록 개발하는것은 expensive 하고 restrict도 많아 비현실적이고, 대신 memory 사용률을 체크하는 것만으로도 GC로 인한 pause를 예방할 수 있다.\n \n8.4 Knowledge, Truth, and Lies  In the rest of this chapter, we will further explore the notions of knowledge and truth in distributed systems, which will help us think about the kinds of assumptions we can make and the guarantees we may want to provide.  8.4.1 The Truth is Defined by the Majority   Imagine a network with an asymmetric fault:\n a node is able to receive all messages sent to it, but any outgoing messages are dropped or delayed After some timeout, the other nodes declare it dead, the semi-disconnected node is dragged to the graveyard, kicking and screaming “I’m not dead!”— but since nobody can hear its screaming, the funeral procession continues with stoic determination.    The moral of these stories is that a node cannot necessarily trust its own judgment of a situation. =\u0026gt; Instead, many distributed algorithms rely on a quorum, that is, voting among the nodes\n  The leader and the lock : Frequently, a system requires there to be only one of some thing. (one node is allowed to be the leader)\n Implementing this in a distributed system requires care: even if a node believes that it is “the chosen one” that doesn’t necessarily mean a quorum of nodes agrees! A node may have formerly been the leader, but if the other nodes declared it dead in the meantime it may have been demoted and another leader may have already been elected. (Fig8-4)    Fencing tokens : When using a lock or lease to protect access to some resource, such as the file storage in Figure 8-4, we need to ensure that a node that is under a false belief of being “the chosen one” cannot disrupt the rest of the system. A fairly simple technique that achieves this goal is called fencing, and is illustrated in Figure 8-5.\n 어떠한 이유로 죽은 older token 의 소지자가 storage에 접근할 수 없도록 lease 를 몇번이나 해줬는지 알 수 있도록 기록\n   8.4.2 Byzantine Faults  Fencing tokens can detect and block a node that is inadvertently acting in error. However, if the node deliberately wanted to subvert the system’s guarantees, it could easily do so by sending messages with a fake fencing token. Byzantine fault : nodes may lie  if a node may claim to have received a particular message when in fact it didn’t.   This concern is relevant in certain specific circumstances. For example:  in aerospace environments, the data in a computer’s memory or CPU register could become corrupted by radiation, multiple participating organizations, some participants may attempt to cheat or defraud others.   Weak forms of lying : it can be worth adding mechanisms to guard against weak forms of lying  8.4.3 System Model and Reality   Many algorithms have been designed to solve distributed systems problems—in Chapter 9.\n  We somehow formalize the kinds of faults that we expect to happen in a system. We do this by defining a system model, which is an abstraction that describes what things an algorithm may assume.\n  With regard to timing assumptions, three system models are in common use\n Synchronous model : assumes bounded network delay, bounded process pau‐ses, and bounded clock error Partially synchronous model : behaves like a synchronous system most ofthe time, but it sometimes exceeds the bounds for network delay, process pauses,and clock drift : realistic model Asynchronous model : is not allowed to make any timing assumptions    Consider node failures.\n Crash-stop faults : node may suddenly stop responding at any moment, and thereafter that node is gone forever Crash-recovery faults : nodes may crash at any moment, and perhaps start responding again after some unknown time. Byzantine (arbitrary) faults : Nodes may do absolutely anything, including trying to trick and deceive other nodes,    Correctness : An algorithm is correct in some system model if it always satisfies its properties in all situations that we assume may occur in that system model.\n  If we are generating fencing tokens for a lock we may require the algorithm to have the following properties:\n Uniqueness: No two requests for a fencing token return the same value. Monotonic sequence : If request x returned token tx, and request y returned token ty, and x completed before y began, then tx \u0026lt; ty. Availability: A node that requests a fencing token and does not crash eventually receives a response.    Safety and liveness : uniqueness and monotonic sequence are safety properties, but availability is a liveness property\n  Mapping system models to the real world : when implementing an algorithm in practice, the messy facts of reality come back to bite you again, and it becomes clear that the system model is a simplified abstraction of reality.\n   이러한 property들을 바탕으로 system model 의 correctness를 판단 =\u0026gt; 현실적으로는 어렵지만, 놓치기 쉬운 문제들을 찾아내는데 도움.\n Summary   In this chapter, we have discussed a wide range of problems that can occur in distributed systems, including:\n Sended packet over the network, may be lost or delayed. A node’s clock may be significantly out of sync with other node A process may pause for a substantial amount of time at any point in its execution    we try to build tolerance of partial failures into software, so that the system as a whole may continue functioning even when some of its constituent parts are broken.\n  To tolerate faults, the first step is to detect them,\n most distributed algorithms rely on timeouts to determine whether a remote node is still available.    Once a fault is detected, making a system tolerate is not easy either: no global variable, no shared memory, no common knowledge\n  This chapter has been all about problems, and has given us a bleak outlook. In the next chapter we will move on to solutions, and discuss some algorithms that have been designed to cope with all the problems in distributed systems.\n  ","id":6,"section":"Engineering","summary":"8.0 Introduction In the last few chapters =\u0026gt; how systems handle things going wrong However, even though we have talked a lot about faults, the last few chapters have still been too optimistic The reality is even darker we will now turn our pessimism to the maximum and assume that anything that can go wrong will go wrong In this chapter we will get a taste of the problems that","tags":null,"title":"Data Intensive App #8 | The trouble with distributed system","uri":"https://koreanbear89.github.io/engineering/5.-study/data-intensive-app-08/","year":"2021"},{"content":"Setup Jupyter-Lab $ conda install -c conda-forge jupyter jupyterlab nbconvert # nodejs\u0026gt;12.0.0 needed $ conda install -c conda-forge nodejs $ conda install -c conda-forge/label/gcc7 nodejs $ conda install -c conda-forge/label/cf201901 nodejs $ conda install -c conda-forge/label/cf202003 nodejs # v13.10 $ conda update nodejs # v16.13 # if above not workconda uninstall --force nodejs $ conda uninstall --force nodejs # remove all $ conda install nodejs -c conda-forge --repodata-fn=repodata.json pip install jupyterlab-unfold $ pip install jupyterlab-unfold # theme $ pip install theme-darcular $ jupyter labextension install @arbennett/base16-nord # theme    ","id":7,"section":"Engineering","summary":"Setup Jupyter-Lab $ conda install -c conda-forge jupyter jupyterlab nbconvert # nodejs\u0026gt;12.0.0 needed $ conda install -c conda-forge nodejs $ conda install -c conda-forge/label/gcc7 nodejs $ conda install -c conda-forge/label/cf201901 nodejs $ conda install -c conda-forge/label/cf202003 nodejs # v13.10 $ conda update nodejs # v16.13 # if above not workconda uninstall --force nodejs $ conda uninstall --force nodejs # remove all $ conda install nodejs -c conda-forge --repodata-fn=repodata.json pip install jupyterlab-unfold $ pip install jupyterlab-unfold # theme $ pip install theme-darcular $ jupyter labextension install @arbennett/base16-nord # theme    ","tags":null,"title":"CheatSheet | Jupyter","uri":"https://koreanbear89.github.io/engineering/1.-tools/cheatsheet-jupyter/","year":"2021"},{"content":"\nSummary - PR # 1. Fork repo to your own repo # 2. clone, set remote $ git remote # 현재 프로젝트에 등록된 리모트 저장소를 확인 $ git remote add \u0026lt;remote_name\u0026gt; \u0026lt;URL\u0026gt; # fork 해온 원래의 remote 추가 # 3. branch 생성 $ git checkout -b \u0026lt;feature/issue_number\u0026gt; # create new branch $ git branch # Show all branches # 4. 수정 작업 후 add, commit, push # 4.1 add $ git add * # add changes to index $ git status # Check git status (staged/unstaged) $ git reset HEAD \u0026lt;FILE\u0026gt; # cancle add, unclearstage file. unstage all w/o \u0026lt;FILE # 4.2 commit $ git commit -m \u0026quot;explain this commit\u0026quot; # commit $ git commit --amend -m \u0026quot;modify commit message\u0026quot; $ git log # show commit log # 4.3 push $ git push \u0026lt;remote_name\u0026gt; \u0026lt;branch_name\u0026gt; # push to own remote repo # git push origin dev # 5. Pull Request # 6. Code Review # 7. Merg # 7.1 - Squash and Merge : PR 안에 있는 여러개 commit 을 새로운 하나의 commit 으로 =\u0026gt; commit message를 새로 써줘야함 # 7.2 - Rebase and Merge : PR 안에 여러개 commit이 있으면 여러개 commit을 그대로 merge  0. Setup Git Environment # git user check $ git config --list $ git config --global --list # Add user only for this project $ git config user.name \u0026quot;Your Name\u0026quot; $ git config user.email you@example.com # Add user for global usage $ git config --global user.name \u0026quot;Your Name\u0026quot; $ git config --global user.email you@example.com  1. Create new git # (1) create new git $ git init # in the working dir, generates .git directory $ git clone \u0026lt;URL\u0026gt; # 저장소를 clone하면 origin이라는 remote가 자동으로 등록 $ git clone --recursive \u0026lt;URL\u0026gt; # 저장소 하위 submodule까지 recursive하게 clone $ git clone -b \u0026lt;BRANCH\u0026gt; \u0026lt;URL\u0026gt; # branch를 따로 지정하지 않으면 master banch clone # (2) Add remote git $ git remote # 현재 프로젝트에 등록된 리모트 저장소를 확인 $ git remote add \u0026lt;remote_name\u0026gt; \u0026lt;URL\u0026gt; # remote 추가  2. Modify : Add, Commit, Push # (1) add $ git add * # add changes to index $ git status # Check git status (staged/unstaged) $ git reset HEAD \u0026lt;FILE\u0026gt; # cancle add, unclearstage file. unstage all w/o \u0026lt;FILE # (2) commit $ git commit -m \u0026quot;explain this commit\u0026quot; # commit $ git log # show commit log # (3) push $ git push \u0026lt;remote_name\u0026gt; \u0026lt;branch_name\u0026gt; # changes from local to server # (4) pull $ git pull # 필요한 파일을 다운로드, 병합 $ git fetch # 필요한 파일을 다운로드만  3. Branch # (0) Show all branches $ git branch # (1) create new local branch $ git branch \u0026lt;BRANCH\u0026gt; # (2) change to the new branch $ git checkout \u0026lt;BRANCH\u0026gt; # (3) Delete branch # (3.1) Delete local branch $ git branch -d \u0026lt;BRANCH\u0026gt; # (3.2) Delete remote branch $ git push origin --delete \u0026lt;BRANCH\u0026gt;  4. gitignore File # 로 시작하는 줄은 주석에 해당한다. # 'file.ext' 이라는 이름의 파일을 ignore 처리한다 file.ext # ignore 규칙을 정의하는 줄에 주석을 함께 섞어 쓰는 것은 허용되지 않는다 # 아래 줄은 'file.ext # not a comment' 라는 이름의 파일을 ignore 처리할 것이다 file.ext # not a comment # 전체 경로를 통해 파일 ignore 처리하기 # 파일명만 기술된 규칙은 최상위 디렉토리뿐만 아니라 모든 서브디렉토리에 동일하게 적용된다 # 예) otherfile.ext 파일은 하부의 디렉토리 어디에 있던 모두 ignore 처리될 것이다 dir/otherdir/file.ext otherfile.ext # 디렉토리 전체를 ignore 처리하기 # 디렉토리 자체와 디렉토리 내의 모든 내용물들이 ignore 처리된다 bin/ gen/ # Glob 패턴 형식을 사용하여 특정 문자를 포함하는 경로를 ignore 처리할 수 있다 # 예를 들어, 아래의 규칙은 build/ 와 Build/ 두가지 경로 모두에 해당한다 [bB]uild/ # / 로 끝나지 않는 경로의 경우에는, 해당 규칙이 기술된 이름을 갖는 파일과 디렉토리 모두에 적용된다 # 따라서, 아래 예제는 `gen` 이라는 이름을 가진 파일들 뿐만 아니라 # 동일한 이름의(`gen`) 디렉토리 및 해당 디렉토리의 내용물까지 모두 ignore 처리하게 된다 bin gen # 파일을 확장자 별로 ignore 처리하기 # 아래 기술된 확장자를 갖는 현재 디렉토리와 # 모든 서브디렉토리 내의 파일들이 ignore 처리될 것이다 *.apk *.class # 특정 디렉토리 지정과 특정 확장자 지정 규칙을 혼합하여 사용하는 것도 가능하다 # 아래에 기술된 규칙이 ignore 처리할 파일들은 # 위에서 지정한 규칙들이 ignore 처리하는 파일들과 중복된다 java/*.apk gen/*.class # 최상위 디렉토리에 존재하는 파일을 ignore 처리하되, # 서브디렉토리 내의 동일한 이름을 갖는 파일들은 제외하고 싶다면 `/` 를 앞에 붙인다 /*.apk /*.class # DirectoryA 라는 이름의 디렉토리가 저장소 내 어떤 위치에 존재하던 # 모두 ignore 처리하고 싶다면 ** 를 앞에 붙인다 # / 를 마지막에 붙이는 것을 잊지 말아야 한다 # 그렇지 않으면 DirectoryA 라는 이름의 디렉토리 뿐만 아니라 파일들까지 ignore 처리하게 된다 **/DirectoryA/ # 이 규칙은 다음 경로들을 ignore 처리할 것이다: # DirectoryA/ # DirectoryB/DirectoryA/ # DirectoryC/DirectoryB/DirectoryA/ # 이 규칙은 DirectoryA 라는 이름의 파일을 ignore 처리하지는 않는다 (해당 파일이 어느 위치에 존재하든 무관) # DirectoryA 라는 이름의 디렉토리 하부에 존재하는 DirectoryB 디렉토리를 ignore 처리하되 # 두 디렉토리 사이에 몇 단계의 다른 경로가 포함되어도 상관없이 ignore 하고 싶다면, # 두 디렉토리 경로 사이에 ** 문자열을 포함시켜 규칙을 작성할 수 있다 DirectoryA/**/DirectoryB/ # 이 규칙은 다음 경로들을 ignore 처리할 것이다: # DirectoryA/DirectoryB/ # DirectoryA/DirectoryQ/DirectoryB/ # DirectoryA/DirectoryQ/DirectoryW/DirectoryB/ # 위에서 이미 살펴 보았듯이, 특정 파일들을 한꺼번에 ignore 처리하리 위해서 규칙 명세에 와일드카드를 이용할 수 있다 # 이 때, '*' 하나를 단독으로 명시할 경우, .gitignore 까지 포함하여 폴더 내의 모든 파일들이 (의도치 않게) ignore 처리되게 된다 # 이렇듯 와일드카드를 사용하되 특정 파일은 ignore 처리하지 않으려면, ! 를 이용하여 예외 처리를 할 수 있다 # 이렇게 예외처리로 명시된 파일은 ignore 목록에서 제외될 것이다: !.gitignore # 파일 이름 중에 해시(#) 문자가 존재할 경우, 백슬래시를 escape 문자로 이용하여 표기할 수 있다 # (1.6.2.1 버전 이후부터 지원) \\#*#  [Reference] https://rogerdudler.github.io/git-guide/index.ko.html)\n","id":8,"section":"Engineering","summary":"Summary - PR # 1. Fork repo to your own repo # 2. clone, set remote $ git remote # 현재 프로젝트에 등록된 리모트 저장소를 확인 $ git remote add \u0026lt;remote_name\u0026gt; \u0026lt;URL\u0026gt; # fork 해온 원래의 remote 추가 # 3. branch 생성 $ git checkout -b \u0026lt;feature/issue_number\u0026gt; # create new branch $","tags":null,"title":"CheatSheet | Git","uri":"https://koreanbear89.github.io/engineering/1.-tools/cheatsheet-git/","year":"2021"},{"content":"4.0 Introduction  Applications inevitably change over time. (Features are added or modified as new products are launched) A change to an app\u0026rsquo;s features also requires a change to data that it stores. This means that old and new versions of the code, and old and new data formats may potentially all coexists in the system at the same time.  Backward Compatibility : Newer code can read data written by older code =\u0026gt; Easy Forward Compatibility : Older code can read data written by newer code =\u0026gt; can be tricker =\u0026gt; requires older code to ignore additions made by newer code   In this chapter,  We will look at several formats for encoding data We will then discuss how those formats are used for data storage and for communication    \n4.1 Formats for Encoding Data   Programs usually work with data at least two differenct representations :\n In memory data : is kept in object, structs, lists, array, hash tables, trees, and so on. Encoded data : When you want to write data to a file or send it over the network, you have to encode it as some kind of self-contained sequence of bytes (e.g. JSON)    Thus, we need some kind of translation between the two representations\n Encoding (Serialization or Marshalling) : translation from the in-memory representation to a byte sequence  Language Specific Formats JSON, XML Binary Variants   Decoding (Parsing, Deserialization, unmarshalling) : translation from a byte sequence to in-memory representation    4.1.1 Language Specific Formats   Many programming language come with built-in support for encoding in-memory objects into byte sequences (java.io.Serializable, Ruby Marshal, Python Pickle)\n  These are very convenient. However they also have a number of deep problems :\n  The encoding is often tied to a particular programming language, and reading the data in another language is very difficult.\n  In order to restore data in the same object types, the decoding process needs to be able to instantiate arbitrary classes : Security Problem\n Pickle docs : The pickle module is not secure. It\u0026rsquo;s possible to construct malicious pickle data which will execute arbitrary code during unpickling. https://docs.python.org/3/library/pickle.html\n   Versioning data is often an afterthought in these libraries : as they ared intended for quick and easy encoding of data =\u0026gt; neglect the inconvinient problems of forward and backward compatibility\n  Efficiency is also often an afterthought (CPU time taken to encode or decode, and the size of the encoded structure)\n     여기까지는 language specific formate 에 대하여 알아보았고 아래부터는 다양한 language에서 사용가능한 format인 json과 xml에 대하여 볼 예정\n 4.1.2 Textual Formats : JSON, XML and CSV  standardized encodings that can be written and read by many programming languages Widely known, widely supported, and alomst as wideley disliked  XML : is often criticized for being too verbose and unnecessarily complicated JSON : \u0026rsquo;s popularity is mainly due to its built-in support in web browsers CSV : is another popular language-independent format, albeitless powerful   JSON, XML, CSV are textual format, human-readable, but also have some subtle problems  ambiguity around the encoding of numbers : XML and CSV cannot distinguish between a number and a string, JSON distinguishes strings and numbers but it doesn\u0026rsquo;t distinguish integers and floating-point numbers =\u0026gt; source of a problem when dealing with large numbers JSON and XML have good support for Unicode character strings (i.e., human readable text), but they don\u0026rsquo;t support binary strings (ssequences of bytes without a character encoding). : so people get aroung this limitation by encoding the binary data as text using Base64 =\u0026gt; This works, but it\u0026rsquo;s somwhat hacky and increases the data size by 33% There is optional schema support for both XML[11] and JSON[12]. These schema languages are quite powerful and thus quite complicated to learn and implement. CSV does not have any schema, so it\u0026rsquo;s up to the application to define the meaning of each row and column.   Despite these flaws, JSON, XML, and CSV are good enough for many purposes. The difficulty of getting different organizations to agree on anything outweighs most other concerns =\u0026gt; It often doen\u0026rsquo;t matter how pretty or efficient the format is.  4.1.3. Binary Variants : Thrift, Protocol Buffers and Avro   For data that is used only internally within yout organization, there is less pressure to use a lowest-common-denominator encoding format =\u0026gt; more compact or faster to parse\n  Apache Thrift and Protocol Buffers (google) are binary encoding libraries on the same principle\n Define Schema : Both Thrift and Protocol Buffers require a schema for any data that is encoded. schema =\u0026gt; code generation tool =\u0026gt; produces classes that implement the schema in various programming language call this generated code to encode or decode records of the schema  // in Thrift, you would descirbe the schema in the Thrift interface definition language struct Person { 1: required string userName, 2: optional i64 favoriteNumber, 3: optional list\u0026lt;string\u0026gt; interests }    Binary Variants can handle schema changes (Schema Evolution) while keeping backward and forward compatibility\n  Avro : is another binary encoding format that is interestingly differenct from Protocol Buffers and Thrift =\u0026gt; started as a subproject of Hadoop, as a result of Thrift noy being a good fit for Hadoon\u0026rsquo;s use cases [21]\n  The Merits of Schemes : although textual data formats such as JSON, XML, and CSV are widespread, binary encodings based on schemas are also a viable option\n  \n4.2 Modes of DataFlow (p.128)  We said that whenever you want to send some data to another process with which you don\u0026rsquo;t share memory, you need to encode it as a sequence of bytes. We then discussed a variety of different encodings for doing this. In the rest of this chapter, we will explore some of the most common ways how data flows between processes :  Via database Via service calls Via asynchronous message passing    4.2.1 Dataflow Through Database   In a database, the process that writes to the database encodes the data, and the process that reads from the database deocdes it\n  **Backward Compatibility **: storing something in the DB =\u0026gt; should be readable in the future\n  Forward Compatibility : It\u0026rsquo;s common for several different processes to be accessing a DB at the same time\n A value in the DB may be written by a newer version of the code, and subsequently read by an older version of the code Be careful not to update data written by a newer version of the application : Figure 4-7    Different values written at different times\n application은 new-version으로 entire replace가 쉽지만 DB는 아님, 물론 rewriting data into a new schema 가 가능할수도있지만 expensive하고 이에따라 대다수 DB들은 null을 default로 새 column을 추가하는등의 schema change를 지원한다 결국 schema evolution이 5년전에 생성된 데이터든 신규 데이터든 single schema로 만들어진것처럼 하게한다.    4.2.2 Dataflow Through Services : REST and RPC   In some ways, services are similar to DB, they typically allow clients to submit and query data. However, these two are different in :\n DB allow arbitrary queries using the query language, Services expose an application-specific API that only allows inputs and outputs that are predetermined by the business logic of the service    A key design goal of a service-oritented/microservices architecture is to make the application easier to change and maintain by making services independently deployable and evolvable.\n =\u0026gt; we should expect old and new version of servers and clients to be running at the same time =\u0026gt; so the data encoding used by servers and clients must be compatible across versions of the service API    web service : when HTTP is used as the underlying protocol for talking to the service, it is called a web service. Two popular apporaches are: REST and SOAP\n REST : emphasizes simple data formats, using URLs for identifying resources and using HTTP features for cache control, authentication =\u0026gt; has been gaining popularity SOAP : aims to be independent from HTTP and avoids using most HTTP features, comes withs a sprawling and complex multitude of related standards =\u0026gt; has fallen out of favor in most companies.    The problems with RPCs (Remote Procedure Calls)\n  Technologies for making API requests over network : Web, EJB, RMI, DCOM\n  All of these are based on the idea of RPC\n  Although RPC seems convenient at first, the aproach is fundamentaly flawed.\n A local function call is predictable. A network request is unpredictable : the request or response may be lost due to a network problem. iIt normally takes about the same time to execute a local function. But a latency of network request is wildly variable You can efficiently pass references (pointers) of objects in local memory to a locla funcion. But for a network request, all those parameters need to be encoded into a sequence of bytes The client and the service may be implemented in different programming languages, so the RPC framework must translate datatypes from one language into another    Despite all these problems, RPC isn\u0026rsquo;t going away. =\u0026gt; Various RPC frameworks have been built on top of all the encodings, mentioned in this chapter.\n    Data encoding and evolution for RPC\n For evolvability, it is important that RPC clients and servers can be changed and deployed independently. The backward and forward compatibility properties of an RPC scheme are inherited from whatever encoding it uses:  Thrift, gRPC (Protocol Buffer), and Avro RPC can be evolved according to the compatibility rules of the respective encoding format In SOAP, requests and responses are specified with XML schemas. RESTful APIs most commonly use JSON (w/o a formally specified schema). Adding \u0026lsquo;optional\u0026rsquo; request parameters and adding new fields to respone objects are usually considered.      4.2.3 Message-Passing Dataflow   We will briefly look at asynchronous message-passing systems\n similar to RPC in that a client\u0026rsquo;s request is delivered to another process with low latency. the message is not sent via a direct network connection, but goes via an intermediary called a message broker (also called message queue)    Using a message broker has several advantages compared to direct RPC:\n  can act as a buffer if the recipient is unavailable or overloaded\n  can automatically redeliver messages to a process that has crashed =\u0026gt; prevent messages from being lost\n  avoids the sender needing to know the IP address and port number of the recipient\n  allows one message to be sent to several recipients\n  logically decouples the sender from the recipient (the sender just publishes messages and doesn\u0026rsquo;t care who consumes them).\n    Message Brokers : RabbitMQ, ActiveMQ, HornetQ, Apache Kafka =\u0026gt; compare them in more detail in Chapter11\n  \n4.3 Summary   We discussed several data encoding formats and their compatibiliry properties :\n Programming language specific encodings Textual formats like JSON, XML, and CSV Binary schema-driven formats like Thrift, Protocol Buffers, and Avro    We also discussed several modes of data flow\n Databases : the process writing to the database encodes the data and the process reading from the database decodes it RPC and REST APIs : the client encodes a request, the server decodes the request and encodes a response, and the client finally decodes the response Asynchronous message passing : communicate by sending each other messages that are encoded by sender and decoded by the recipient    We can conclue that with a bit of care, backward/forward compatibility and rolling upgrades are quite achievable.\n  ","id":9,"section":"Engineering","summary":"4.0 Introduction Applications inevitably change over time. (Features are added or modified as new products are launched) A change to an app\u0026rsquo;s features also requires a change to data that it stores. This means that old and new versions of the code, and old and new data formats may potentially all coexists in the system at the same time. Backward Compatibility : Newer code can read data written by older","tags":null,"title":"Data Intensive App #4 | Encoding And Evolution","uri":"https://koreanbear89.github.io/engineering/5.-study/data-intensive-app-04/","year":"2021"},{"content":"3.0 Storage and Retrieval   색인의 방식에 따라 다음 두가지로 나뉨\n log structured storage engine Page-oriented storage engine Other indexing structures    3.1 Data Structures That Power your Database  db_set Db_get : full table scan O(n) =\u0026gt; 색인의 필요성 색인 (index)  3.1.1 Hash Indexes\n conventional 한 느낌?  3.1.2 Log Structured Storage Engine : SSTables and LSM-Trees\n Sorted String Table : merge-sort 느낌, (figure 3-4) , 사전에서 handbag을 찾는다면 h로 먼저가서 찾음, 처음부터 하나씩 볼 필요가 없음 Log-Structured Merge Tree  3.1.3 Page oriented storage engine : B-Trees\n   3.1.4 Comparing B-Trees and LSM-Trees\n LSM은 쓰기가 빠르고 Btree는 읽기가 빠름   3.1.5 Other Indexing Structures\n  storing values within the index\n  multi-column indices\n  Full-text search and fuzzy indices\n  keeping everything in memory\n  3.2 Transaction Processing or Analytics 3.3 Column-Oriented Storage  Base  3.4 Summary  In this chapter we tried to get to the bottom of how database handle storage and retrieval. we saw that storage engine fall into twh broad categories:  OLTP (optimized for transaction processing) OLAP (optimized for analytics)    ","id":10,"section":"Engineering","summary":"3.0 Storage and Retrieval 색인의 방식에 따라 다음 두가지로 나뉨 log structured storage engine Page-oriented storage engine Other indexing structures 3.1 Data Structures That Power your Database db_set Db_get : full table scan O(n) =\u0026gt; 색인의 필요성 색인 (index) 3.1.1 Hash Indexes conventional 한 느낌? 3.1.2 Log Structured Storage Engine : SSTables and","tags":null,"title":"Data Intensive App #3 | Storage and Retrieval","uri":"https://koreanbear89.github.io/engineering/5.-study/data-intensive-app-03/","year":"2021"},{"content":"1.0 Introduction  Data-intensive (vs compute-intensive) : raw CPU power is out of interest, bigger problems are usually the amount of data, the complexity of data, and the speed at which it is changing  Database : store data so that they, or another application, can find it again later Caches : Remember the result of an expensive operation, to speed up reads Search Indexes : Allow users to search data by keyword or filter it in various ways Stream Processing : Send a message to another process, to be handled asynchronously Batch Processing: Periodically crunch a large amount of accumulated data    Figure 1-1. One possible architecture for a data system that combines several components. \n1.1 Reliability  Reliability : The system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware or software faults, and even human error)  Hardware Faults : 하드디스크 고장, 램 결함, 정전, 케이블 분리 등과 같은 하드웨어 결함, 대체로 독립적 Software Errors : 하드웨어와 달리 다수의 궝요서에 동시장애가 발생할 수 있음 Human Errors : 시스템을 설계하고 구축, 운영하는 과정에서 실수발생    \n1.2 Scalability  Scalability : As the system grows, there should be resasonable ways of dealing with that growh Describing Load : 부하기술하기 =\u0026gt; 시스템의 현재 부하를 간결하게 기술할 수 있어야 부하 성장을 논의할 수 있음 Describing Performance  response time : 응답시간을 활용한 현재 서버의 성능 기술하기 =\u0026gt; 대개 산술 평균을 많이 이용하지만 이보다는 백분위를 이용하는게 좋다 ( p50-중앙값, P999-응답시간이 가장 느린 요청 위주로 - 가장 많은 데이터를 갖고 있는 VIP일 확률이 높다 )   Approaches for Coping with Load - scaling up (vertical scaling): 사양이 좋은 장비로 업그레이드 - scaling out (horizontal scaling): 사양이 낮은 다수 장비로 부하를 분산  \n1.3 Maintainability  Maintainability : both maintaining current behavior and adapting the system to new use cases =\u0026gt; should be able to work on it productively  Operability ; 운영, 유지보수를 편리하게 Simplicity ; 프로젝트 전반의 복잡도 관리 Evolvability ; 요구사항에 맞춰 시스템 변경을 용이하게    1.4 Summary  In this chapter, we have explored some fundamental ways of thinking about data intensive applications. An Application has to meet various requirements in order to be useful. (Functional and nonfunction requiremnets)  Reliability means making systems work correctly, even when faults occur Scalability means having strategis for keeping performance good, even when load increases Maintainability has many facets, but in essence it\u0026rsquo;s about making life better for the engineering and operations terms who need to work with the system.    \n1.5 Appendix   DB read/write 비율 : 1.2절의 Describing Load 에서 response time 과 함께 DB의 read/write 도 중요한 load param\n  DB마다 조금씩 다르지만, read/write를 담당하는 인스턴스가 다르며\n Write 는 보통 primary (leader/master) 가 담당 Read 는 secondary (follower/slave) 가 담당    Write-Heavy 한 경우 primary가 여러개가 되도록 설계\n sharding을 적용하여 primary-secondary가 여러쌍 나오도록 sharding이란 =\u0026gt; 같은 테이블 스키마를 가진 데이터를 다수의 DB로 분산하여 저장 (DB 를 horizontal로 자름) e.g., shard1 : primary-secondary (id 1~100까지 저장) / shard2 : primary-secondary (id 101~200까지 저장)    Read-Heavy 한 경우 primary를 그대로 두고 많은 secondary로 replication\n sharding 없이 primary-N member secondary N member로 분산시켜 접속    Read/Write 가 heavy 하지 않터라도 replication/sharding\n Read-heavy 관계없이 high availability 를 제공하기 위해 replication을 적용하는게 일반적 Write-heavy 하지 않아도 초기 데이터가 너무 많은경우 read performance 향상을 위해 sharding 가능      추천 서비스 사례\n 추천 서비스의 경우 read-heavy, write는 우리가 조절 가능 Redis 속도가 매우 빨라서 master-slave 3pair , 총 6대로 4000qps를 여유있게 처리 write는 ras/neo4j 가 병목이라 이쪽에 맞추고 있음. Redis - Telegraf (InfluxData사) - InfluxDB : Redis 에 붙여주면 telegraf 가 필요한 load param을 timeseries로 influxDB에 넣어줌    ","id":11,"section":"Engineering","summary":"1.0 Introduction Data-intensive (vs compute-intensive) : raw CPU power is out of interest, bigger problems are usually the amount of data, the complexity of data, and the speed at which it is changing Database : store data so that they, or another application, can find it again later Caches : Remember the result of an expensive operation, to speed up reads Search Indexes : Allow users to search data by","tags":null,"title":"Data Intensive App #1 | Reliable, Scalable, Maintainable Applications","uri":"https://koreanbear89.github.io/engineering/5.-study/data-intensive-app-01/","year":"2021"},{"content":"0. Introduction  reference. https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf Quick Example : 한/영/숫자/- 를 포함하려면 '[가-힣|a-z|0-9|\\-]+'  1. Special Characters  ^ | Matches the expression to its right at the start of a string. It matches every such instance before each \\n in the string. $ | Matches the expression to its left at the end of a string. It matches every such instance before each \\n in the string. . | Matches any character except line terminators like `\\n`` ``` | Escapes special characters or denotes character classes A|B | Matches expression A or B. If A is matched first, B is left untried + | Greedily matches the expression to its left 1 or more times * | Greedily matches the expression to its left 0 or more times ? | Greedily matches the expression to its left 0 or 1 times. But if ? is added to qualifiers (+, *, and ? itself) it will perform matches in a non-greedy manner {m} | Matches the expression to its left m times, and not less {m,n} | Matches the expression to its left m to n times, and not less. {m,n}? | Matches the expression to its left m times, and ignores n. See ? above.  2. Character Classes (a.k.a. Special Sequences)  \\w | Matches alphanumeric characters, which means a-z, A-Z, and 0-9. It also matches the underscore, _. \\d | Matches digits, which means 0-9. \\D | Matches any non-digits. \\s | Matches whitespace characters, which include the \\t, \\n, \\r, and space characters. \\S | Matches non-whitespace characters. \\b | Matches the boundary (or empty string) at the start and end of a word, that is, between \\w and \\W. \\B | Matches where \\b does not, that is, the boundary of \\w characters. \\A | Matches the expression to its right at the absolute start of a string whether in single or multi-line mode. \\Z | Matches the expression to its left at the absolute end of a string whether in single or multi-line mode.  3. Sets  [ ] | Contains a set of characters to match. [amk] | Matches either a, m, or k. It does not match amk. [a-z] | Matches any alphabet from a to z. [a\\-z] | Matches a, -, or z. It matches - because \\ escapes it. [a-] | Matches a or -, because - is not being used to indicate a series of characters. [-a] | As above, matches a or -. [a-z0-9] | Matches characters from a to z and also from 0 to 9. [(+*)] | Special characters become literal inside a set, so this matches (, +, *, and ). [^ab5] | Adding ^ excludes any character in the set. Here, it matches characters that are not a, b, or 5.  4. Groups   ( ) | Matches the expression inside the parentheses and groups it.\n  (? ) | Inside parentheses like this, ? acts as an extension notation. Its meaning depends on the character immediately to its right.\n  (?PAB) | Matches the expression AB, and it can be accessed with the group name.\n  (?aiLmsux) | Here, a, i, L, m, s, u, and x are flags:\n a — Matches ASCII only i — Ignore case L — Locale dependent m — Multi-line s — Matches all u — Matches unicode x — Verbose    (?:A) | Matches the expression as represented by A, but unlike (?PAB), it cannot be retrieved afterwards.\n  (?#...) | A comment. Contents are for us to read, not for matching.\n  A(?=B) | Lookahead assertion. This matches the expression A only if it is followed by B.\n  A(?!B) | Negative lookahead assertion. This matches the expression A only if it is not followed by B.\n  (?\u0026lt;=B)A | Positive lookbehind assertion. This matches the expression A only if B is immediately to its left. This can only matched fixed length expressions.\n  (?\u0026lt;!B)A | Negative lookbehind assertion. This matches the expression A only if B is not immediately to its left. This can only matched fixed length expressions.\n  (?P=name) | Matches the expression matched by an earlier group named “name”.\n  (...)\\1 | The number 1 corresponds to the first group to be matched. If we want to match more instances of the same expresion, simply use its number instead of writing out the whole expression again. We can use from 1 up to 99 such groups and their corresponding numbers.\n  5. Popular Python re Module Functions  re.findall(A, B) | Matches all instances of an expression A in a string B and returns them in a list. re.search(A, B) | Matches the first instance of an expression A in a string B, and returns it as a re match object. re.split(A, B) | Split a string B into a list using the delimiter A. re.sub(A, B, C) | Replace A with B in the string C.  ","id":12,"section":"Engineering","summary":"0. Introduction reference. https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf Quick Example : 한/영/숫자/- 를 포함하려면 '[가-힣|a-z|0-9|\\-]+' 1. Special Characters ^ | Matches the expression to its right at the start of a string. It matches every such instance before each \\n in the string. $ |","tags":null,"title":"CheatSheet | Regex","uri":"https://koreanbear89.github.io/engineering/1.-tools/210802-cheatsheet-regex/","year":"2021"},{"content":"Introduction   Video summarization : is the process of distilling a raw video into a more compact form without losing much information.\n  Can be categorized into two forms :\n  Static Video Summarization (key framing, storyboard) : Static video summaries are composed of a set of keyframes extracted from the original video\n storyboards are not restricted by timing or synchronization issues and, therefore, they offer more flexibility in terms of data organization for browsing and navigation purposes    Dynamic Video Summarization (video skimming ) : dynamic video summaries are composed of a set of shots (fragments) and are produced taking into account the similarity or domain-specific relationships among all video shots.\n the ability to include audio and motion elements that offer a more natural story narration      Can be categorized :\n  Feature-based Methods : Feature-based video summarization techniques are classified on the basis of motion, color, dynamic contents, gesture, audio-visual, speech transcripts, objects, etc. These techniques work well if a user wants to focus on the features of the video.\n  Clustering-based Methods : Clustering is the most frequently used technique when we encounter similar characteristics or activities within a frame. It also helps to eliminate those frames that have irregular trends. Other methods for video summarization enable a more efficient way of browsing video but also create summaries that are either too long or confusing. Video summarization based on clustering is classified into similar activities, K-means, partitioned clustering, and spectral clustering.\n  Bag of Importance Model : A video can be viewed as a collection of weighted features instead of equally-important ones. The BoI model provides a mechanism to exploit both inter-frame and intra-frame properties by quantifying the importance of the individual features representing the whole video. The representative frames hence can be identified by aggregating the weighted features. It’s very reasonable to assume that a video sequence in its raw feature space is a dense manifold. In order to remove redundant visual features, a video sequence needs to be projected to a low-dimensional sparse space. The locality-constrained linear coding method provides such a mechanism, which can take advantage of the manifold geometric structure to learn a nonlinear function in a high dimensional space/manifold, and locally embed the points on the manifold in a lower-dimensional space, expressed as the coordinates with respect to a set of anchor points.\n    Benchmark\n SumMe : The SumMe dataset is a collection of 25 videos that cover a variety of events (e.g. sports, holidays, etc.) The videos in SumMe are 1.5 to 6.5 minutes in length. TVSum : The TVSum dataset contains 50 YouTube videos of 10 different categories. The videos in this dataset are typically 1 to 5 minutes in length.    \nConventional Approaches VSUMM (2011)   Introduction : Video SUMMarization, a methodology for the production of static video summaries\n  Methods : based on color feature extraction from video frames and k-means clustering algorithm\n The original video is split into frames (sampling rate is fixed on 1frame/sec) Color features are extracted to form a color histogram in HSV color space The frames are grouped by k-means clustering algorithm and one frame per cluster is selected (key-frame) The key frames that are too similiar are eliminated    \nSupervised Learning Learning frame importance by modeling the temporal dependency among frames   Introduction : Early DL-based approaches cast summarization as a structured prediction problem and try to make estimates about the frames’ importance by modeling their temporal dependency. =\u0026gt; Simply extract visual features from each frame and model the temporal relation between them using LSTM.\n  Methods\n  During the training phase, the Summarizer gets the sequence of the video frames (visual features extracted from each frame) and the available ground-truth data that indicate the importance of each frame according to the users’ preferences.\n  These data are then used to model the dependencies among the video frames in time and estimate the frames’ importance\n  uses LSTM-like units to model variable range temporal dependency among video frames.\n  Frames’ importance is estimated using a multi-layer perceptron (MLP),\n    References\n (2016) Video Summarization with LSTM : The first proposed approach to this direction using LSTM (2018) Summarizing Videos with Attention : proposes a seq2seq network for video summarization. (2019) Learning Hierarchical Self-Attention for Video Summarization (2018) Video Summarization Using Fully Convolutional Sequence Networks : tackles video summarization as a semantic segmentation task where the input video is seen as a 1D-image (of size equal to the number of video frames) with K-channels that correspond to the K dimensions of the frames’representation vectors (2018) Extractive VideoSummarizerwith Memory Augmented Neural Networks : stores information about the entire video in an external memory and predicts each shot’s importance by learning an embedding space that enables matching of each shot with the entire memory information.    TODO : Pretrained Models, Benchmark Set\n  \nLearn frame importance by modeling the spatio-temporal structure of the video.   Introduction : To make better estimates for the importance of video frames/fragments, some techniques pay attention to both the spatial and temporal structure of the video.\n  Methods : Again, the Summarizer gets the sequence of the video frames and the available ground-truth data that indicate the importance of each frame according to the users’ preferences. But, extending the analysis pipeline of the previously described group of methods, it then also models the spatiotemporal dependencies among frames.\n  References\n (2019) Online video summarization : Predicting future to better summarize present (2019) Spatiotemporal Modeling and Label Dis-tribution Learning for Video Summarization (2019) Video Summarization Via Actionness Ranking (2020) A Novel Key-Frames Selection Framework for Comprehensive Video Summarization : CapsulesNet for Motion extraction \u0026amp; attention mechanism  \n  Learn summarization by fooling a discriminator (Adversarial Learning).  Introduction : Minimizing the distance between the machine-generated and the ground-truth summaries, using GANs Methods : The Summarizer (which acts as the Generator of the GAN) gets the sequence of the video frames and generates a summary by computing frame-level importance scores. Reference  (2019) DTR-GAN : Dilated Temporal Relational Adversarial Network for Video Summarization    \nUnsupervised Learning Learn Summarization by Fooling a Discriminator when trying to discriminate the original video (or set of key-frames) from a summary-based reconstruction of it.  Introduction : Due to the lack of GT for learning video summarization, most existing unsupervised approaches rely on the rule that a representative summary ought to assist the viewer to infer the original video content. Methods : utilize GANs to learn how to create a video summary that allows a good reconstruction of the original video  Summarizer : simply summarize input video by estimating the importance of each frame Reconstructor : reconstruct original video from summarized one Discriminator : outputs a score that quantifies the similarity between the original video and reconstructed one.   Reference  (2017) Unsupervised VideoSummarization with Adversarial LSTM Networks (2019) A stepwise, label-based approach for improving the adversarialtraining in unsupervised video summarization (2019) Cycle-SUM : Cycle-Consistent Adversarial LSTM Networks for Unsupervised VideoSummarization (2019) Discriminativefeature learning for unsupervised video summarization, (2020) Unsupervised Video Summarization via Attention-Driven Adversarial Learning, (2019) Unsupervised Video Summarization with AttentiveConditional Generative Adversarial Networks, (2019) Video Summarization by LearningFromUnpaired Data    \nLearn Summarization by reinforcement learning based on hand-crafted rewords   Introduction : Aiming to deal with the unstable training and the restricted evaluation criteria of GAN-based methods. Some unsupervised approaches perform summarization by targeting specific properties of an optimal video summary.\n  Methods : utilize the principles of reinforcement learning in combination with hand-crafted reward functions that quantify the existence of desired characteristics in the generated summary\n Summarizer : gets the sequence of the video frames and creates a summary by predicting frame-level importance scores Evaluator : is responsible to quantify the existence of specific desired characteristics with the help of hand-crafted reward functions    (2018) Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward\n  (2019) Enhanced Deep Video Summarization Network\n  ","id":13,"section":"Research","summary":"Introduction   Video summarization : is the process of distilling a raw video into a more compact form without losing much information.\n  Can be categorized into two forms :\n  Static Video Summarization (key framing, storyboard) : Static video summaries are composed of a set of keyframes extracted from the original video\n storyboards are not restricted by timing or synchronization issues and, therefore, they offer more flexibility in terms of data organization for browsing and navigation purposes    Dynamic Video Summarization (video skimming ) : dynamic video summaries are composed of a set of shots (fragments) and are produced taking into account the similarity or domain-specific relationships among all video shots.","tags":null,"title":"MLCV #12 | Video Summarization","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv12-video-summarization/","year":"2021"},{"content":"Groupby agg_functions = {'col1':'first', 'col2' : 'sum', 'col3' : lambda col: ' \u0026amp;\u0026amp; '.join(col), } df_new = df.groupby(df['id']).aggregate(agg_functions)  Filter df.iloc[[0,1,2,3,4,5]] # get rows by indices # not iloc() =\u0026gt; iloc[] # Filtering rows that contain either sub_str1 or sub_str2 df_new = df.loc[df['Column'].str.contains(\u0026quot;sub_str1|sub_str2\u0026quot;, case=False)]  Manipulate Row df.drop(i) # remove i-th row df.sort_values(by, ascending=True) # sort  ","id":14,"section":"Engineering","summary":"Groupby agg_functions = {'col1':'first', 'col2' : 'sum', 'col3' : lambda col: ' \u0026amp;\u0026amp; '.join(col), } df_new = df.groupby(df['id']).aggregate(agg_functions)  Filter df.iloc[[0,1,2,3,4,5]] # get rows by indices # not iloc() =\u0026gt; iloc[] # Filtering rows that contain either sub_str1 or sub_str2 df_new = df.loc[df['Column'].str.contains(\u0026quot;sub_str1|sub_str2\u0026quot;, case=False)]  Manipulate Row df.drop(i) # remove i-th row df.sort_values(by, ascending=True) # sort  ","tags":null,"title":"CheatSheet | Pandas","uri":"https://koreanbear89.github.io/engineering/1.-tools/210610-cheatsheet-pandas/","year":"2021"},{"content":"ajax HTTP request \u0026lt;script\u0026gt; $.ajax({url: \u0026quot;http://cgpu018-cvision:20012/vidcls?title=\u0026quot;+searchTitle, method : \u0026quot;GET\u0026quot;, datatype : \u0026quot;JSON\u0026quot;}) .done(function(data) { alert('Done');}) .fail(function(xhr, status, error){alert('Failed');}); }); \u0026lt;/script\u0026gt;  ","id":15,"section":"Engineering","summary":"ajax HTTP request \u0026lt;script\u0026gt; $.ajax({url: \u0026quot;http://cgpu018-cvision:20012/vidcls?title=\u0026quot;+searchTitle, method : \u0026quot;GET\u0026quot;, datatype : \u0026quot;JSON\u0026quot;}) .done(function(data) { alert('Done');}) .fail(function(xhr, status, error){alert('Failed');}); }); \u0026lt;/script\u0026gt;  ","tags":null,"title":"CheatSheet | HTML","uri":"https://koreanbear89.github.io/engineering/1.-tools/210609-cheatsheet-html/","year":"2021"},{"content":"0. Introduction   Language Modeling : 이전 단어들이 주어졌을 때 다음 단어를 예측하는 모델. 즉 가장 자연스러운 단어 시퀀스를 찾아내는 모델\n  Machine Translation (기계 번역) : 언어 모델로 두 문장을 비교하여 더 자연스러운 좌측의 문장을 출력\n$$P(\\text{나는 버스를 탔다}) \u0026gt; P(\\text{나는 버스를 태운다})$$\n  Spell Correction (오타 교정)\n$$P(\\text{달려갔다}) \u0026gt; P(\\text{잘려갔다})$$\n  Speech Recognition (음성 인식) : 좌측의 결과가 확률이 더 높으므로 인식 결과를 correction\n$$P(\\text{나는 메론을 먹는다}) \u0026gt; P(\\text{나는 메롱을 먹는다})$$\n    Topic Modeling : 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나로, 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법\n  \n1. Seq2Seq (2014)  Introduction : DNNs work well but they cannot be used to map sequences to sequences. Authors present a general end-to-end approach to sequence learning. Method : Simply using a multilayered LSTM to map the input sequence to a fixed dim vector (context vector) and then another deep LSTM to decode the target sequence from the vector   \n2. Attention Mechanism (Neural Machine Translation by Jointly Learning to Align and Translate, 2014)   Introduction : A potential issue with the encoder-decoder approach is that a NN needs to be able to compress all the necessary information of a source sentence into a fixed-length vector : context vector. \n  Method : Rather than using fixed context vector (last hidden state value of encoder), we can use encoder\u0026rsquo;s each state with current state to generate dynamic context vector\n Attention Weight : Simply adding (FC + Softmax) to encoder output, we can get weight values of each input words for output at time step $t$. Dynamic Context Vector : We can get context vector for each timestep $c_t$ by calculating weighted sum of $h_i$ (hidden state of encoder) and $s_i$ (attention weight, softmax). Teacher Forcing : If model made wrong prediction $y_t$ for time step $t$. This causes the model to be trained in the wrong way for time step $t+1$. So we passed GT as the nex/t input for timestep $t$ to decoder rather than wrong prediction.     Architecture : (1)+(2) 단순히 seq2seq의 Encoder에 FC+Softmax를 하나 붙이면 출력에 대한 각 입력 단어의 weight 를 얻을 수 있고 (3), 이를 바탕으로 매 timestep 마다 context vector를 새로 만들어 Decoder에 feeding\n    3. Transformer (Attention is all you need)   Introduction : propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely\n  Method : Self-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n  Encoder : Self Attention Layer (Scaled Dot-product Attention) + Feed Forward NN\n Word Embedding : word to 512-dim vector Generate key, query, value vector of $i_{th}$ word : $q_i, k_i, v_i$ by simply multiplying trainable weights, $W_K, W_Q, W_V$ Attention Score, $softmax(s_{ij})$ : by calculating $s_{ij} = q_i \\cdot k_j$ we can get score between $i_{th}$ word and other $j_{th}$ words. and scale the dot products by $\\frac{1}{\\sqrt{d}}$  We suspect that for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients   Weighted Sum : $output_1 = S_{11} v_1 + S_{12}v_2 + S_{13}v_3\u0026hellip;$    Decoder : Almost same architercture with Encoder\n In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence . This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation. above diffrence makes attention score changed like this : $s_{ij} = q_i \\cdot k_j (i\u0026lt;j)$ The decoder stack outputs a vector of floats. And we turn that into a word by the final Linear layer which is followed by a Softmax Layer.    Where does query, key, value comes from :\n The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values). The attention operation can be thought of as a retrieval process as well.    Positional Encoding (fixed, different from positional embedding) : no recurrence and no convolution in model, in order for the model to make use of the order of the sequence =\u0026gt; we must inject some information about the relative or absolute position of the tokens in the sequence.\n  Simple indexing : just created a new vector where every entry is its index number. =\u0026gt; exploding gradients, unstable training\n  Normalized indexing : Just divide everything by the largest integer so all of the values are in [0,1]\n  binarized indexing : Instead of writing say 35 for the 35th element, we could instead represent it via its binary form 100011. =\u0026gt; Our binary vectors come from a discrete function, and not a discretization of a continuous function\n  Sinusoidal Positional Encoding : find a way to make the binary vector a discretization of something continuous. (Vanilla transformer)\n  Learnable Positional Embedding : train the vectors in figure of binarized indexing (Vision Transformer)\n      References:\n neural networks - What exactly are keys, queries, and values in attention mechanisms? - Cross Validated Transformer는 이렇게 말했다, \u0026quot;Attention is all you need.\u0026quot; Master Positional Encoding : Part1      Self-Attention Layer    Handcrafted Positional Encoding    ","id":16,"section":"Research","summary":"0. Introduction Language Modeling : 이전 단어들이 주어졌을 때 다음 단어를 예측하는 모델. 즉 가장 자연스러운 단어 시퀀스를 찾아내는 모델 Machine Translation (기계 번역) : 언어 모델로 두 문장을 비교하여","tags":null,"title":"NLP #3 | Language Modeling ","uri":"https://koreanbear89.github.io/research/5.-natural-language/nlp-3-language-modeling/","year":"2021"},{"content":"Frequently used vim shortcuts Basic commands\n- a # write text to the cursor - A # write text at the end of the current line - i # write text to the cursor - I # write text to the front of the current line - ^ # move cursor to the front of the line - $ # move cursor to the back of the line - ESC # shift to command mode - x # in command mode, remove the text - u # in command mode, undo - :w # in command mode, save (write on the disk) - :q # in command mode, quit  - y # copy in visual mode - p # paste in visual mode  Commands for multiple windows\n- :split # split in horizontal - :vsplit # split in vertical - ctrl + w + w # move to next window - ctrl + w + p # move to previous window  Fold and Expand\n- :set foldmethod=indent - zi # toggle folding  Indent \u0026amp; Un-indent\n# insert mode - ctrl + d - ctrl + t # visual mode : if 1 or more lines are selected - \u0026lt; - \u0026gt;  Plugin Usage NerdTree\n:nerd + tab # turn on nerdtree ctrl+w+w # move focus between editor and file tree m + a # add a child file  ","id":17,"section":"Engineering","summary":"Frequently used vim shortcuts Basic commands\n- a # write text to the cursor - A # write text at the end of the current line - i # write text to the cursor - I # write text to the front of the current line - ^ # move cursor to the front of the line - $ # move cursor to the back of the line - ESC # shift to command mode - x # in command mode, remove the text - u # in command mode, undo - :w # in command mode, save (write on the disk) - :q # in command mode, quit  - y # copy in visual mode - p # paste in visual mode  Commands for multiple windows","tags":null,"title":"Cheat Sheet | VIM","uri":"https://koreanbear89.github.io/engineering/2.-linux/cheatsheet-vi/","year":"2021"},{"content":"Frequently Used Options # -s, --silent $ curl -s URL # -X, --request \u0026lt;command\u0026gt; Specify request command to use $ curl -X GET URL $ curl -X POST URL  ","id":18,"section":"Engineering","summary":"Frequently Used Options # -s, --silent $ curl -s URL # -X, --request \u0026lt;command\u0026gt; Specify request command to use $ curl -X GET URL $ curl -X POST URL  ","tags":null,"title":"Cheat Sheet | CURL","uri":"https://koreanbear89.github.io/engineering/2.-linux/cheatsheet-curl/","year":"2021"},{"content":"0. Introduction  Conventional Encoding : Integer encoding, Sparse Representation : One-hot encoding, Document Term Matrix Word Embedding (Dense Representation): Word2Vec, Glove, FastText Pretraind Word Embedding : ELMo, GPT, BERT    1. Conventional Encoding  Integer Encoding : 단어를 빈도수 순으로 정렬한 단어 집합을 만들고, 빈도수가 높은 순서대로정수를 부여하여 encoding 하는 방법 Sparse Representation : 주어진 text를 대부분의 값이 0인 벡터로 표현하는 방법 (One-hot Encoding) Dense Representation (Word Embedding) : one-hot vector와 달리 벡터의 차원을 사용자가 지정하여, 다양한 값으로 이루어진 정해진 차원의 벡터로 표현하는 방법    2. Bag of Words   Introduction : 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법입니다.\n  Methods :\n 우선, 각 단어에 고유한 정수 인덱스를 부여합니다. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.    DTM (Document-Term Matrix) : 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것을 말합니다. 쉽게 생각하면 각 문서에 대한 BoW를 하나의 행렬로 만든 것\n  TF-IDF (Term Frequency Inverse Doc Frequency) : 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법입니다. 사용 방법은 우선 DTM을 만든 후, 각 element에 IDF를 곱해 가중치를 부여합니다\n  TF : 특정 문서에서 특정단어의 등장횟수로 DTM과 동일\n  IDF : \u0026lsquo;특정 단어가 등장한 문서의 수 $(t)$ \u0026rsquo; 의 역수, 다만 총 문서의 수 $(n)$ 가 커질수록 IDF가 기하급수로 커지기에 log를 취해줌 , 각 문서에서 몇번씩 등장했는지는 관심 없고\n​ $$IDF = log(\\frac{n}{1+df(t)})$$\n    Class-based TF-IDF\n The goal of the class-based TF-IDF is to supply all documents within a single class with the same class vector If documents are not individuals but part of a larger collective, then it might be interesting to actually regard them as such by joining all docs in a class together    doc1 = \u0026quot;정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\u0026quot; doc2 = \u0026quot;소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\u0026quot; doc3 = \u0026quot;정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\u0026quot; # (1) 각 단어에 고유한 정수 인덱스를 부여하면 ('정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17) # (2) 각 인덱스의 위치에 단어 토큰의 등장 횟루를 기록 BoW1 \u0026gt;\u0026gt; [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0] BoW2 \u0026gt;\u0026gt; [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1] BoW3 \u0026gt;\u0026gt; [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1] # (3) DTM DTM \u0026gt;\u0026gt; [[1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1], [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1] ]  ~~~python from sklearn.feature_extraction.text import CountVectorizer # Convert a collection of text documents to a matrix of token counts vectorizer = CountVectorizer( min_df=0.1, # 10% 아래의 문서에서 출현한 단어 제외 max_df=0.9, # 90% 이상의 문서에서 출현한 단어 제외 ngram_range=(1,1), lowercase=True, tokenizer=lambda x:x.split()) corpus.iter_sent=False X = vectorizer.fit_transform(corpus) print(X.shape) ~~~    2. Word2Vec  Introduction : 위의 One-hot vector와 같은 sparse vector로는 각 단어의 유사성을 표현할 수 없는 한계가 존재, 이에 따라 단어간 유사성을 표현하기 위해 하나의 차원이 아닌 여러 차원에 단어의 의미를 분산하여 표현하는 대표적인 방법 Methods :  CBOW (Continuous Bag of Words): Simple Neural Net에 context word를 one-hot vector형태로 넣으면 center word 를 출력하도록 학습, Skip-Gram : CBOW와 반대로 작동 즉, center word로 나올 수 있는 context word를 추론.     CBOW(T) , Skip-Gram (B)    3. ELMo (Embeddings from Language Model, 2018)   Introduction : 기존의 임베딩 방법들은 학습할 때만 주변 맥락을 고려함. 즉, 학습이 끝난 뒤에는 단어와 임베딩 벡터가 1:1로 매칭되어 변하지 않음. 따라서 \u0026ldquo;River Bank\u0026rdquo; 와 \u0026ldquo;Bank Account\u0026rdquo; 의 bank는 전혀다른 의미를 갖고 있음에도 동일한 벡터로 임베딩되는 문제가 발생.\n  Method:\n Contextualized Word Embedding : ELMo는 학습과정 뿐 아니라 prediction(embedding) 을 할때도 맥락을 반영할 수 있도록 만들어짐 Bidirectional-LSTM : n개의 단어를 보고 n+1을 예측하는 순방향 LSTM 기반의 언어 모델과, n개의 단어를 보고 n-1번째 단어를 예측하는 역방향 LSTM에 기존의 embedding vector를 입력하여 맥락을 고려한 임베딩벡터를 새로 만듦      4. BERT (Bidirectional Encoder Representations from Transformers, 2018)   Introduction : introduce a new language representation model called BERT, that is designed to pre-train deep bi-directional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\n  Method:\n  Input Representation = Token Embeddings + Segment Embeddings + Position Embeddings\n [CLS] : 모든 sentence 의 첫번째 토큰은 [CLS]로 이 토큰은 전체 레이어를 다 거치고나면 token sequence의 결합된 의미를 함축, 여기에 간단한 classifier를 붙이면 classification task 해결가능, 분류문제 이외에는 무시  output을 다시 agg 해서 MLP에 붙이는 방법도 있겠으나 그 과정이 번거로우니 그냥 처음부터 [cls] 라는 토큰을 하나 같이 붙여놓고, MLP에는 얘만 붙어있으니 BackProp하는 과정에서 얘만 분류문제에 최적화되도록 학습되게 만들어둠   [SEP] : 후술할 Pretrain Task#2 와 같이 두개의 문장을 분리하기 위한 토큰 (Learnable Positional Embeddings) :    Architecture : BERT’s model architecture is a multi-layer bidirectional Transformer encoder (Only encoder part)\n  Pretrain : pre-train BERT using two unsupervised tasks\n  Task #1 Masked LM : 단어 중 일부를 (15%) [Mask] 토큰으로 바꾼뒤 [Mask] 토큰을 predict 하는 pretraining task를 수행\n  Task #2 Next Sentence Prediction : 단순히 두개 문장을 주고 연속된 문장인지 판단하도록 하는 Binarized Next Sentence Prediction Task\n    Finetune : For each sub-task, simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end\n    Reference\n    ","id":19,"section":"Research","summary":"0. Introduction Conventional Encoding : Integer encoding, Sparse Representation : One-hot encoding, Document Term Matrix Word Embedding (Dense Representation): Word2Vec, Glove, FastText Pretraind Word Embedding : ELMo, GPT, BERT 1. Conventional Encoding Integer Encoding : 단어를 빈도수 순으로 정렬한 단어 집합을 만들고, 빈도수가 높은 순서대로정수를 부","tags":null,"title":"NLP #2 | Language Representation ","uri":"https://koreanbear89.github.io/research/5.-natural-language/nlp-2-language-representations/","year":"2021"},{"content":"0. Interface  pyplot interface : Rely on pyplot to automatically create and manage the figures and axes, and use pyplot functions for plotting. (MATLAB-like interface) object-oriented interface : Explicitly create figures and axes, and call methods on them  import matplotlib.pyplot as plt import numpy as np # pyplot interface (MATLAB-like, rely on pyplot) # x = np.linspace(0,2,100) # plt.plot(x,x**2) # Recommended) object-oriented interface (create figures and axes manually) fig, ax = plt.subplots(figsize=(15,4)) ax.plot(x,x**2)   즉, matplotlib으로 plot을 하기위해 신경써야하는 객체는 Figure, Axes 둘 이고 디테일한 plot을 하기에는 object oriented interface를 활용\n \n1. Plot fig, ax = plt.subplots() ax.plot(x,x**2)  \n2. Axes fig, ax = plt.subplots() # set axes ax.set_title(\u0026quot;Title\u0026quot;) ax.set_xlabel('xlabel') # set limits ax.set_ylim([0, 1e5]) # rotate tick ax.tick_params(axis = 'x', labelrotation =90)  ","id":20,"section":"Engineering","summary":"0. Interface pyplot interface : Rely on pyplot to automatically create and manage the figures and axes, and use pyplot functions for plotting. (MATLAB-like interface) object-oriented interface : Explicitly create figures and axes, and call methods on them import matplotlib.pyplot as plt import numpy as np # pyplot interface (MATLAB-like, rely on pyplot) # x = np.linspace(0,2,100) # plt.plot(x,x**2) # Recommended) object-oriented interface (create figures and axes manually) fig, ax","tags":null,"title":"CheatSheet | Matplotlib","uri":"https://koreanbear89.github.io/engineering/1.-tools/210112-cheatsheet-matplotlib/","year":"2021"},{"content":"0. Introduction  일반적으로 NLP modeling 은 다음과 같은 방식으로 진행  Text Processing : 주어진 데이터 (Corpus) 를 필요에 따라 전처리하고 적합한 (단어) 단위로 Tokenization Text Representation : Word Embedding 과 같은 방법을 활용하여 자연어를 숫자형태로 바꾸어 표현 Modeling : 목적에 따라 ML Modeling    \n1. Tokenization   Tokenization : 주어진 corpus를 특정한 목적에 따라 정의한 단위인 token으로 나누는 작업.\n  Sentence Tokenization : 토큰의 단위가 문장인 경우. 온점(.) 는 일반적으로 문장의 boundary 역할을 하지만 \u0026lsquo;Ph.D.\u0026rsquo; 와 같이 abbreivation으로 사용되는 경우도 많음. 따라서 이를 구분하기 위한 binary classifier를 사용하기도 함.\n  Word Tokenization : 토큰의 기준이 단어인 경우. 영어는 띄어쓰기 단위로 자르면 단어토큰을 구분할 수 있으나 한국어의 경우 그렇지 않음.\n  Morpheme Tokenization : 토큰의 기준이 형태소인 경우. 한국어의 경우 다음과 같은 이유로 형태소 토큰화를 수행\n 조사의 존재 : 영어로는 \u0026lsquo;he\u0026rsquo; 라는 동일한 단어이나 국어에는 \u0026lsquo;그가\u0026rsquo; \u0026lsquo;그는\u0026rsquo; 과 같이 다양한 조사가 띄어쓰기 없이 바로 붙어 서로 다른 단어로 인식되기 쉬움. 따라서 한국어에서는 형태소 tokenization을 수행하기도 함. 띄어쓰기 부정확 : 영어와 비교하여 띄어쓰기가 잘 지켜지지 않는 경향이 있다. 띄어쓰기를 제대로 하지 않아도 이해하기 쉽고 띄어쓰기가 상대적으로 어렵기 때문.    Python Packages\n NLTK : 영어 토큰화 패키지 KoNLPY : 한국어 토큰화 패키지로 Okt, Mecal, Komoran, Hannanum, Kkma 와 같은 형태소 분석기를 제공.    from konlpy.tag import Okt okt = Okt() target = \u0026quot;오늘 아침 서울 기온 영하 12도로 어제보다 무려 6도나 낮았는데요\u0026quot; print(okt.morphs()) # \u0026gt;\u0026gt; ['오늘', '아침', '서울', '기온', '영하', '12', '도로', ... print(okt.pos()) # \u0026gt;\u0026gt; [('오늘', 'Noun'), ('아침', 'Noun'), ('서울', 'Noun') ... print(okt.nouns()) # \u0026gt;\u0026gt; ['오늘', '아침', '서울', '기온', '영하', '도로', ...  \n1.1 Word Piece Tokenization  TODO  2. Lemmatization and Stemming  Lemmatization (표제어 추출) : 서로 다른 형태를 갖는 단어들의 뿌리가 되는 단어를 찾아가 corpus 를 구성하는 단어의 개수를 줄임. e.g. \u0026ldquo;am\u0026rdquo;, \u0026ldquo;are\u0026rdquo;, \u0026ldquo;is\u0026rdquo; 의 표제어는 \u0026ldquo;be\u0026rdquo;. Stemming (어간 추출) : 정해진 규칙에 따라 (rule-based) 주어진 단어의 어미를 자름. 대표적으로는 poter algorithm이 있으며 nltk.stem.PorterStemmer 에 제공.  \n3. Stopword  Stopword (불용어) : I, my me , the, a 와 같이 자주 사용되지만 실제 의미 분석에는 큰 도움이 되지 않는 단어들로 직접 정의할 수도 있음  from nltk.corpus import stopwords print(stopwords.words('english')[:10]) # \u0026gt;\u0026gt; ['i', 'me', 'my', 'myself' ...  \n4. Regular Expression  Regular Expression : 특정 규칙을 갖는 문자열을 나타내기 위해 사용되는 일종의 패턴 파이썬에서는 re module 을 통해 아래와 같이 정규표현식 사용 가능.  import re text = \u0026quot;\u0026quot;\u0026quot;100 John PROF 101 James STUD 102 Mac STUD\u0026quot;\u0026quot;\u0026quot; # '\\s'는 공백을 나타내며 +는 하나 이상의 패턴을 의미. re.split('\\s+', text) # \u0026gt;\u0026gt; ['100', 'John', 'PROF', '101', 'James', 'STUD'..   RegexpTokenizer : nltk.tokenize.RegexpTokenizer 혹은 pyspark.ml.feature.RegexTokenizer 를 활용하여 정규표현식을 활용한 토큰화도 가능  from nltk.tokenize import RegexpTokenizer tokenizer=RegexpTokenizer(\u0026quot;[\\s]+\u0026quot;, gaps=True) print(tokenizer.tokenize(\u0026quot;Don't be fooled by the dark sounding name\u0026quot;) # \u0026gt;\u0026gt; [\u0026quot;Don't\u0026quot;, 'be', 'fooled', 'by', 'the', 'dark', ...  \n6. Text Preprocessing Tools for Korean Text  PyKoSpacing (pykospacing) : 띄어쓰기 관련 패키지 Py-Hanspell (hanspell) : 한글 맞춤법 검사 관련 패키지 SOYNLP (soynlp) : 품사태깅, 비지도학습 기반의 단어 토큰화를 바탕으로 신조어 혹은 형태소 분석기에 등록되지 않은 단어 토큰화에 유리 Customized KoNLPy : 형태소 분석기에 사용자 정의 단어를 추가하여 konlpy를 사용가능.  ","id":21,"section":"Research","summary":"0. Introduction 일반적으로 NLP modeling 은 다음과 같은 방식으로 진행 Text Processing : 주어진 데이터 (Corpus) 를 필요에 따라 전처리하고 적합한 (단어) 단위로 Tokenization Text Representation : Word Embedding 과 같은 방법을 활용하여 자연","tags":null,"title":"NLP #1 | Text Preprocessing","uri":"https://koreanbear89.github.io/research/5.-natural-language/nlp-1-text-preprocessing/","year":"2021"},{"content":"1. Initialization \u0026amp; Configuration   SparkContext : provides connection to Spark with the ability to create RDDs\n  SQLContext : procides connection to Spark with the ability to run SQL queries on data\n  SparkSession : all encompassing context which includes coverage for SparkContext, SQLContext and HiveContext\n  import pyspark from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql import SQLContext # create a SparkSession instance with the name 'appname' spark = SparkSession.builder.appName(\u0026quot;appname\u0026quot;).getOrCreate() # create a SparkContext instance which allows the Spark Application to access sc = SparkContext.getOrCreate() # create a SQLContext instance to access the SQL query engine built on top of Spark sqlContext = SQLContext(spark)  \n2. Dataframe Explore Dataframe\n# Prints out the schema in the tree format. df.printSchema() # To get the number of rows df.count()  Modifying Dataframe\n# Create a column df = df.withColumn('column_new', F.lit('abc')) # with the defalut value 'abc' df = df.withColumn('column_new', 2*F.col('exisisting column')) # using an existing column # Change Column Name df.withColumnRenamed(\u0026quot;column_ori\u0026quot;, \u0026quot;column_new\u0026quot;) # add index column from pyspark.sql.functions import monotonically_increasing_id df_index = df.select(\u0026quot;*\u0026quot;).withColumn(\u0026quot;id\u0026quot;, monotonically_increasing_id()) # filter (=where) df.filter(df.column_float.between(7.5,8.2)) # remove column df.drop(\u0026quot;column_drop\u0026quot;)  Join\n Inner join : essentially removes anything that is not common in both tables. It returns all data that has a match under the join condition(on expression is true) from both sides of the table Outer join : allows us to include in the result rows of one table for which there are no matching rows round in another table Left join : all rows of the left table remain unchanged, regardless of wheter there is a match in the right table or not. When a id match is found in the right table, it will be returned or null otherwise Right join : performs the same task as the left join, but for the right table.  df1.join(df2, on='column', how='inner')  Read and Write\n# create a SparkSession instance with the name 'appname' spark = SparkSession.builder.appName(\u0026quot;appname\u0026quot;).getOrCreate() # read (csv, json, text, parquet) df = spark.read.csv('PATH_CSV') # write df.coalesce(1).write.csv(\u0026quot;sample.csv\u0026quot;)  GroupedData\n  df.groupBy() returns GroupedData\n  GroupBy allows you to group rows together based off some column value\n  즉, groupBy는 column name을 입력으로 받는데, 이때 입력된 column에 동일한 value가 있는 row들 끼리 일단 group 해줌, 여기서 column을 필요에따라 자유롭게 변형하여 넣어줄 수 있고, 결과로 만들어진 group 역시 자유롭게 변형하여 원하는 출력을 얻을 수 있음\n  df.groupBy(\u0026quot;column_name\u0026quot;). .count() # Returns the count of rows for each group. .mean() # Returns the mean of values for each group. .max() # Returns the maximum of values for each group. .min() # Returns the minimum of values for each group. .sum() # Returns the total for values for each group. .avg() # Returns the average for values for each group. .agg() # Using agg() function, we can calculate more than one aggregate at a time. .pivot() # This function is used to Pivot the DataFrame  \n3. SQL functions from pyspark.sql import functions as F # Aggregate function: returns a set of objects with duplicate elements eliminated. F.collect_set() # Bucketize rows into one or more time windows given a timestamp specifying column. F.window(timeColumn='timestamp', windowDuration='20 minute') # Creates a Column of literal value. F.lit() # Replace all substrings of the specified string value that match regexp with rep. F.regexp_replace()  User Defined Function\n  input arguments for udf should be column name as you can see in line no.3 or you might get an TypeError: Invalid argument, not a string or column: 2 of type \u0026lt;class ‘int’\u0026gt;. For column literals, use ‘lit’, ‘array’, ‘struct’ or ‘create_map’ function\n# User Defined Functions def func(x): return x test_udf = F.udf(lambda x : func(x), ArrayType(StringType())) df_udf = df.withColumn('udf', test_udf('input_column_name'))    \n4. Others  $\u0026quot;colname\u0026quot; is converted to Column by SQLContext.implicits$.StringToColumn  # zeppelin plot def show(p): import io img = io.StringIO() p.savefig(img, format='svg') img.seek(0) print(\u0026quot;%html \u0026lt;div style='width:400px'\u0026gt;\u0026quot; + img.getvalue() + \u0026quot;\u0026lt;/div\u0026gt;\u0026quot;)  \nReferences   PySpark Cheat Sheet, TowardsDataScience\n  Introduction to PySpark join types\n  ","id":22,"section":"Engineering","summary":"1. Initialization \u0026amp; Configuration SparkContext : provides connection to Spark with the ability to create RDDs SQLContext : procides connection to Spark with the ability to run SQL queries on data SparkSession : all encompassing context which includes coverage for SparkContext, SQLContext and HiveContext import pyspark from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql import SQLContext # create a SparkSession instance with the name 'appname' spark = SparkSession.builder.appName(\u0026quot;appname\u0026quot;).getOrCreate()","tags":null,"title":"Cheat Sheet | Spark","uri":"https://koreanbear89.github.io/engineering/3.-spark/201211-cheatsheet-spark/","year":"2020"},{"content":"1. Prerequisite   Check java version first\n  Then, just install miniconda3 and create virtual environment based on python 3.6\njava -version # The command above might show something like below \u0026gt;\u0026gt; openjdk version \u0026quot;1.8.0_262\u0026quot; \u0026gt;\u0026gt; OpenJDK Runtime Environment (build 1.8.0_262-b10) \u0026gt;\u0026gt; OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode)    2. Download Spark # Download spark wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz # unzip tar -xvzf spark-3.0.1-bin-hadoop2.7.tgz # move to home and rename mv spark-3.0.1-bin-hadoop2.7 ~/spark  3. Install pyspark pip install pyspark  4. Change the execution path for pyspark export SPARK_HOME=\u0026quot;/your_home_directory/spark/\u0026quot; export PATH=\u0026quot;$SPARK_HOME/bin:$PATH\u0026quot;  5. Test $ pyspark  6. PySpark in Jupyter # add below lines in .bashrc export PYSPARK_DRIVER_PYTHON=jupyter export PYSPARK_DRIVER_PYTHON_OPTS='notebook'  $ pyspark # \u0026gt;\u0026gt; Now it will give you a url for jupyter notebook # \u0026gt;\u0026gt; [I 17:34:45.744 NotebookApp] Serving notebooks from local directory: /home/jupyter # \u0026gt;\u0026gt; [I 17:34:45.744 NotebookApp] Jupyter Notebook 6.1.5 is running at: # \u0026gt;\u0026gt; [I 17:34:45.744 NotebookApp] http://host:port/ # \u0026gt;\u0026gt; [I 17:34:45.744 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).  ","id":23,"section":"Engineering","summary":"1. Prerequisite   Check java version first\n  Then, just install miniconda3 and create virtual environment based on python 3.6\njava -version # The command above might show something like below \u0026gt;\u0026gt; openjdk version \u0026quot;1.8.0_262\u0026quot; \u0026gt;\u0026gt; OpenJDK Runtime Environment (build 1.8.0_262-b10) \u0026gt;\u0026gt; OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode)    2. Download Spark # Download spark wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz # unzip tar -xvzf spark-3.0.1-bin-hadoop2.7.tgz # move to home and rename mv spark-3.","tags":null,"title":"How to install pySpark locally","uri":"https://koreanbear89.github.io/engineering/3.-spark/201209-install-spark-locally/","year":"2020"},{"content":"1. Install Vundle $ git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim  2. color scheme setup  Jellybean color scheme official install And simply add line color jellybean in .vimrc  mkdir -p ~/.vim/colors cd ~/.vim/colors curl -O https://raw.githubusercontent.com/nanotech/jellybeans.vim/master/colors/jellybeans.vim  3. write .vimrc   wget https://raw.githubusercontent.com/jjeaby/jscript/master/.vimrc 로 preset을 받음\n  Added Plugin 'preservim/nerdcommenter'\n  4. vimrc에 설정한 플러그인 설치   :PluginInstall 로 설치  5. Shell # install Vundle git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim # setup jellybean color scheme mkdir -p ~/.vim/colors cd ~/.vim/colors curl -O https://raw.githubusercontent.com/nanotech/jellybeans.vim/master/colors/jellybeans.vim # copy .vimrc cp ~/configuration/vim/.vimrc ~/.vimrc vim -c 'PluginInstall' -c 'qa!'  Reference  Vim을 IDE처럼 사용하기  ","id":24,"section":"Engineering","summary":"1. Install Vundle $ git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim 2. color scheme setup Jellybean color scheme official install And simply add line color jellybean in .vimrc mkdir -p ~/.vim/colors cd ~/.vim/colors curl -O https://raw.githubusercontent.com/nanotech/jellybeans.vim/master/colors/jellybeans.vim 3. write .vimrc wget https://raw.githubusercontent.com/jjeaby/jscript/master/.vimrc 로 preset을 받음 Added Plugin 'preservim/nerdcommenter' 4. vimrc에 설정한 플러그인 설치 :PluginInstall 로 설","tags":null,"title":"How to setup vim","uri":"https://koreanbear89.github.io/engineering/2.-linux/how-to-set-up-vim/","year":"2020"},{"content":"Introduction  Structured Query Language, SQL 은 관계형 데이터베이스 관리 시스템(RDBMS)의 데이터를 관리하기 위해 설계된 특수 목적의 프로그래밍 언어이다.  Commands  SELECT : 데이터베이스에서 데이터를 선택하는데 사용된다 (column)  FROM : SELECT 문에서 사용할 데이터가 포함된 테이블 또는 쿼리를 지정합니다.   WHERE : 테이블 값을 필터링하는데 사용  BETWEEN : 조건 범위를 지정    // 특정 column만 가져오고 싶을 때 SELECT column1, column2, ... FROM table_name; // 모든 column을 가져오고 싶을때 SELECT * // 와일드카드를 사용 FROM table_name; // 특정 범위를 갖는 조건을 지정할떄 WHERE [column_name] BETWEEN [condition1] AND [condition2]  ","id":25,"section":"Engineering","summary":"Introduction Structured Query Language, SQL 은 관계형 데이터베이스 관리 시스템(RDBMS)의 데이터를 관리하기 위해 설계된 특수 목적의 프로그래밍 언어이다. Commands SELECT : 데이터베이스에서 데이터","tags":null,"title":"CheatSheet | SQL basic","uri":"https://koreanbear89.github.io/engineering/1.-tools/201207-cheatsheet-sql-basic/","year":"2020"},{"content":"\nIntroduction  docker docker compose Glossary  swarm : cluster 와 동일한 개념 node (manager/worker) : 클러스터에 속한 서버의 단위. 스웜 명령어는 매니저 노드에서만 실행된다 service : 각 프로젝트를 구성하고 있는 서비스 단위 정도, 기본적인 배포단위, 하나의 이미지를 기반으로 생성되고 복제 컨테이너를 여러개 실행가능 stack : 프로젝트의 단위 정도로 생각하면 될 듯, 하나의 스택으로 묶인 컨테이너들은 기본적으로 같은 overlay 네트워크에 속하게 된다     1. Install docker in centOS  Install Docker Engine on CentOS | Docker Documentation  # (1) Set up the repository $ sudo yum install -y yum-utils $ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # (2) Install Docker Engine $ sudo yum install docker-ce docker-ce-cli containerd.io   Post-installation steps for Linux | Docker Documentation  # (3) Create the docker group. # $ sudo groupadd docker # (4)Add your user to the docker group. # sudo usermod -aG docker irteam $ sudo /usr/sbin/usermod -aG docker irteam $ sudo /usr/sbin/usermod -aG docker irteamsu # (5) change root directory (storage for default docker directory is not enough) # add {\u0026quot;data-root\u0026quot;: \u0026quot;/home1/irteam/docker-data\u0026quot;} in /etc/docker/daemon.json $ sudo vim /etc/docker/daemon.json # (6) Run docker $ sudo systemctl start docker # if got permission error for /var/run/docker.sock $ sudo chmod 666 /var/run/docker.sock # check root directory $ docker info | grep Root $ docker run hello-world   2. Docker Commands  Dockerize an SSH service  # docker 버전확인하기 $ docker --version # 현재위치의 Dockerfile로 build 하기 $ docker build -t [TAG] . # 등록된 docker 이미지 확인하기 $ docker images # 실행중인 도커 머신 정보 보기 $ docker ps -a # 도커 머신 삭제 하기 $ docker rm -f [container name] # 도커 이미지 삭제 하기 $ docker rmi [image name] # 도커 머신 중지하기 $ docker stop [container name] # 도커 머신 시작하기 $ docker start [container name] # 도커 머신으로 파일 복사하기 $ docker cp [소스파일] [머신이름:디렉토리] # 도커 머신으로 부터 파일 가져오기 $ docker cp [머신이름:소스파일] [디렉토리] # 도커 머신으로 재접속하기 $ docker attach [머신이름] # 나오기 ctrl p q # 도커 머신 실행하기 $ docker run --dit -p 22022:22 --name [컨테이너이름] -v $(pwd):[/매핑할경로] --shm-size 4G [이미지이름] -d # backgorund -it --name # 컨테이너에 이름을 부여 --rm # 컨테이너가 종료될때 관련 리소스를 제거 $ docker exec -it [컨테이너이름] /bin/bash # root로 실행하기 $ docker exec -u 0 -it [컨테이너이름] bash # 도커 이미지 커밋/push $ docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] $ docker push [REPOSITORY[:TAG]] # 안쓰는 도커 컨테이너 정리 $ docker container prune # 안쓰는 도커 이미지 정리 $ docker image prune # building cache까지 모두 정리 $ docker system prune -a # show logs in container $ docker logs -f \u0026lt;CONTAINER_NAME\u0026gt; --tail=1000 2\u0026gt;\u0026amp;1 | grep complete   3. Dockerfile Instructions # base image를 지정 FROM ubuntu:16.04 # build 시에만 사용되는 변수 ARG PYVERSION # shell cmd 를 bin/sh -c 로 해당 docker image에 실행시킬때, # shell이 없는 platform에서 실행하려면 exec RUN [\u0026quot;apt-get\u0026quot;, \u0026quot;install\u0026quot;, \u0026quot;-y\u0026quot;, \u0026quot;nginx\u0026quot;] # Docker container 외부에 노출시킬 포트를 지정 할 때, container를 실행할때도 열어줘야함 EXPOSE 8080 # 환경변수를 지정할 때, 지정한 환경변수는 $variable_name으로 사용가능 ENV FOO /bar # docker container가 시작할 때 실행할 커맨드를 지정 CMD [\u0026quot;python\u0026quot;, \u0026quot;main.py\u0026quot;] #해당 도커 이미지를 실행할 user를 지정 USER nginx #호스트의 dir를 docker 컨테이너에 연결시킬 수 있다 VOLUME [\u0026quot;opt/project\u0026quot;] # 파일과 디렉토리를 호스트에서 docker image로 copy 한다 ADD file /some/dir/file # ADD와 기본적으로 동일하나 압축파일을 자동으로 풀어주지 않고, URL을 파일소스로 사용할 수 없다 COPY file /some/dir/file   4. Docker Compose $ docker-compose up -d # Docker Compose에 정의되어있는 모든 서비스 컨테이너를 한번에 생성하고 실행하기 위해 $ docker-compose down $ docker-compose stop [컨테이너] $ docker-compose ps  version: '3.7' services: my_service1: build: # if wanna build a image context: ./ dockerfile: ./Dockerfile image: # image name hostname: # host name tty: true # docker run -t container_name: \u0026lt;my_cont1\u0026gt; # container_name volumes: # mout volumes - ./src:/naver/shoppinglive/src networks: # 같은 networks 안의 다른 컨테이너에서는 my_cont1:3003으로 접속가능 # my_service1:3003 도 가능, service, container name 둘다 가능한듯 - shoppinglive # network를 추가하면 and 로 추가됨, 여기저기 다 가능해짐 ports: - 2003:3003 user: celery command: python -m black /naver/shoppinglive/src -t py39 depends_on: - black volumes: rabbitmq: driver: local redis: driver: local networks: shoppinglive:   5. Docker swarm  Server Orchestration  Scheduling : 컨테이너 여러개를 각 서버에 나누어 배포하고, 서버가 죽으면 실행중이던 서버를 다른 서버로 배포하여 서비스에 차질이 없도록 한다 Clustering : 여러개의 서버를 하나의 서버처럼 사용할 수 있게 해준다. 클러스터에 새로운 서버를 추가/제거 하여 여기 저기 흩어져 있는 컨테이너도 가상 네트워크를 이용, 같은 서버에 있는 것 처럼 쉽게 통신할 수 있게 해준다. Service Discovery : Logging, Monitoring   Why Docker Swarm?  API 서버를 구축하고 서버의 트래픽이 증가하여 서버 한대로 감당할 수 없을 때, 컨테이너를 구성하는 이미지가 업데이트 된다면 ? 현재 구동중인 모든 컨테이너를 지우고 docker-compose로 다시 새로운 컨테이너를 만들어야 할까 =\u0026gt; 도커 스웜의 롤링 업데이트 기능 스웜은 도커와 별도로 개발되어오다 1.12 버전부터 스웜모드라는 이름으로 합쳐졌다    # (1) init docker swarm $ docker swarm init # run on manager node # This will return the following command. To add a worker to this swarm, just run that command on the worker node \u0026gt; docker swarm join --token ............. # (2) deploy $ docker stack deploy --compose-file \u0026lt;docker-compose.yml\u0026gt; \u0026lt;STACK_NAME\u0026gt; # deploy using docker-compse.yml # manage the swarm node $ docker node ls # show all nodes joined to current node $ docker stack ls # show all stacks in current node (manager) $ docker service ls # show all services (including worker nodes) managed by current node $ docker service ps \u0026lt;SERVICE_NAME\u0026gt;   6. File sharing in docker container   Bind-mount : are files mounted from your host machine (the one that runs your docker daemon) onto your container.\n  Volume : are like storage spaces totally managed by docker. In fact, volumes are managed in the hidden(?) path of host machine such as \u0026lsquo;/var/lib/docker/volumes/VOLUME_NAME\u0026rsquo;\n  named volumes : you provide the name of it\n  anonymous volumes : usual UUID names from docker, like you can find them on container or untagged images\n    docker volume ls docker volume rm docker volume inspect VOLUME_NAME  ","id":26,"section":"Engineering","summary":"Introduction docker docker compose Glossary swarm : cluster 와 동일한 개념 node (manager/worker) : 클러스터에 속한 서버의 단위. 스웜 명령어는 매니저 노드에서만 실행된다 service : 각 프로젝트를 구성하고 있는 서비스 단위 정도,","tags":null,"title":"CheatSheet | Docker","uri":"https://koreanbear89.github.io/engineering/1.-tools/cheatsheet-docker/","year":"2020"},{"content":"Summary   3D object detection : classifies the object category and estimates oriented 3D bounding boxes of physical objects from 3D sensor data.\n  Applications : By extending prediction to 3D, one can capture an object’s size, position and orientation in the world, leading to a variety of applications in robotics, self-driving vehicles, image retrieval, and augmented reality\n  Benchmarks\n KITTI (car, cyclist, pedestrian easy, mod, hard) : for autonomous driving ScanNet : for indoor scene understanding task SUN-RGBD : Scene Understanding Benchmarks YCB-V : 21 objects from the YCB dataset captured in 92 videos with 133,827 frames.    Approaches based on Point Clouds (LiDAR)\n Projection based methods : projects point clouds to 2D planes such as bird view, front view, etc. + RPN Volumetric methods : voxelization + heavy 3D CNN Point-net based methods : extract features from raw data Graph based methods : construct graphs to learn the order-invariant of point clouds    Approaches based on monocular/stereo images\n Approaches based on cheaper monocular or stereo imagery data have resulted in drastically lower accuracies until now. A key ingredient for image-based 3D object detection methods is a reliable depth estimation approach to replace LiDAR. Existing algorithms are largely built upon 2D object detection, imposing extra geometric constraints to create3D proposals. apply stereo-based depth estimation to obtain the true 3D coordinates of each pixel.    Others\n Tools to read point clouds (.xyz) : CloudCompares, Mesh Lab, Gom Inspect kewords : 3D Obj Detection, 6DoF Pose Estimation    \n1. SUN-RGBD(2017)   An RGB-D benchmark suite for all major scene understanding tasks\n  Dataset is captured by four different sensors and contains 10,335 RGB-D images.\n  Densely annotated and includes 146k 2d polygons, and 64k 3D bboxes with accurate object orientations, as well as a 3D room layout and scene category for each images.\n  \n2. ScanNet(2017)  An RGB-D Video Dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations.  \n3. Pseudo LiDAR(2019)   Introduction : Mono/stereo depth estimation based methods show worse performance than LiDAR based methods because of poor precision of image based depth estimation.\n  Contrib : convert image-based depth maps to pseudo-LiDAR representations\n  Methods :\n  convert the estimated depth map from stereo or monocular imagery into a 3D point cloud, which we refer to as pseudo-LiDAR\n  then take advantage of ex-isting LiDAR-based 3D object detection pipelines\n    \n4. Realtime 3D Obj Detection on Mobile Dev with Media Pipe (Google, 2020)   Introduction : detects objects in 2D images, and estimates their poses and sizes through a ML model, trained on a newly created 3D dataset\n  Obtaining real world 3D Training Data : using ARCore, Google\u0026rsquo;s AR Platform, integrated in most smartphones, which can provide the information of the camera pose, sparse 3D point clouds, estimated lighting, and planar surfaces.\n  AR Synthetic Data Generation : places virtual objects into scenes that have AR session data.\n  An ML Pipeline : a single-stage model to predict the pose and physical size of an object from a single RGB image\n  \n5. SVGA-Net(2020)   Introduction : graph based methods for point clouds data, 2020 SOTA in KITTI datasets\n  Methods : Spars Voxel Graph Attention Network\n  \nBranch. LiDAR, ToF and Point Clouds  Introduction : LiDAR systems send out pulses of light just outside the visible spectrum and register how long it takes each pulse to return. Once the individual readings are processed and organised, the LiDAR data becomes point cloud data. A 3D point cloud is a collection of data points analogous to the real world in three dimensions. Each point is defined by its own position and (sometimes) colour. Photogrammetric point clouds : have an RGB value for each point, resulting in a colourised point cloud (acqutision using digital camera rather than LiDAR, worse than LiDAR in terms of accuracy). Difference between LiDAR and TOF : A ToF(most android phones) is a scannerless LiDAR system, relying on a single pulse of light to map an entire space, while Apple is using scanner LiDAR, which uses multiple points of light to take these readings much more frequently and with greater accuracy. LiDAR \u0026amp; TOF in mobile : LG G8 (2019) , Galaxy Note10, S20, IPAD pro 4, iphone12 , Huawei Mate30, Sony Xperia2    LG (얼굴인식, 제스처), 삼성 (사진촬영), Apple (AR).   \nBranch. AR Applications on smartphones   Google AR Core Depth API : Single Camera Depth Estimation\n  IKEA Place : allows the user to directly insert furniture into a picture of the room\n  modiFace : virtual beauty counter\n  measure kit : AR Ruler\n  pixie : find lost key through the iPhone screen. (with Bluetooth hardware)\n  magic sudoku : Solve sudoku puzzles in AR\n  \nBranch. Stereo/monocular image based depth Estimation   Benchmarks\n NYU Depth (Mono) : includes 1449 densely labeled pairs of aligned RGB and depth images KITTI Eigen Split (Mono) : part of the KITTI dataset proposed by D.Eigen et al. for monocular depth estimation task    BTS (monocular) : a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase.\n  DORN (monocular) : combine multi-scale features with ordinal regression to predictpixel depth with remarkably low errors\n  PSMNet : applies Siamese networks for disparity estimation, followed by 3D convolutions for refinement\n  Y Wang et al. : has made these methods more efficient, enabling accurate disparity estimation to run at 30 FPS on mobile devices.\n  \nBranch. 3D Modeling  IKEA 3D Models : includes a lot of simple CAD models, but there\u0026rsquo;s no additional information required for detection like RGBD, point clouds, etc. These 3D Models can be used to estimate the 3D pose of specific objects in image ( Lim et al. ICCV2013 )    3D model을 기반으로 영상에서 특정한 타겟 object를 detection 하는 수준은 가능하지 않을까 3D model과 AR을 바탕으로 데이터 수집에도 활용 가능하지 않을까   \n","id":27,"section":"Research","summary":"Summary 3D object detection : classifies the object category and estimates oriented 3D bounding boxes of physical objects from 3D sensor data. Applications : By extending prediction to 3D, one can capture an object’s size, position and orientation in the world, leading to a variety of applications in robotics, self-driving vehicles, image retrieval, and augmented reality Benchmarks KITTI (car, cyclist, pedestrian easy, mod,","tags":null,"title":"MLCV #9 | 3D Object Detection","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv09-3d-object-detection/","year":"2020"},{"content":"6.1 Principal Component Analysis  Principal component analysis (PCA) : is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest. Principal components : of a collection of n-dim data points are a sequence of $n$ unit vectors, where the $i$-th vector is the direction of a line that best fits the data while being orthogonal to the previous $i-1$ vectors. Here, a best-fitting line is defined as one that minimizes the average squared distance from the points to the line. Methods  가장 직관적으로 PC를 찾는 방법은 위와 같이 각 데이터로부터 거리가 가장 짧은 축 (벡터) 을 찾고 이를 반복하는 방법 일반적으로는 주어진 데이터셋의 covariance matrix를 eigendecomposion 하여 얻어낸 eigenvector를 eigenvalue가 큰 순서대로 정렬하여 PC 얻는 방법    C1축으로 투영한 데이터가 분산을 최대로 보존하는것을 알 수 있다.   PCA는 주어진 n-dim 데이터들을 가장 잘 나타낼수 있는 n개의 벡터 (축) 를 찾는 방법. 이 과정에서 찾아낸 벡터들을 principal component라 부르며 이 principal component (축, 벡터) 위로 데이터를 투영시켜 (projection) 데이터들의 차원을 낮추는데 자주 활용 즉, Principal Components는 주어진 데이터를 가장 잘 나타낼 수 있는 적절한 축 (초평면, 벡터) 이며 이는 곧 원본 데이터의 분산을 최대한 유지 할 수 있는 축이된다. (각 데이터를 구분하는데 필요없는 feature 축을 제거해야 데이터 특성이 유지됨).   \n2. Covariance Matrix   주어진 데이터셋에서 분산이 최대인 축을 (Principal Components) 찾기위해 앞서 언급했듯 원본 데이터셋과 투영된 데이터셋의 거리가 최소가 되는 축을 찾는 방법도 있겠으나, Covariance Matrix를 선형변환의 하나로 보아 이를 해결하는게 일반적이다.\n  Covariance Matrix : 각 원소 $a_{i,j}$ 가 $i$ 와 $j$ 번째 feature의 covariance로 구성되어 있는 행렬로 x,y 축의 2차원 공분산 행렬의 원소들은 다음의 의미를 갖는다.\n $a_{11}$ 은 x축 방향으로 퍼진정도, 이 경우 $a_{11}$ 값은 $Cov(X,X) = \\sigma_X^2 , \\text{ (=variance) }$ $a_{22}$ 는 y축 방향으로 퍼진정도, 마찬가지로 값은 $Cov(Y,Y) = \\sigma_Y^2$ $a_{12} = a_{21}$ 은 x,y축 방향으로 함께 퍼진 정도로 $Cov(X,Y) = E((X-\\mu)(Y-v))$    Covariance Matrix 를 선형변환으로 보고 아래와 같이 Principal Components 를 얻을 수 있다.\n 어떠한 행렬은 선형변환 으로도 볼 수 있으며, 이를 주어진 벡터공간에 곱하여 다른벡터공간으로 mapping 할 수 있다. 각 feature들이 선형 독립인, Covariance가 없는 데이터 분포를 가정해보자 (Figure.A below) 이 데이터 분포에 아래 행렬들을 각각 곱하여 선형변환한 결과는 그림과 같다. 즉, 선형독립인 데이터분포를 Covariance Mat로 선형변환하면 Covariance Mat의 성분에 따라 X,Y 축으로 Shearing 된 분포를 얻을 수 있다. 반대로 말하면 우리가 관측을 통해 갖고있는 데이터는 그 데이터의 CovMat에 의해 선형변환된 분포로 볼 수 있으며 이는 곧, 그 CovMat의 Eigenvector가 선형변환을 통해 방향이 변치 않는 벡터들이고 Eigenvalue가 이 벡터들의 scale된 크기임을 의미한다. 따라서 우리는 어떤 데이터의 CovMat을 구하고 이 CovMat을 Eigendecomposition하여 Principal Components를 찾을 수 있다 고유값은 고유벡터의 방향으로 벡터공간이 얼마나 확장될지를 이야기한다. 따라서 Eigendecomposition을 한 이후에 고유값의 크기에 따라 고유벡터를 정렬하면 중요한 순서대로 주성분을 구할 수 있다.    $$ B = \\begin{pmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 5 \\end{pmatrix}. \\quad C = \\begin{pmatrix} 5 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{pmatrix} , \\quad D = \\begin{pmatrix} 3 \u0026amp; 2 \\\\ 2 \u0026amp; 4 \\end{pmatrix} $$\n(A) original data dist, (B), (C), (D) linearly transformed by matrix B, C, D \n3. Implementation   주어진 데이터의 공분산행렬 Covariance Matrix 를 구하고 이를 eigendecomposition 하여 Eigenvalue와 EigenVector를 구하는방법. 즉 공분산행렬의 고유벡터들이 Principal Component가 되며 이 고유벡터로의 projection (내적) 을 통해 차원축소가 가능하다.\n  이미지와 같이 data-dim이 과도하게 커지는 경우 (100x100 이미지의 공분산행렬의 크기는 10000x10000) 공분산행렬을 구하는데 어려움이 있다. 이를 위해 굳이 공분산행렬을 구하지 않고 centerized 데이터를 SVD 하여 Singular Value 와 Singular Vector를 구하는 방법\n  # (1) Covariance Matrix 의 Eigenvalue 와 Eigenvector를 계산 X_cen = X - X.mean(axis=0) # 평균을 0으로 X_cov = np.dot(X_cet.T, X_cen) / (N-1) w, v = np.linalg.eig(X_cov) # (2) SVD를 이용한 방법 U, D, V_t = np.linalg.svd(x_cen)  \n4. Applications   Dimensionality Reduction : PCA를 통해 PC를 찾고나서 Variance가 큰 순서대로 n개의 basis를 골라 (Eigenvalue가 큰 Eigenvector들) 만든 벡터공간 (hyperplane, 초평면) 위로 데이터를 projection하면 차원축소가 가능하다. 일반적으로 차원을 축소하는 방법은 이러한 Projection을 이용한 방법과 manifold learning 의 두가지 방법론이 있다.\n  데이터를 표현하는 feature가 증가 → 데이터의 차원이 증가 → 벡터공간에서 부피의 증가 → 데이터의 밀도는 차원이 증가할수록 감소 → 데이터간의 거리 증가 → 고차원, 저밀도의 데이터를 모델링하기 위해 모델이 복잡해짐 → Overfit 예로 어떠한 위치를 표현하기 위하여 X,Y 좌표의 두개 feature를 사용하다가 Z축을 추가하면 2차원에서 3차원이 된다. 그러나 네비게이션과 같이 특정 목적에 따라 굳이 Z축 정보가 필요하지 않을 수도 있다.    A hyperplane is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines. This notion can be used in any general space in which the concept of the dimension of a subspace is defined.\n   Eigenface: 분산이 큰 순서대로 (Principal Component) 일수록 얼굴의 전반적인 형태를 나타내고, 뒤로갈수록 세부적인 차이를 반영하고 더 뒤로가면 노이즈정보를 나타냄.\n  Noise Reduction : PCA로 얻어진 주성분벡터들은 서로 Orthogonal 이므로 주성분 벡터들이 n차원 공간을 생성하는 basis의 역할도 할 수 있다. 즉 PCA로 얻은 주성분 벡터들을 $e_1, e_2, \u0026hellip; e_n$ 이라 하면, 임의의 n차원 데이터 x는 $x = c_1e_1 + c_2e_2 + , \u0026hellip; c_ne_n$ 과 같이 나타낼 수 있으며 이와같이 어떤 데이터집합의 데이터들을 그 주성분벡터들의 일차결합으로 표현하는 것을 Karhunen-Loeve Transform 이라한다. 이 때 뒷 부분의 주성분 벡터들은 noise성 정보를 나타내므로 이부분을 제하고 전반부 k개의 벡터들만으로 원래 데이터를 표현하면 노이즈가 제거된 데이터를 얻을 수 있다.\n  Background Subtraction in surveillance camera : 마찬가지로 각 프레임을 데이터 샘플로 보고 PCA를 수행하면 변화가 없는 배경부분이 PC에 해당하므로 이부분을 제거하여 배경제거에도 사용할 수 있다.\n  Whitening : 이상적으로 모든 픽셀이 서로 독립이라면 공분산행렬이 단위행렬이 된다. 즉 위의 그림 (a) 와 같은 형태로 데이터를 변형한다.즉 mean 과 variance를 각각 0과 1로 만드는 작업이라고 볼 수 있을듯.\n  Figure . Differences between projection and manifold learning* -- \n5. Reference  https://angeloyeo.github.io/2019/07/27/PCA.html  ","id":28,"section":"Mathematics","summary":"6.1 Principal Component Analysis Principal component analysis (PCA) : is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest. Principal components : of a collection of n-dim data points are a sequence of $n$ unit vectors, where the $i$-th vector is the direction of a line that best","tags":null,"title":"Linear Algebra for ML #6 | PCA ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/2020-06-27-linear-algebra-for-ml-pca/","year":"2020"},{"content":"1. Introduction   앞서 Bayes\u0026rsquo; Rule을 이용한 방식의 단점은 likelihood의 probability distribution을 알고있어야 한다는 점인데, 관찰을 통해 likelihood를 얻는 것도 쉽지 않을뿐더러 완벽한 Distrib function을 얻을 수 있는것도 아님.\n  따라서 (관측된 몇몇) Data로 부터 직접 decision policy를 구하기 위한 방법을 찾고자 한다. 이에 몇가지 parameter로 이루어진 함수로 데이터분포를 모델링(가정) 하고 이 모델이 주어진 데이터를 가장 잘 표현하도록 하는 parameter들을 구함\n  대표적으로 DL 방법론들이 이러한 방식이다.\n  \n2. Problem Definition   Regression 문제는, continuous input $x$ 에 대하여 실제 정답 $t$ 와 최대한 같은 output $y$ 를 만들도록 한다. (키를 보고 몸무게를 예측하는 모델)\n  이 모델을 학습시키기 위해 우리는 N명의 실제 키($x_i$) 와 몸무게 ($t_i$) 에 대한 데이터셋 $D$ 를 갖고있다.\n  $$ D = {(x_1, t_1),(x_2, t_2),(x_3, t_3), \u0026hellip; ,(x_N, t_N)} $$\n  이제 우리는 parameter $w$를 갖고 키를 넣으면 예측된 몸무게가 나오는 함수 $y(x|w)$를 정의하여 가능한 모든 키 $x$ 에 대하여 예측된 몸무게 $y(x|w)$가 실제 몸무게 $t$에 최대한 가깝도록 하는 $w$를 찾아야한다. 이렇게 완벽한 모델 $y(x|w)$를 얻고나면 $t=y(x|w)$ 로 주어진 키에 대한 몸무게를 예측할 수 있다.\n  그러나 예측이라는게 정확할수는 없기에 일반적으로는 실제 몸무게 $t$는 예측한 몸무게 $y$를 평균으로하는 Gaussian을 따른다고 할 수 있다 고 말하며 아래와 같이 쓸 수 있다.\n  $$ t \\sim N (y(x|w), \\sigma^2) \\ p(t|x, w, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{-\\frac{t-y(x|w))^2}{2 \\sigma^2}}$$\n\n3. Solution : Maximum Likelihood Estimation  Maximum likelihood estimation (MLE) : is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model is most probable. 위에서 $p(t|x)$ 는 키가 $x$ 일때 실제 몸무게가 $t$일 확률을 의미한다. 그렇다면 Dataset이 위와 같이 구성될 확률 $P(D)$ 는 어떻게 구할 수 있을까. 다시말해, 키가 $x_1$ 일때 실제 몸무게가 $t_1$ 이고 키가 $x_2$ 일때 실제 몸무게가 $t_2$ ,\u0026hellip; 일 확률은 각 data가 독립일 때 아래와 같이 곱의 법칙으로 구할 수 있다.  $$p(D) = \\prod_{i=1}^{N} p(t_i|x_i) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{-\\frac{t-y(x|w))^2}{2 \\sigma^2}}$$\n  사실, 위의 p(D)는 $w$에 따라 달라진다. 그래서 $p(D)$ 보다는 $p(D|w)$라 보는게 맞다. 그렇다면 여기서 어떻게 $w$ 를 정해야 예측을 잘하는 모델이 될까.\n  $p(D|w)$가 가장 높은 모델이어야 한다. 다시말해, 키가 $x_1$ 일때 실제 몸무게가 $t_1$ 일 확률이 가장 높다고 말하고 키가 $x_2$ 일때 실제 몸무게가 $t_2$일 확률이 가장 높다고 말하고 ,\u0026hellip; 하는 모델이다.\n  정리하면 우리가 하고자하는것은 $p(D|w)$가 최대가 되는 $w$를 찾는것이고 여기서 p(D|w)가 likelihood 이기에 이 방법을 Maximum Likelihood Estimation이라 한다.\n  \n4. 참고 : Prior, Likelihood, Posterior   구하고자 하는 대상 : prior, likelihood, posterior 를 구분하기 위하여는 우리가 그 분포를 구하고자 하는 대상을 명확히 해야한다. 이전(#1)에는 그 대상이 농어인지 연어인지의 여부였고, 대다수의 ML 알고리즘들이 구하고자 하는 대상은 바로 모델의 Parameter 인 $w$ 이다.\n  주어진 대상: 이전(#1)에는 농어와 연어의 밝기에 대한 정보가 주어졌다. 대부분의 ML 알고리즘에게는 데이터셋 즉, $D$ 가 주어진다.\n  Posterior : 주어진 대상에 대한 구하고자하는 대상의 분포, $P(w|D)$\n  Likelihood : 구하고자 하는 대상을 모르지만 안다고 가정했을 경우 주어진 대상의 분포, $P(D|w)$\n  Prior : 주어진 대상들과 무관하게 우리가 구하고자 하는 대상에 대해 이미 알고있는 사전정보, $p(w)$\n  \n5. Solution : MLE 계산  다시 MLE로 돌아가서 우리가 해야할 일은 바로 다음 식을 최대로 하는 $w$를 찾는 것.  $$\\text{likelihood} = p(D|w) = \\prod_{i=1}^{N} p(t_i|x_i) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{-\\frac{t-y(x|w))^2}{2 \\sigma^2}}$$\n 보통 이를 풀기위해 주로 log를 취해, 복잡한 곱연산을 덧셈으로 바꾸어 수식의 전개를 용이하게 한다.  $$ \\text{log likelihood} = log(p(D|w)) = \\sum_{i=1}^{N} ( -log(\\sqrt 2\\pi\\sigma) - \\frac{(t_i - y(x_i|w))^2}{2\\sigma^2} ) $$\n 이제 이 log likelihood 를 최대가 되게하는 $w$를 찾으면 되는데 이와 무관한 상수항을 제외하면 아래와 같은 L2 Loss를 얻을 수 있다.  $$ \\sum_{i=1}^{N}(t_i - y(x_i|w))^2$$\n  즉, L2 Loss를 최소화 시키는 일은 Likelihood를 최대화 시키는 일과 같다.\n  정리하면, 우리는 MLE를 이용하여 likelihood가 최대가 되는 $\\theta$ 를 찾을 수 있다\n  $$ \\theta_{MLE} = \\text{argmax}\\theta \\prod_i p{model} (x_i ; \\theta) $$\n\n6. Solution : Maximum A Posterior   MLE가 Likelihood를 최대화 하는 작업이라면 MAP는 Posterior를 최대화 하는 작업이다. 둘의 차이는 이전(#1)에 언급했듯 Prior의 유무이며 이에 따라 구하고자하는 대상을 철저하게 데이터로 부터 추정하고자 한다면 MLE 를 이용하는 것이고, 데이터와 더불어 우리가 갖고있는 사전지식까지 반영하고자 한다면 MAP 를 이용할 수 있다.\n  그렇다면 prior를 반영해서 좋은점은 무엇일까. 매우 강력한 사전지식을 갖고 있다면 w를 구하는데 큰 도움이 될 것이다. 그러나 그렇지 않더라도 output을 우리가 원하는대로 제어할 수 있기에 prior를 반영하는게 좋은경우가 많다.\n  이전 (#1)에서 구한 Posterior에서 $w$ 가 continuous 하므로 분모를 integral로 바꾸면 아래와 같다. 여기서 $D$ 는 주어진 값이고 $w$에 대하여 적분을 하고 있으므로 분모는 상수가 되어\n  $$ \\text{Posterior} = P(w|D) = \\frac{P(D|w)P(w)}{\\int P(D|w) P(w) dw} = \\eta P(D|w) P(w)$$\n 이제 $w$의 prior를 정해주어야 하는데, 위에서 말한 것 처럼 $w$에 대한 사전지식을 갖고있지 않다. 그래서 그냥 $w$ 에 0을 평균으로하는 Gaussian Distrib 라는 prior를 걸어줘보자. (이렇게 하면 w의 크기가 작아져 Overfitting을 방지하는 효과를 보임, Weight Decay라고도 함)  $$ p(w) = \\frac{1}{\\sqrt 2 \\pi \\sigma_w} e ^{-\\frac{w^2}{2\\sigma^2_w}}$$\n likelihood 에서와 같이 posterior 에 log를 취해주고 그 값을 최대로 하는 w를 찾는것이 우리의 목표이다.  $$ \\text{argmax} ( log P(w|D)) = \\text{argmax} (log\\eta + logP(D|w) + logP(w)) $$\n 위에서 보았듯 두번째 항을 Maximize 하는것은 L2 loss 를 minimize 하는것과 같다. 따라서 L2 loss 를 $L(w)$ 라 하면  $$ w^* = \\text{argmax} (log\\eta - L(w) - logP(w)) \\= \\text{argmax} (log\\eta - L(w) - log(\\sqrt 2 \\pi \\sigma_w) - \\frac{w^2}{2\\sigma^2_w} )$$\n 위에서 $eta, \\pi, \\sigma_w$ 와 같은 상수를 제하면 다음 식을 minimize 하는 문제가 된다  $$ L(w) + \\frac{w^2}{2\\sigma^2_w} = L(w) + \\alpha w^2$$\n  이는 weight decay (L2 Regularization) 이 적용된 DL의 loss 함수가 된다. 즉 L2 Regularization을 적용하는 것은 $w$에 Gaussian Distrib 를 prior로 걸어주는일이 된다.\n  추가로, Laplacian Distrib 를 Prior로 걸어주면 L1 Regularization 을 얻을 수 있다\n  \n","id":29,"section":"Mathematics","summary":"1. Introduction 앞서 Bayes\u0026rsquo; Rule을 이용한 방식의 단점은 likelihood의 probability distribution을 알고있어야 한다는 점인데, 관찰을 통해 likelihoo","tags":null,"title":"Bayesian statistics #2 | MLE and MAP","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/200623-bayes2-mle-and-map/","year":"2020"},{"content":"Overview   New Topics : Explainable AI, Fairness, Accountability, Transparency and Ethics in Vision\n  Recognition \u0026gt; Learning Methods \u0026gt; Face, Gesture, Body Pose \u0026gt; Img and Video Synthesis \u0026hellip;\n  Young Researcher Award : Jon Barron and Deqing Sun\n  Best Paper Award : Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the wild\n  Best Studen Paper Award : BSP-Net: Generating Compact Meshes via Binary Space Partitioning\n  Best Stud Honorable Mention : DeepCap: Monocular Human Performance Capture Using Weak Supervision\n  \nAction Recognition   Action Recog 분야는 skeleton based Graph ConvNet 활용해 문제를 해결하려는 연구들이 보임\n  Skeleton-Based Action Recognition With Shift Graph Convolutional Network\n  PaStaNet: Toward Human Activity Knowledge Engine\n  \nFace, Gesture, Body Pose Estimation   Hand Position 관련 논문들이 많이 있음 : 캐스팅을 비롯한 손목 관련 오류분석\n  Attention Mechanism Exploits Temporal Contexts: Real-Time 3D Human Pose Reconstruction : 3D Human Pose Recon based on monocular video : 3D Pose estimation Follow Up 목적으로 한번쯤 읽어보면 좋을듯\n  A Morphable Face Albedo Model : 유사한 body model을 활용할 수 있지 않을까\n  Cascaded Deep Monocular 3D Human Pose Estimation With Evolutionary Training Data: 백스윙탑과 같은 일반적이지 않은 포즈에 대하여 evolutionary data 가 효과가 있다는 연구인듯한데, 3D Pose 관련해서 일단 읽어보면 좋을 듯, 아카데미사업부에 3D pose data가 일단 있는지 문의해보고 관련 데이터가 있다면 활용방안이 있을듯.\n  Bayesian Adversarial Human Motion Synthesis : Pose Synthesis\n  \nImage and Video Synthesis   GAN based Tech, 잔기술들이 늘어가는 느낌, 동시에 응용분야를 찾아가려는 느낌이 강함\n  옷을 직접 입어보지 않는 virtual Try-on 관련 기술들이 눈에 띔 : 골프존유통\n  Controllable Person Image Synthesis With Attribute-Decomposed GAN  : 사람은 그대로 두고 pose 옷 기타등등을 바꿔서 image synthesis : nasmo2field 같은데 응용 해볼 수 있지 않을까\n  \nVideo Analysis  SpeedNet: Learning the Speediness in Videos s: Google Research, normal speed 를 기준으로 빨리감기 된 비디오인지 슬로우모션인지 classification, self supervised, 주어진 video를 원래 속도로 되돌릴 수도 있고, Video Classification 에도 사용하고, Video Retrieval 에도 사용한다.  \n3D Reconstruction   source에 따라 다음의 세 분야로 나뉘어지고 있음. single image, multiview, Sensor, etc.\n  Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the wild : 3D recon from single view image - Demo/code\n  DeepCap : Monoular Human Performance Capture using weak Supervision : 읽어볼 것\n  \nMachine Learning Basis   단순한 학습시간 감소뿐 아니라 적은 Training Data로 학습하기 위한 방법들을 다루는 Efficient Training : Neural Net Pruning\n  Model Compression, Quantization\n  Adversarial Learning\n  Representation Learning\n  AutoML 분야 논문이 잘 안보이는것 같음 : Tutorial Session에 모여있음\n  Transfer, low-shot, semi, unsupervised Learning\n  \nOthers   Embedding Expansion: Augmentation in Embedding Space for Deep Metric Learning\n  StarGAN v2: Diverse Image Synthesis for Multiple Domains\n  \n","id":30,"section":"Engineering","summary":"Overview New Topics : Explainable AI, Fairness, Accountability, Transparency and Ethics in Vision Recognition \u0026gt; Learning Methods \u0026gt; Face, Gesture, Body Pose \u0026gt; Img and Video Synthesis \u0026hellip; Young Researcher Award : Jon Barron and Deqing Sun Best Paper Award : Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the wild Best Studen Paper Award : BSP-Net: Generating Compact Meshes via Binary Space Partitioning Best Stud Honorable","tags":null,"title":"2020 | CVPR","uri":"https://koreanbear89.github.io/engineering/9.-others/2020-06-17-2020-cvpr/","year":"2020"},{"content":"1. Introduction   연역적 추론(Freq) 에서 귀납적 추론(Bayes) 으로의 확률론 패러다임의 전환\n  Frequentist View : 확률을 \u0026lsquo;발생하는 현상의 빈도수\u0026rsquo;로 해석하는 관점 =\u0026gt; 연역적 사고에 기반, 가설을 먼저 세워놓고 결론적으로 여러 사실에 적용, 확률계산, 유의성 검증 (e.g. 1이나올 확률은 16%다 = 100번 던지면 16번은 앞면이 나온다)\n  Bayesian View : 확률을 \u0026lsquo;어떤 주장에 대한 신뢰도\u0026rsquo;로 해석하는 관점 =\u0026gt; 경험에 기반, 여러사실들을 통해 결론에 도달하는 형태의 추론 방식 , 추가되는 정보를 바탕으로 사전확률을 갱신함. (e.g. 1이나올 확률은 16%다 = 1이 나왔다는 주장의 신뢰도가 16%다)\n    \n2. Bayes Rule  베이즈정리는 아래와 같이 나타내며, $P(A), P(A|B)$ 는 각각 사전확률, 사후 확률이라 부른다. 즉, 베이즈 정리는 기본적으로 사전확률과 사후확률의 관계를 나타냄을 알 수 있다. 이로부터 베이즈정리는 베이즈 확률론에서 새로운 정보, $P(B)$ 를 토대로 어떤 사건이 발생했다는 주장에 대한 신뢰도를 갱신해나가는데 활용 가능함을 알 수 있다.  $$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n\n3. Example (Bass vs Salmon) Problem Definition : 수학적 문제 정의\n Bayes Rule을 설명하기 위해 흔히 농어(Bass)와 연어(salmon)를 구분하는 문제를 예로 든다. 즉 우리는 지금부터 물고기의 밝기를 이용해 그 물고기가 농어인지 연어인지 맞추는 문제를 푼다. Prior를 바탕으로 Posterior를 추론하는 과정. 즉 확률 P(A|B)를 알고있을때 관계가 정반대인 확률 P(B|A)를 구하기 위하여, P(A|B)를 활용할 수 있다. 주어진 피부밝기를 이용해 어종을 구분하기 위하여 피부밝기를 $x$ , 어종을 $w$로 하여 이 문제를 수학적으로 모델링 할 수 있다. 즉 피부 밝기가 $k$ 일 때 그 물고기가 농어, 연어일 확률은 각각 아래의 조건부 확률로 표현할 수 있다. 정리하면, 물고기 분류 문제는 밝기 $x$ 가 주어졌을 때 그 고기가 class $w_i$ 에 속할 확률인 $P(w_i|x)$를 구하면 풀 수 있다. 그리고 확률론에서는 이를 Posterior라 부른다.  $$ P(w=bass| x=k) \\quad P(w=salmon|x=k)$$\n\nSolution (1) : 관찰\n 위의 Posterior $P(w_i|x)$ 를 직접 구할 수 있는 방법은 없다. 그러나 $P(x|w_i)$, 즉 물고기의 종류가 $w_i$ 일 경우 피부 밝기가 $x$일 확률(확률밀도) 는 아래 fig1과 같이 관찰을 통해 알 수 있다. 이렇게 관찰을 통해 얻은 확률분포 $P(x|w_i)$를 likelihood 라고 부른다. 아래 분포로부터 농어가 일반적으로 연어에 비해 밝음을 알 수 있다. 그렇다면 굳이 posterior를 구할필요 없이 likelihood로 분류 할 수 있지 않을까. 예로 $x=5$ 가 나왔다면 이 물고기를 농어라고 분류 할 수 있어 보인다. 그러나 likelihood 에는 연어와 농어가 잡힐 확률이 반영되어있지 않다. 즉, 연어가 잘 살지않는 해역에서 잡아올린 물고기라면 $x=4$ 라해도 그 물고기는 연어가 아닐 확률이 높다.  Figure1. Distribution of brightness (likelihood) for bass and salmon \nSolution (2) : Prior\n 위와 같이, 바다에 살고있는 농어와 연어의 비율이 반영되지 않았기에, likelihood 만으로는 posterior를 알 수 없다. 따라서 우리는 일단, $x$와 무관하게 농어가 잡힐 확률 $P(w_1)$ 과 $P(w_2)$를 알아야하며, 이 값을 Prior 라고 부른다. 사실 이러한 prior는 더 많이 존재할 수 있다. 계절에 따라 잡히는 농어와 연어의 비율이 달라질 수도 있으며 밤,낮에 따라서도 달라질 수 있다. 이러한 prior들을 활용하여 posterior를 갱신할수록 주장의 신뢰도를 높일 수 있다.  \nSolution3 : Bayes\u0026rsquo; Rule\n  Posterior, $P(w_i|x)$ : 밝기가 주어졌을때 그 물고기가 농어,연어일 확률. 즉, feature 가 주어졌을 때, 대상이 특정 클래스에 속할 확률로 우리가 최종적으로 구해야하는 값.\n  Likelihood, $P(x|w_i)$ : 농어 또는 연어의 피부밝기가 어느 정도로 분포되어 있는지의 정보. 즉, 각 클래스에서 우리가 활용할 단서가 어떤 형태로 분포 되어 있는지를 알 수 있음.\n  Prior, $P(w_i) $ : 밝기와 무관하게 농어와 연어의 비율이 얼마나 되는지를 나타내는 값. 보통 사전정보로 주어지거나 연구자의 사전지식을 통해 정해줘야하는 값.\n  우리의 목적은 Posterior를 구하는 것이며, 이는 Likelihood와 Prior로 아래와 같이 구할 수 있다.\n  $$ P(A|B)=\\frac{P(A,B)}{P(B)}, \\quad P(A,B) = P(A|B) P(B) = P(B|A) P(A)$$\n$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{P(B|A)P(A)}{\\sum_A P(B|A)P(A)}$$\n$$ P(w_i|x) = \\frac{P(x|w_i)P(w_i)}{\\sum_j P(x|w_j)P(w_j)}$$\n 위에서 좌변은 우리가 구하고자하는 Posterior이고 우변의 분자는 Likelihood와 Prior의 곱이며, 우변의 분모는 Evidence라고 보통부르는데 이 또한 Likelhood와 Prior로 구할 수 있다. 이를 Bayes\u0026rsquo; Rule 이라고 부른다.  Figure2. Distribution of brightness and species (Posterior)  최종적으로 위 그래프는 Bayes\u0026rsquo; Rule 에 따라 농어와 연어의 Posterior를 구한 결과이다. 앞서 Likelihood와 달리 연어가 더 잘 안잡힌다는 Prior가 반영되어 Decision Boundary가 좌측으로 옮겨진것을 알 수 있다.  \n4. Appendix : Difference between probability and Likelihood   연속사건의 경우 특정 사건이 발생할 확률은 0이다. Fig1에서 연어를 건져올렸는데 그 밝기가 3일 확률은 0~10 사이의 무수히 많은 숫자들 중 3이 뽑힐 확률이 되므로 0이된다.\n  이에 따라 연속사건의 확률을 논할때는 일반적으로 확률밀도함수의 구간을 이야기 한다. 가령 밝기가 2~4 일 확률은 전체 넓이와 해당 구간의 넓이의 비로 이야기 할 수 있다.\n  그러나 다시 Fig1에서 연어의 밝기가 1일 확률과 3일 확률이 모두 0이라는것만으로는 3일 가능성이 높을것 같다.이에 가능도라는 개념을 적용할 수 있으며, 분포의 y값인 0.4와 0.1이 이를 반영하고있음을 알 수 있다.\n  ","id":31,"section":"Mathematics","summary":"1. Introduction 연역적 추론(Freq) 에서 귀납적 추론(Bayes) 으로의 확률론 패러다임의 전환 Frequentist View : 확률을 \u0026lsquo;발생하는 현상의 빈도수\u0026rsquo;","tags":null,"title":"Bayesian statistics #1 | Bayes' Rule","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/200609-bayes1-bayes-rule/","year":"2020"},{"content":"1. Python Basic Data Type 1.1 Number a**b # 제곱 a//b # 몫 a%b # 나머지  1.2 String \u0026gt;\u0026gt;\u0026gt; y = 3.42134234 \u0026gt;\u0026gt;\u0026gt; \u0026quot;{0:0.4f}\u0026quot;.format(y) # 문자열포맷 '3.4213'  1.3 List a = [1,2,3] b = [4,5,6] # 1. 리스트 더하기 \u0026gt;\u0026gt;\u0026gt; a + b : [1,2,3,4,5,6] # 2. 리스트 반복 \u0026gt;\u0026gt;\u0026gt; a*3 : [1,2,3,1,2,3,1,2,3]  1.4 Tuple   튜플은 리스트와 달리 값을 변화시킬 수가 없다.\n  값이 바뀌지 않았으면 할때 리스트 대신 튜플사용\nt1 = (1,) # ele가 하나일때는 ,붙여야함 t2 = 1,2,3 # 괄호 생략 가능    1.5 Dict   Hash 자료구조로 되어있어 리스트나 튜플처럼 순차적으로 해당 요솟값을 구하지 않고 Key를 통해 Value를 얻는다.\ndic = {'name':'pey', 'phone':'0119993323', 'birth': '1118'}    1.6 Set   중복을 허용하지 않는다\n  순서가 없다. 따라서 인덱싱으로 set 안의 element 를\n# generate set A = set([1,2,2,3]) # \u0026gt;\u0026gt; {1, 2, 3} B = set([1,3,4]) # indexing A[2] # \u0026gt;\u0026gt; TypeError: set object does not support indexing # operation A\u0026amp;B, A|B, A-B # methods A.remove(1) A.add('b') A.update(['a','b'])    1.7 Bool  True/False 의 두가지 값만을 가질 수 있다.  \n2. Lambda   쓰고 버리는 일회용 함수로 간단한 기능을 일반적인 함수와 같이 정의해두고 쓰는게 아니라 필요한 곳에서 즉시 사용하고 버림\n  map(func, iterable), filter(func, iterable) 함수와 주로 함께 쓰임, map은 func를 iterable에 모두 적용한 결과를 리턴하고, filter는 iterable중 func를 만족하는 ele만 리턴함\n  # lambda arg리스트:표현식 으로 정의하여 사용 \u0026gt;\u0026gt; g = lambda x:x**2 \u0026gt;\u0026gt; g(8) 64 # map(func, iterable) 과 함께 쓰일 때 \u0026gt;\u0026gt;\u0026gt; list(map(lambda x:x+3, [1,2,3,4])) [4, 5, 6, 7] # filter(func, iterable)와 함께 쓰일 때 \u0026gt;\u0026gt;\u0026gt; list(filter(lambda x:x\u0026gt;0, range(-5,5))) [1, 2, 3, 4]  \n3. init, call  Class는 객체를 만들어내기 위한 설계도(틀)로서 연관된 변수와 메서드의 집합 Object(객체) 는 클래스타입으로 선언되었을 때, Instance 는 그 객체가 메모리에 할당되어 실제 사용될 때  class A: def __init__(self): # 객체를 생성할때 사용 print('init') def __call__(self): # 마치 함수를 호출하는 것처럼 인스턴스를 호출할 수 있도록 만듦 print('call') \u0026gt;\u0026gt;\u0026gt; a = A() init \u0026gt;\u0026gt;\u0026gt; a() call  \n4. Decorator   함수를 arg로 받아서, 원본함수를 변경하지 않고 앞 뒤에 새로운 로직을 추가하는 경우가 자주 있음,\n  가령 아래와 같이 함수를 입력으로 받아 함수의 실행시간을 출력해주는 로직을 추가해서 사용할 수 있음\n  def time_check(func): def new_func(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print('실행시간:', end_time-start_time) return result return new_func def big_number(n): return n**n**n new_func = time_checker(big_number) new_func(6)   Decorator를 적용하여 이를 아래와 같이 바꿀 수 있다.  def time_check(func): def new_func(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print('실행시간:', end_time-start_time) return result return new_func @time_check def big_number(n): return n**n**n big_number(6)  \n5. Method   instance method : 가장 흔히 쓰이는 메서드로 인스턴스 변수에 엑세스 할 수 있도록 첫번째 파라미터에 항상 self 를 갖는다.\n  static method : 정적 메서드는 이러한 self 파라미터를 갖지 않고 인스턴스 변수에 엑세스 할 수 없다. 따라서 보통 객체 필드와 독립적이지만 로직상 클래스내에 포함되는 메서드에 사용한다. 클래스 안에 있지만 일반 함수와 다를게 없이 그냥 클래스 인스턴스에서 호출할 수 있음. 인스턴스 메소드는 인스턴스 안의 멤버 변수의 값을 바꾸거나 하지만 정적 메서드는 그러지 못함.\n  class method : self 대신 클래스를 cls 라는 파라미터로 전달받아 클래스 변수에 엑세스 [참고]\n  6. Path os.chdir('/opt/vidClassifier/classifier/') sys.path.append('/opt/vidClassifier/classifier')  ","id":32,"section":"Engineering","summary":"1. Python Basic Data Type 1.1 Number a**b # 제곱 a//b # 몫 a%b # 나머지 1.2 String \u0026gt;\u0026gt;\u0026gt; y = 3.42134234 \u0026gt;\u0026gt;\u0026gt; \u0026quot;{0:0.4f}\u0026quot;.format(y) # 문자열포맷 '3.4213' 1.3 List a = [1,2,3] b = [4,5,6] # 1. 리스트 더하기 \u0026gt;\u0026gt;\u0026gt; a + b : [1,2,3,4,5,6] # 2. 리스트 반복 \u0026gt;\u0026gt;\u0026gt; a*3 : [1,2,3,1,2,3,1,2,3] 1.4 Tuple 튜","tags":null,"title":"CheatSheet | Python Basic","uri":"https://koreanbear89.github.io/engineering/1.-tools/160601-cheatsheet-python-basic/","year":"2020"},{"content":"0. Install RVM \u0026amp; Ruby $ sudo apt-get update $ sudo apt-get install -y curl gnupg build-essential $ sudo gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 $ curl -sSL https://get.rvm.io | sudo bash -s stable $ sudo usermod -a -G rvm `whoami` $ rvm install ruby-X.X.X $ rvm --default use ruby-X.X.X $ gem install bundler --no-rdoc --no-ri # If bash cannot find ruby, $ source /etc/profile  \n1. Install MathJax   Problem: MathJax does not work\n  Solution: add the source in \u0026ldquo;__include\u0026rdquo; directory and head of your blog post\n  \u0026lt;script type=\u0026quot;text/x-mathjax-config\u0026quot;\u0026gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \u0026quot;AMS\u0026quot; }}, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, }}); MathJax.Hub.Register.MessageHook(\u0026quot;Math Processing Error\u0026quot;,function (message) {alert(\u0026quot;Math Processing Error: \u0026quot;+message[1]);}); MathJax.Hub.Register.MessageHook(\u0026quot;TeX Jax - parse error\u0026quot;,function (message) {alert(\u0026quot;Math Processing Error: \u0026quot;+message[1]);}); \u0026lt;/script\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; async src=\u0026quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\u0026quot;\u0026gt; \u0026lt;/script\u0026gt;  \u0026lt;head\u0026gt; \u0026lt;!-- should remove space between '{' and '%' --\u0026gt; { % include mathjax_support.html % } \u0026lt;/head\u0026gt;  \n2. Unpublished Problem: wanna hide specific posts\nSolution\n add \u0026ldquo;published\u0026rdquo; in yaml header of each post you can publish \u0026ldquo;unpublished\u0026rdquo; posts using \u0026ndash;unpublished option  --- layout : post title : [title post] published : false ... ---  bundle exec jekyll serve --unpublished  3. Markdown    Header1 Header2 Header3     cell 1x1 cell 1x2  cell 1x3   cell 2x1 cell 2x2  cell 2x3   cell 3x1 cell 3x2  cell 3x3    ","id":33,"section":"Engineering","summary":"0. Install RVM \u0026amp; Ruby $ sudo apt-get update $ sudo apt-get install -y curl gnupg build-essential $ sudo gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 $ curl -sSL https://get.rvm.io | sudo bash -s stable $ sudo usermod -a -G rvm `whoami` $ rvm install ruby-X.X.X $ rvm --default use ruby-X.X.X $ gem install bundler --no-rdoc --no-ri # If bash cannot find ruby, $ source /etc/profile  \n1. Install MathJax   Problem: MathJax does not work","tags":null,"title":"CheatSheet | Jekyll","uri":"https://koreanbear89.github.io/engineering/1.-tools/200418-cheatsheet-jekyll/","year":"2020"},{"content":"1. Covariance   두 확률변수 사이의 상관성/의존성/유사성의 방향 및 정도에 대한 척도, 즉 데이터간의 관계성을 정량화한 정도.\n  이때의 관계성은 일반적으로 선형 관계성만을 의미하며 아래와 같이 $X$ 편차와 $Y$ 편차의 곱의 평균으로 계산\n  즉 $y=x^2$ 과 같은 관계성은 Covariance로 확인할 수 없다.\n  $$ Cov(X,Y) = E((X-\\mu_x)(Y-\\mu_y)) $$\n  다시말해, 주어진 2차원 데이터를 x, y축으로 각 축의 평균만큼 shift 함으로써 편차를 구하고 각 데이터샘플의 x,y 값의 곱을 구해 전부 더함\n  구해진 Covriance에 따라 아래와 같은 관계성을 유추해볼 수 있음\n $Cov(X,Y) \u0026gt; 0$ : Positive Relationship, X가 증가할 때 Y도 증가한다 $Cov(X,Y) = 0$ : No Relation ship, 특별한 선형관계가 없다 $Cov(X,Y) \u0026lt; 0$ : Negative Relationship, X가 증가할 때 Y는 감소한다.     (L) positive relationship, (R) Negative Relatioship, projected to each axis  \n2. Covariance Matrix   선형변환 관점에서의 Covariance Matrix : CovMat 또한 $n \\times n$ 의 정방행렬로 선형변환의 하나로 볼 수 있으며, $i,j$축의 방향으로 $a_{i,j}$ 만큼 shearing 하는 선형변환으로 볼 수 있다.\n  Cov Matrix의 수학적 의미 : 일반적으로 $n$ dimension의 feature를 갖고있는 Data에 대하여 CovMat $\\Sigma$ 는 $n \\times n$ 행렬로 나타내어지며, $a_{i,j} = Cov(X_i, X_j)$와 같다\n  $$ \\Sigma = \\frac{X^T X}{n-1}, \\quad X = \\begin{pmatrix} \u0026amp; X_1 \u0026amp; X_2 \u0026amp; X_3 \u0026amp; \u0026hellip; \\\\ S_1 \u0026amp; 90 \u0026amp; 85 \u0026amp; 95 \u0026amp; \u0026hellip;\\\\ S_2 \u0026amp; 80 \u0026amp; 80 \u0026amp; 85 \u0026amp; \u0026hellip;\\\\ S_3 \u0026amp; 75 \u0026amp; 75 \u0026amp; 70 \u0026amp; \u0026hellip;\\\\ \\end{pmatrix}, \\quad X^TX = \\begin{pmatrix} dot(X_1, X_1) \u0026amp; dot(X_1, X_2) \u0026amp; \u0026hellip; \u0026amp; dot(X_1, X_d) \\\\ dot(X_2, X_1) \u0026amp; dot(X_2, X_2) \u0026amp; \u0026hellip; \u0026amp; dot(X_2, X_d) \\\\ \u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip; \\\\ dot(X_d, X_1) \u0026amp; dot(X_d, X_2) \u0026amp; \u0026hellip; \u0026amp; dot(X_d, X_d) \\end{pmatrix} $$\n\n3. Correlation   위의 Covariance 는 $X,Y$ 의 단위와 크기에 영향을 받는다는 문제가 있다.\n  가령, 100점 만점인 언어와 수학의 공분산에 비해 50점 만점인 생물1과 생물2의 공분산이 더 적게 나올 수 있다.\n  상관계수 Correlation은 이를 보완하기 위한 방법으로 확률변수의 절대적 크기에 영향을 받지 않도록 Covariance를 분산의 크기로 나누어준다\n  $$ \\text{Correlation} \\quad \\rho = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}} , \\quad -1 \\leq \\rho \\leq 1$$\nVisualization of relation between X, Y according to $\\rho$ value \n","id":34,"section":"Mathematics","summary":"1. Covariance 두 확률변수 사이의 상관성/의존성/유사성의 방향 및 정도에 대한 척도, 즉 데이터간의 관계성을 정량화한 정도. 이때의 관계성은 일반적으로 선형 관계성만을 의","tags":null,"title":"Statistics #8 | Covariance and Correlation","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/200227-all-of-stats8_covariance/","year":"2020"},{"content":"6.1 Introduction  Statistical Inference, or \u0026ldquo;learning\u0026rdquo; as it is called in computer science, is the process of using data to infer distribution that generated the data.  6.2 Parametric and Nonparametric Models   A statistical model $\\Im$ is a set of distributions (or densities or regression functions)\n  Parametric model : is a set of $\\Im$ that can be parameterized by a finite number of parameters\n  If we assume that the data come from a Normal distribution, then It would be two-prarmeter model.\n  $$ \\Im = {f(x; \\mu \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}} exp {-\\frac{1}{2 \\sigma^2}(x-\\mu)^2 } $$\n In general, a parametric model takes the form  $$ \\Im = {f(x; \\theta) : \\theta \\in \\Theta }$$\n  Non-Parametric model : is a set $\\Im$ that cannot be parameterized by a finited number of parameters.\n  Frequentists and Bayesians : The two dominant approaches to statistical inference are called frequentists inference and Bayyesian Inference.\n  6.3 Fundamental Concepts in inference   Many inferential problems can be identified as being one of three types : estimation, confidence sets, or hypothesis testing.\n  Point Estimation : refers to providing a single \u0026ldquo;best guess\u0026rdquo; of some quantity of interest\n  Confidence Sets : A $1-\\alpha$ confidence interval for a parameter $\\theta$ is an interval $C_n = (a,b)$ where $a = a(X_1, \u0026hellip; , X_n)$ and $b = b(X_1, \u0026hellip; , X_n)$\n  Hypothesis Testing : In hypothesis testing, we start with some default theory , called null hypothesis, and we ask if the data provide sufficient evidence to reject the theory.\n  ","id":35,"section":"Mathematics","summary":"6.1 Introduction  Statistical Inference, or \u0026ldquo;learning\u0026rdquo; as it is called in computer science, is the process of using data to infer distribution that generated the data.  6.2 Parametric and Nonparametric Models   A statistical model $\\Im$ is a set of distributions (or densities or regression functions)\n  Parametric model : is a set of $\\Im$ that can be parameterized by a finite number of parameters\n  If we assume that the data come from a Normal distribution, then It would be two-prarmeter model.","tags":null,"title":"Statistics #6 | Models, Statistical Inference and Learning ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/200202-all-of-statistics-ch6/","year":"2020"},{"content":"5.1 Introduction   The law of large numbers says that the sample average converges in proability to the expectation $ \\mu = \\mathbb{E}(X_i)$\n  The central limit theorem says that $ \\sqrt{n} (\\overline{X - \\mu})$ converges in distribution to a Normal distribution.\n  5.2 Types of Convergence  $X_n$ converges to $X$ in probability, written $X_n \\xrightarrow{P}{} X$ if for every $\\epsilon \u0026gt; 0$  $$ \\mathbb{P}(|X_n - X| \u0026gt; \\epsilon) \\rightarrow 0$$\n $X_n$ converges to $X$ in distribution, written $X_n \\rightsquigarrow X$ if  $$ \\lim_{n\\rightarrow \\infty} F_n(t) = F(t) $$\n $X_n$ converges to $X$ in quadratic mean (also called convergence in $L_2$), written $X_n \\xrightarrow{qm}{} X$, if  $$ \\mathbb{E}(X_n - X)^2 \\rightarrow 0 \\ \\text{as} \\ n \\rightarrow \\infty $$\n5.3 The Law of Large Numbers  The weak Law of Large Numbers  $$ If X_1 , \u0026hellip; , X_n \\ \\text{ are IID, then } \\overline{X}_n \\xrightarrow{P}{} \\mu$$\n5.4 The Central Limit Theorem  Let $X_1, \u0026hellip; , X_n$ be IID with mean $\\mu$ and variance $\\sigma^2$. Let $\\overline{X_n} = n^{-1} \\sum_{i=1}^n X_i$ Then,  $$Z_n \\equiv \\frac{\\overline{X_n}-\\mu}{\\sqrt{\\mathbb{V(\\overline{X_n})}}} = \\frac{\\sqrt{n}({\\overline{X_n}-\\mu)}}{\\sigma} \\rightsquigarrow Z$$\n","id":36,"section":"Mathematics","summary":"5.1 Introduction   The law of large numbers says that the sample average converges in proability to the expectation $ \\mu = \\mathbb{E}(X_i)$\n  The central limit theorem says that $ \\sqrt{n} (\\overline{X - \\mu})$ converges in distribution to a Normal distribution.\n  5.2 Types of Convergence  $X_n$ converges to $X$ in probability, written $X_n \\xrightarrow{P}{} X$ if for every $\\epsilon \u0026gt; 0$  $$ \\mathbb{P}(|X_n - X| \u0026gt; \\epsilon) \\rightarrow 0$$","tags":null,"title":"Statistics #5 | Convergence of Random Variable ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/200130-all-of-statistics-ch5/","year":"2020"},{"content":"4.1 Probability Inequalities   Inequalities are useful for bounding quantities that might otherwise be hard to compute.\n  Markov\u0026rsquo;s inequality : Let $X$ be a non-negative random variable and suppose that $\\mathbb{E}(X)$ exists. For any $t\u0026gt;0$,\n  $$\\mathbb{P}(X\u0026gt;t) \\leq \\frac{\\mathbb{E}(X)}{t}$$\n Chebyshev\u0026rsquo;s inequality : Let $\\mu = \\mathbb{E}(X)$ and $\\sigma^2 = \\mathbb{V}(X)$  $$ \\mathbb{P}(| X-\\mu| \\geq t) \\leq \\frac{\\sigma^2}{t^2} \\ \\text{and} \\mathbb{P}(|Z| \\geq k) \\leq \\frac{1}{k^2}$$\nInequalities for Expectations  Cauch-Schwartz inequality : If $X$ and $Y$ have finite variances then  $$ \\mathbb{E} |XY| \\leq \\sqrt{\\mathbb{E}(X^2) \\mathbb{E}(Y^2)} $$\n Jesen\u0026rsquo;s inequality : If $g$ is convex then, $ \\mathbb{E}g(X) \\geq g(\\mathbb{EX})$ else if $g$ is concave $ \\mathbb{E}g(X) \\leq g(\\mathbb{EX})$  ","id":37,"section":"Mathematics","summary":"4.1 Probability Inequalities   Inequalities are useful for bounding quantities that might otherwise be hard to compute.\n  Markov\u0026rsquo;s inequality : Let $X$ be a non-negative random variable and suppose that $\\mathbb{E}(X)$ exists. For any $t\u0026gt;0$,\n  $$\\mathbb{P}(X\u0026gt;t) \\leq \\frac{\\mathbb{E}(X)}{t}$$\n Chebyshev\u0026rsquo;s inequality : Let $\\mu = \\mathbb{E}(X)$ and $\\sigma^2 = \\mathbb{V}(X)$  $$ \\mathbb{P}(| X-\\mu| \\geq t) \\leq \\frac{\\sigma^2}{t^2} \\ \\text{and} \\mathbb{P}(|Z| \\geq k) \\leq \\frac{1}{k^2}$$","tags":null,"title":"Statistics #4 | Probability Inequalities ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/200129-all-of-statistics-ch4/","year":"2020"},{"content":"3.1 Expectation of a Random Variable  The expected value, or mean, or first moment of $X$ is defined to be  $$ \\mathbb{E}(X) = \\int xdF(x) = \\begin{cases} \\sum_x xf(x) \u0026amp; (\\text{if X is discrete})\\ \\int xf(x)dx \u0026amp; ( \\text{if X is continuous}) \\end{cases}$$\n3.3 Variance and Covariance  The variance measures the spread of a distribution  $$ \\sigma^2 = \\mathbb{E}(X-\\mu)^2 = \\int (x-\\mu)^2 dF(x) $$\n The covariance and correlation between $X$ and $Y$ measure how strong the linear relationship is between $X$ and $Y$  $$\\text{Cov}(X,Y) = \\mathbb{E}((X-\\mu_X)(Y- \\mu_Y ))$$\n$$ \\text{Correlation} \\ \\rho = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} $$\n3.5 Conditional Expectation  The conditional expectation of $X$ given $Y=y$ is  $$ \\mathbb{E}(X | Y=y) = \\begin{cases} \\sum x f_{X|Y} (x|y) dx \u0026amp; (\\text{discrete case}) \\ \\int x f_{X|Y}(x|y)dx \u0026amp; (\\text{continuous case}) \\end{cases}$$\n3.6 Moment Generating Functions  The moment generating function or Laplace transform , of $X$ is defined by  $$ \\psi_X(t) = \\mathbb{E}(e^{tX}) = \\int e^{tx}dF(x) $$\n","id":38,"section":"Mathematics","summary":"3.1 Expectation of a Random Variable  The expected value, or mean, or first moment of $X$ is defined to be  $$ \\mathbb{E}(X) = \\int xdF(x) = \\begin{cases} \\sum_x xf(x) \u0026amp; (\\text{if X is discrete})\\ \\int xf(x)dx \u0026amp; ( \\text{if X is continuous}) \\end{cases}$$\n3.3 Variance and Covariance  The variance measures the spread of a distribution  $$ \\sigma^2 = \\mathbb{E}(X-\\mu)^2 = \\int (x-\\mu)^2 dF(x) $$\n The covariance and correlation between $X$ and $Y$ measure how strong the linear relationship is between $X$ and $Y$  $$\\text{Cov}(X,Y) = \\mathbb{E}((X-\\mu_X)(Y- \\mu_Y ))$$","tags":null,"title":"Statistics #3 | Expectation ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/200128-all-of-statistics-ch3/","year":"2020"},{"content":"2.1 Introduction   How do we link sample spaces and events to data? Random Variable!\n  A random variable is a mapping $ X : \\Omega \\rightarrow \\mathbb{R} $ that assigns a real number $X(\\omega)$ to each outcome $\\omega$.\n  2.2 Distribuition Functions and Probability Functions  Cumulative Distribution Function (CDF) : is the function $F_x : \\mathbb{R} \\rightarrow [0,1] $ defined by  $$F_X(x) = \\mathbb{P}(X \\leq x)$$\n Probability Mass Function : A Random Variable $X$ is discrete if it takes countably many values ${x_1, x_2, \u0026hellip;}$. We define probability mass function for $X$ by  $$f_X(x) = \\mathbb{P}(X=x).$$\n Probability Density Function : A Random Variabe $X$ is continuous if there exists a function $f_X$ such that $f_X(x) \\geq$ for all $x$, $\\int f_X(x)dx = 1$ and for every $a \\leq b$, the probability Density function is  $$ \\mathbb{P}(a\u0026lt;X\u0026lt;b) = \\int_{a}^b f_X(x)dx $$\n2.3 Some Important Discrete Random Variables  The Point Mass Distribution  $$ F(x) = \\begin{cases} 0 \u0026amp; (x\u0026lt;a)\\ 1 \u0026amp; (x\\geq a) \\end{cases} $$\n The Discrete Uniform Distribution  $$ f(x) = \\begin{cases} 1/k \u0026amp; ( \\text{for} \\ x=1, \u0026hellip; ,k ) \\ 0 \u0026amp; ( \\text{otherwise} ) \\end{cases} $$\n The Bernoulli Distribution : Let $X$ represent a binary coin flip. Then $\\mathbb{P}(X=1) = p $ and $\\mathbb{P}(X=0) = 1- p$ for some $p \\in [0,1] $. We saye that $X$ has a Bernoulli Distribution written $X \\sim Bernoulli(p)$.  $$ f(x) = p^x(1-p)^{1-x} \\ \\text{for} \\ x \\in {0,1}$$\n The Binomial Distribution  $$ f(x) = \\begin{cases} {n \\choose x} p^x (1-p)^{n-x} \u0026amp; ( \\text{for} \\ x=0, \u0026hellip; ,n ) \\ 0 \u0026amp; ( \\text{otherwise} ) \\end{cases} $$\n The Geometric Distribution  $$ \\mathbb{P}(X=k) = p(1-p)^{k-1} , \\ k \\geq 1 $$\n The Poisson Distribution : The Poisson is often used as a model for counts of rare events like radioactive decay and traffic accidents.  $$ f(x) = e^{- \\lambda} \\frac{\\lambda^x}{x!}, \\ x \\geq 0 $$\n2.4 Some Important Continuous Random Variables The Uniform Distribution:\n$$ f(x) = \\begin{cases} \\frac{1}{b-a} \u0026amp; ( \\text{for} \\ x \\in [a,b]) \\ 0 \u0026amp; ( \\text{otherwise} ) \\end{cases} $$\nGaussian Distribution : $X$ has a Normal distribution with parameters $ \\mu $ and $\\sigma$, denoted by $X \\sim N(\\mu, \\sigma^2)$\nExponential Distribution : $X$ has an Exponential distribution with parameter $\\beta$, denoted by $X \\sim \\text{Exp}(\\beta)$, if\n$$f(x) = \\frac{1}{\\beta} e^{-x/\\beta}$$\n2.5 Bivariate Distributions  Joint Mass Function : Given a pair of discrete random variables $X$ and $Y$ define the joint mass function by $f(x,y) = \\mathbb{P}(X = x, Y = y)$  2.6 Marginal Distributions  If $(X, Y)$ have joint distribution with mass function $f_{X,Y}$ then the marginal mass function for $X$ and $Y$ is defined by  $$ f_X(x) = \\mathbb{P}(X=x) = \\sum_y \\mathbb{P}(X=x, Y=y) = \\sum_y f(x,y) $$\n$$ f_Y(y) = \\mathbb{P}(Y=y) = \\sum_x \\mathbb{P}(X=x, Y=y) = \\sum_x f(x,y) $$\n For continuous random variables, the marginal densities are  $$ f_X(x) = \\int f(x,y) dy, \\ \\text{and} f_Y(y) = \\int f(x,y)dx $$\n2.7 Independent Random Variables  Two random variables $X$ and $Y$ are independent if for every $A$ and $B$,  $$ \\mathbb{P}(X \\in A, Y \\in B) = \\mathbb{P}(X \\in A) \\mathbb{P}(Y \\in B)$$\n2.8 Conditional Distributions  The conditional probability mass function is  $$ f_{X|Y}(x|y) = \\mathbb{P}(X=x | Y=y) = \\frac{\\mathbb{P}(X=x, Y=y)}{\\mathbb{P}(Y=y)} = \\frac{f_{X,Y}(x,y)}{f_Y(y)}$$\n2.9 Multivariate Distributions and IID Samples  If $X_1, \u0026hellip; , X_n $ are independent and each has the same marginal distribution with CDF $F$ we say that $X_1, \u0026hellip; , X_n$ are IID(independent and identically distributed) and we write  $$ X_1 , \u0026hellip; , X_n \\sim F.$$\n2.10 Two Important Multivariate Distributions   Multinomial : The multivariate version of a Binomial is called a Multinomial.\n  Multicariate Normal : The univariate normal has two parameters, $\\mu$ and $\\sigma$. In the multivariate version, $\\mu$ is a vector and $\\sigma$ is replaced by a matrix $\\Sigma$\n  2.11 Transformations of Random variables  Suppose that $X$ is a random variable with PDF $f_X$ and CDF $F_X$. Let $Y$ = r(X) be a function of $X$, for example, $Y = X^2$ or $Y = e^X$. We call $Y=r(X)$ a Transformation of $X$.  2.12 Transformations of Several Random variables   Three Steps for Transformation\n  For each $z$, find the set $A_z = { (x,y) : r(x,y) \\leq z }$\n  Find the CDF\n$$F_Z(z) = \\mathbb{P}(Z \\leq z) = \\mathbb{P}(r(X,Y) \\leq z) = \\mathbb{P}({ (x,y); r(x,y) \\leq z })$$\n  Them $f_Z(z) = F\u0026rsquo;_{Z}(z)$\n    ","id":39,"section":"Mathematics","summary":"2.1 Introduction   How do we link sample spaces and events to data? Random Variable!\n  A random variable is a mapping $ X : \\Omega \\rightarrow \\mathbb{R} $ that assigns a real number $X(\\omega)$ to each outcome $\\omega$.\n  2.2 Distribuition Functions and Probability Functions  Cumulative Distribution Function (CDF) : is the function $F_x : \\mathbb{R} \\rightarrow [0,1] $ defined by  $$F_X(x) = \\mathbb{P}(X \\leq x)$$","tags":null,"title":"Statistics #2 | Random Variables ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/200120-all-of-statistics-ch2/","year":"2020"},{"content":"1.1 Introduction  Probability is a mathematical language for quantifying uncertatinty  1.2 Sample Spaces and Events   Sample Space $\\Omega$ : is the set of possible outcomes of an experiment\n  Events : Subsets of Ω are called Events\n  1.3 Probability  A function $\\mathbb{P}$ that assigns a real number $ \\mathbb{P}(A) $ to every event $A$ is a probability distribution or a probability measure.  1.4 Probability on Finite Sample Spaces  If $\\Omega$ is finite and if each outcome is equally likely, then, the uniform probability distribution is  $$ \\mathbb{P} = \\frac{|A|}{| \\Omega |}$$\n And we need to count the number of points in an event using combinational methods, to compute probabilities.  $${n \\choose x} = \\frac {n \\times (n-1) \\times \u0026hellip; (n-k-1)}{k !} $$\n1.5 Independent Event  Two events $A$ and $B$ are independent if $ \\mathbb{P}(AB) = \\mathbb{P}(A) \\mathbb{P}(B)$  1.6 Conditional probability  If $\\mathbb{P}(B)\u0026gt;0$ then the conditional probability of A given B is  $$ \\mathbb{P(A|B)} = \\frac{\\mathbb{P}(AB)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} $$\n  In general, $ \\mathbb{P} (A | B) \\neq \\mathbb{P} (B | A) $\n  $A$ and $B$ are independent if and only if $ \\mathbb{P}(A | B) = \\mathbb{P}(A) $\n   $\\mathbb{P}(A | B)$ 는 new $\\mathbb{P}(A)$ if $\\mathbb{P}{B} =1$ 과 같다. 여기서 조건부확률이 위와같이 정의된 근거는 다음과 같다.\n 사건 $B$ 가 사실이므로 모든 가능한 표본은 사건 $B$에 포함되어야한다. 즉, $B$ 가 새로운 표본공간이 된다. 마찬가지로 조건부확률의 대상이 되는 사건은 기존의 $A$ 에서 $A \\cap B$ 로 바뀐다고 볼 수 있음   1.7 Bayes\u0026rsquo; Theorem   Basis of expert systems and Bayes\u0026rsquo; nets\n  Bayes\u0026rsquo; Theorem : Let $A_1, \u0026hellip; A_k$ be a partition of $\\Omega$ such that $\\mathbb{P}(A_i) \u0026gt; 0$ for each $i$. If $\\mathbb{P}(B)\u0026gt;0$, then, for each $i = 1 , \u0026hellip; k$,\n  $$ \\mathbb{P}(A_i | B) = \\frac{\\mathbb{P}(A_i B)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B | A_i) \\mathbb{P}(A_i)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B | A_i) \\mathbb{P}(A_i)}{ \\sum_j \\mathbb{P}(B | A_j) \\mathbb{P}(A_j)} $$\n 관측값을 바탕으로 에측을 하고 싶을 떄 사용 (example 1.9 참고)\n ","id":40,"section":"Mathematics","summary":"1.1 Introduction Probability is a mathematical language for quantifying uncertatinty 1.2 Sample Spaces and Events Sample Space $\\Omega$ : is the set of possible outcomes of an experiment Events : Subsets of Ω are called Events 1.3 Probability A function $\\mathbb{P}$ that assigns a real number $ \\mathbb{P}(A) $ to every event $A$ is a probability distribution or a probability measure. 1.4 Probability on Finite Sample Spaces If $\\Omega$ is","tags":null,"title":"Statistics #1 | Probability ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/200116-all-of-statistics-ch1/","year":"2020"},{"content":"0. Introduction  Neural Architecture Search : A technique for automating the design of Artificial Neural Network  1. NAS : Neural Architecture Search (Google, 2017)    Introduction : Neural Nets are still hard to design. And this paper presents a gradient-based method for finding good architectures.\n  Methods :\n  use RNN controller which returns HyperParams of Conv in the order of (FH \u0026gt; FW \u0026gt; SH \u0026gt; SW \u0026gt; NF , Fig4) to generate neural networks. After the RNN builds the desired number of layers, we stop the RNN and train the network from scratch to convergence.\n  train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set.\n  Additionally, set-selection attention is used to form skip connections.\n    Results : NAS can design several promising architectures that performs as well as DenseNet for CIFAR10, after the controller trains 12,800 architectures. And it took 28 days for CIFAR10 using 800 GPUs (Nvidia k40).\n  Fig4와 같이 Conv-Layer의 HyperParam를 출력하는 RNN Controller를 N번 반복하여 원하는 깊이(N)의 Network을 만듭니다. 그리고 이를 학습한 결과를 reward로 하여 RNN을 강화학습 합니다. -- TODO) How does set-selecetion attention work? -- 2. NAS-Net (Google, 2017)   Introduction : Applying NAS is computationally expensive. Therefore, authors propose a faster method to search a good architectures on small dataset like CIFAR-10, which can be transfered to larger datasets such as ImageNet.\n  Methods :\n  Main search method used in this work is the NAS frameworks (RNN controller, Reinforcement Learning)\n  The overall architectures of the CNNs are manually predetermined\n  To easily build scalable architectures for images of any size, we need to search two types of cells : (1) Normal cell that returns a feature map of the same size, (2) Reduction cell that returns a feature map is reduced by a factor of two.\n  The search space is 13 predefined operations, not the continuous HyperParams like NAS\n    Results : The resulting architectures approach or exceed state of the art performance in both CIFAR-10 and ImageNet. And it took 4 days using 500 GPUs (Nvidia P100).\n  NAS가 Conv-Layer의 HyperParam를 출력한 것과 달리 NASNet은 search space를 미리 정해둔 13개 연산(1x1conv, 3x3conv, ...)으로 제한합니다. -- 3. ENAS : Efficient NAS via Parameter Sharing (2018)   Introduction : The computational bottleneck of NAS(Net) is the training process of each child model from scratch. In this paper, authors proposed fast and inexpensive approach for automatic model design.\n  Methods :\n  Authors proposed an easier way to represent NAS\u0026rsquo;s model-building process using DAG (Directed Acyclic Graph). Here, red arrows define a model in the search space.\n  Parameter Sharing : Forcing all child models to share weights to eschew training each child model from scratch to convergence\n    Results : ENAS takes less than 16 hours to search for architectures using only one GTX1080Ti.\n  ","id":41,"section":"Research","summary":"0. Introduction Neural Architecture Search : A technique for automating the design of Artificial Neural Network 1. NAS : Neural Architecture Search (Google, 2017) Introduction : Neural Nets are still hard to design. And this paper presents a gradient-based method for finding good architectures. Methods : use RNN controller which returns HyperParams of Conv in the order of (FH \u0026gt; FW \u0026gt; SH \u0026gt; SW \u0026gt; NF , Fig4) to","tags":null,"title":"ML Basic #9 | Neural Architecture Search","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml09-neural-architecture-search/","year":"2019"},{"content":"0. Introduction  Pose Estimation : The task aims to detect the locations of human anatomical keypoints (e.g., elbow, wrist, etc)  1. Deep Pose (2014)   Introduction : The first major paper that applied Deep Learning to Human pose estimation\n  Method :\n  DNN-based regression : Alexnet backend (7 layers) with an extra final layer that outputs 2k joint coordinates (where $k$ is the number of joints).\n  Cascade of pose regressors : refinement of the predictions using cascaded regressors.\n  Since the ground truth pose vector is defined in absolute image coordinates and poses vary in size from image to image, authors normalize their training set (coordinates)\n  linear regression on top of the last network layer to predict a pose vector by minimizing $L_2$ distance between the prediction and the true pose vector.\n    2. Efficient Object Localization Using ConvNets (2015)   Introduction : ConvNet architecture which outpus a heatmap, describing the likelihood of a joint occurring in each spatial location\n  Method : Using an additional ConvNet to refine the localization result of the coarse heat-map.\n  Coarse Heat-Map Regression Model : Multi-resolution ConvNet that receives multiple input images with the same content but different sizes\n  Fine Heat Map Regression Model : Siamese Network with $k$ heads($k$ is the number of joint instance)\n  /figures/2019-08-07-Fig1.png ) Figure 4. Overview of our Cascaded Architecture\n  3. Simple Baselines for Human Pose Estimation and Tracking (2018)   Introduction : This work provides simple and effective baseline method for human pose estimation(Task1) \u0026amp; pose tracking(Task2).\n  Method :\n  Model Architecture : Simply adds a few deconv layers over the last conv layer in the ResNet.\n  Training Strategy : Use the label (heatmap, $H^k$ for joint $k$ is generated by applying a 2D Gaussian centered on the $k^{th}$ joint\u0026rsquo;s ground truth location with std-dev=1 pixel).\n  Flow-Based Pose Tracking: Two different kinds of human boxes, one is from a human detector and the other are boxes generated from previous frames using optical flow.\n  input, label = frames, keypoints_to_hmap(keypoints) bbox= Human_Detector(input) keypoints = hmap_to_coord(Pose_Estimator(input, bbox_det)) for i in range(len(input)): bbox_det = Human_Detector(input) bbox_flow = FlowBox_Generator() # Non-maximum suppression : unify detection and flow boxes bbox_unified = NMS(bbox_det, bbox_flow) joints = Pose_Estimator(input[1], bbox_det[1]) sim_matrix = calc_sim(output[i-1], joints) pose = (sim_matrix, id) output.append(pose) # update the output list    4. HR-Net, Microsoft (2019)   Introduction : Existing approaches consist of a stem subnetwork, which decreases the resolution based on high-to-low design pattern.\n  Method : Novel network architecture that connects high-to-low subnetworks in parallel that can maintains high-resolution representations through the whole process for spatially precise heatmap estimation.\n  /figures/2019-08-07-Fig2.png ){:width=\u0026ldquo;70%\u0026rdquo; height=\u0026ldquo;70%\u0026rdquo;}\nFigure1. Illustrating the architecture of the proposed HRNet.\n5. Higher HR-Net   Top-Down methods : take a dependency on person detector to detect person instances to reduce the problem.\n  However, they are normally computationally intensive and not truly end-to-end systems\n  ","id":42,"section":"Research","summary":"0. Introduction Pose Estimation : The task aims to detect the locations of human anatomical keypoints (e.g., elbow, wrist, etc) 1. Deep Pose (2014) Introduction : The first major paper that applied Deep Learning to Human pose estimation Method : DNN-based regression : Alexnet backend (7 layers) with an extra final layer that outputs 2k joint coordinates (where $k$ is the number of joints). Cascade of pose regressors : refinement","tags":null,"title":"MLCV #8 | Pose Estimation","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv08-pose-estimation/","year":"2019"},{"content":"5.1 Singular Value decomposition (SVD)   singular value decomposition (SVD) : is a factorization of a real or complex matrix that generalizes the eigendecomposition (EVD) of a square normal matrix to any $m \\times n$ matrix via an extension of the polar decomposition.\n$$ A = U \\Sigma V^T$$  $A \\in \\mathbb{R}^{m \\times n}$ : A given rectangular matrix $U \\in \\mathbb{R}^{m\\times m} $ : matrices with orthonormal columns, providing an orthonormal basis of $Col(A)$ $V \\in \\mathbb{R}^{n \\times n}$ : matrices with orthonormal columns, providing an orthonormal basis of $Row(A)$ $\\Sigma \\in \\mathbb{R}^{m \\times n}$ : a diagonal matrix whose entries are in a decreasing order, i.e., $\\sigma_1 \\geq \\sigma_2 \\geq \u0026hellip; \\geq \\sigma_{min(m,n)}$    \n5.2 SVD as Sum of Outer Products  SVD as Sum of Outer Products : $A$ can also be represented as the sum of outer products  $$ A = U \\Sigma V^T = \\Sigma_{i=1}^n \\sigma_i \\mathbf{u_j v_i^T}$$\n  From now on, we just want to show the equation below is true in this chapter (5.2) for the next chapter (5.3)\n$$AV = U\\Sigma \\Longleftrightarrow A=U \\Sigma V^T$$\n  Another Perspective of SVD : We can easily find two orthonormal basis sets, {$u_1, \u0026hellip; , u_n$} for $Col(A)$ and {$v_1, \u0026hellip; v_n$} for $Row (A)$, by using, Gram-Schmidt orthogonalization.\n  Are these unique orthonormal basis sets? No, then can we jointly find them such that\n  $$ A \\mathbf{v_i} = \\sigma_i \\mathbf{u_i}$$\n  Let us denote $ U = \\begin{bmatrix} \\mathbf{u_1 \\ u_2 \\ \u0026hellip; \\ u_m} \\end{bmatrix} \\in \\mathbb{R}^{m\\times n}$, $ V = \\begin{bmatrix} \\mathbf{v_1 \\ v_2 \\ \u0026hellip; \\ v_n} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}$ and $ \\Sigma = diag(\\sigma_n) \\in \\mathbb{R}^{n \\times n}$\n  $AV = U\\Sigma \\Longleftrightarrow \\begin{bmatrix} A\\mathbf{v_1} \\ A\\mathbf{v_2} \\ \u0026hellip; \\ A\\mathbf{v_n} \\end{bmatrix} = \\begin{bmatrix} \\ \\sigma_1 \\mathbf{u_1} \\ \\sigma_2 \\mathbf{u_2} \\ \u0026hellip; \\ \\sigma_n \\mathbf{u_n} \\end{bmatrix}$\n  $V^{-1} = V^T $ since $\\mathbb{R}^{n \\times n}$ has orthonormal columns.\n  Thus $AV = U\\Sigma \\Longleftrightarrow A=U \\Sigma V^T$\n  \n5.3 Geometrical Interpretation of SVD  Geometrical Interpretation : For every linear transformation (rectangular matrix) $A$ ,  one can find $n$ dimensional orthonormal basis of $V$ and transformed orthonormal basis $U$   $$A = U \\Sigma V^T \\Longleftrightarrow AV = U\\Sigma$$\n 즉, 임의의 rectangular matrix $A$ 에 대하여, 선형변환 후에 그 크기는 ($\\Sigma$에 따라) 변하지만 각 column vector들은 여전히 직교하는 ($U$) 어떤 orthogonal vector set $V$ 를 찾을 수 있다.\n   Furthermore : The figure below shows the orthogonal vector set $x, y$ and the linearly transformed one $Ax, Ay$.\n  And you can see two geometrical features :\n There could be one or more sets of orthogonal {$Ax, Ay$}. After transformed by matrix $A$, the lengths of $x,y$ are scaled by scaling factor. It is called singular value (like eigenvalue at EVD) and represented as $\\sigma_1, \\sigma_2, \\sigma_3, \u0026hellip;$ starting with the larger value.     Figure. the orthogonal vector set $x, y$ and the linearly transformed one $Ax, Ay$. [reference]  \n5.4 Computing SVD  First, we form $AA^T$ , $A^T A$ and then get SVD of each as followings :  $$ AA^T = U \\Sigma V^T V \\Sigma^T U^T = U \\Sigma \\Sigma^T U^T = U \\Sigma^2 U^T$$\n$$ A^T A = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^T \\Sigma U^T = V \\Sigma^2 V^T$$\n From the above, you can see the form of result equation is very similar to EVD:  $U$ Is a orthogonal matrix which is from the eigen-decomposition of $AA^T$, and it is called left singular vector. $\\Sigma$ is a diagonal matrix whose diagonal entries are equal to the square root of the eigenvalues from $AA^T$. $V$ is a orthogonal matrix which is from the eigen-decomposition of $A^TA$, and it is called right singular vector.     즉 주어진 행렬 $A$ 의 SVD를 구하기 위해서는 $AA^T$ 를 EVD 하여 $U$ 를 얻고, $A^TA$ 를 EVD하여 $V$ 를 얻고, 이 과정에서 얻어진 eigenvalue를 square-root 하여 diagonal entries 로 채워넣어 $\\Sigma$ 를 얻어야한다.\n \n0 $ - Positive Semi-Definite Matrices : $A \\in \\mathbb{R}^{n \\times n}$ is positive semi-definited if $\\mathbf{x}^T A \\mathbf{x} \\geq 0 $ - $A$ is positive definite if and only if the eigenvalues of $A$ are all postive. - If, $S$ is symmetric and positive-definite, then the spectral decomposition will have all positive eigenvalues: $$ S = UDU^T = \\lambda_1 \\mathbf{u_1 u_1^T} + \\lambda_2 \\mathbf{u_2 u_2^T} + ... + \\lambda_n \\mathbf{u_n u_n^T}$$ In the following, $$ AA^T = U \\Sigma V^T V \\Sigma^T Y^T = U \\Sigma^2 U^T$$ $$ A^T A = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^T \\Sigma U^T = V \\Sigma^2 V^T$$ - Can we prove that both $AA^T$ and $A^TA$ are symmetric positive-(semi-)definite? - Symmetric : $(AA^T)^T = AA^T$ and $(A^T A)^T = A^TA$ - Positive-(semi-)definited $$\\mathbf{x}^TAA^T\\mathbf{x} = (A^T\\mathbf{x})^T(A^T\\mathbf{x}) = \\| A^T\\mathbf{x}\\|^2 \\geq 0$$ $$\\mathbf{x}^TA^TA\\mathbf{x} = (A \\mathbf{x})^T(A \\mathbf{x}) = \\| A \\mathbf{x}\\|^2 \\geq 0$$ - Thus we can find 1. Orthogonal eigenvector matrices $U$ and $V$ 2. Eigenvalues in $\\Sigma ^2$ that are all Positive - Things to Note 1. Given any rectangula matrix $A \\in \\mathbb{R}^{m \\times n}$, its SVD always exists 2. Given a square matrix $A \\in \\mathbb{R}^{n \\times n}$, its eigendecomposition does not always exists, but its SVD always exists. 3. Given a square, symmetric positive (semi-)definite matrix $S \\in \\mathbb{R}^{n \\times n}$, its eigendecomposition always exists, and it is actually the sane as its SVD. -- \n5.5 Application of SVD  Recall SVD as Sum of Outer Products : matrix $A$ can also be represented as the sum of outer products  $$ A = U \\Sigma V^T = \\Sigma_{i=1}^n \\sigma_i \\mathbf{u_j v_i^T} = \\sigma_1 \\mathbf{u_1 v_1^T} + \\sigma_2 \\mathbf{u_2 v_2^T} + \\sigma_3 \\mathbf{u_3 v_3^T} \u0026hellip;$$\n Here, $\\mathbf{u_j v_i^T} $ is $m \\times n$ matrix and we can see matrix $A$ as a sum of multiple layers via SVD. That is, we can reconstruct $A$ with only a few singular values rather than using all of them ($\\Sigma$).  And this can be used for dimensionality reduction like task such as image compression.   SVD 는 분해되는 과정보다는 분해된 행렬을 다시 조합하는 과정에서 그 응용력이 빛을 발한다. 기존의 $U,\\Sigma, V^T$ 로 분해되어 있던 행렬 $A$ 를 $p$개 의 특이값만을 이용해 $A’$라는 행렬로 ‘부분 복원’ 할 수 있는데, 이 때 singular value 의 크기에 따라 $A$의 정보량이 결정되기 때문에 값이 큰 몇 개의 singular value만을 가지고도 충분히 유용한 정보를 유지할 수 있다.\n  Figure. Partial reconstruction of matrix using SVD [reference] Figure. image compression using SVD (original, p=20, p=50, p=100) [reference] ","id":43,"section":"Mathematics","summary":"5.1 Singular Value decomposition (SVD) singular value decomposition (SVD) : is a factorization of a real or complex matrix that generalizes the eigendecomposition (EVD) of a square normal matrix to any $m \\times n$ matrix via an extension of the polar decomposition. $$ A = U \\Sigma V^T$$ $A \\in \\mathbb{R}^{m \\times n}$ : A given rectangular matrix $U \\in \\mathbb{R}^{m\\times m} $ : matrices with orthonormal columns, providing an","tags":null,"title":"Linear Algebra for ML #5 | Singular Value Decomposition","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/2019-07-29-linear-algebra-for-ml-lec5/","year":"2019"},{"content":"4.0 Introduction   Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource.\n  The above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$\n  $D$ is a diagonal matrix with eigenvalues in diagonal entries\n  $V$ is a matrix whose column vectors are eigenvectors\n  $A$ is a given square matrix $A \\in \\mathbb{R}^{n \\times n}$\n    Consider the linear transformation $T(\\mathbf{x}) = A \\mathbf{x} = VDV^{-1} \\mathbf{x} $, it can be seen to be a series of following transformation. (4.5.1+ 4.5.2 + 4.5.3)\n  Change of Basis\n  Element-wise Scaling\n  Back to Original Basis\n    \n4.1 Eigenvectors and Eigenvalues   An eigenvector of a square matrix $A \\in \\mathbb{R}^{n \\times n}$ is a nonzero vector $\\mathbf{x} \\in \\mathbb{R}^n$ such that $A \\mathbf{x} = \\lambda \\mathbf{x}$ for some scalar $\\lambda$. In this case $\\mathbf{\\lambda}$ is called an eigenvalue of $A$, and such an $\\mathbf{x}$ is called an eigenvector.\n  $A \\mathbf{x}$ can be considered as a linear transformation $T( \\mathbf{x})$. If $\\mathbf{x} $ is an eigenvector, then $T(x) = A \\mathbf{x} = \\lambda \\mathbf{x} $, which means  the output vector has the same direction as $\\mathbf{x}$, but the length is scaled by a factor of $\\lambda$ \n     Fig1. Example of eigenvector and eigenvalue   And here, $8 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $ is faster than $ \\begin{bmatrix} 2\\ 6 \\\\ 5 \\ 3 \\end{bmatrix}\\begin{bmatrix} 1\\ 1 \\end{bmatrix} $\n  The equation $A \\mathbf{x} = \\lambda \\mathbf{x}$ can be re-written as $$(A-\\lambda I)\\mathbf{x}=\\mathbf{0} $$\n  $\\lambda$ is an eigenvalue of an $n \\times n$ matrix $A$ if and only if this equation has a nontrivial solution. (since $\\mathbf{x}$ should be a nonzero vector).\n  The set of all solutions of the above equation is the null space of the matrix $A-\\lambda I$, which we call the eigenspace of A corresponding to $\\lambda$\n  The eigenspace consists of the zero vector and all the eigenvectors corresponding to $\\lambda$, satisfying the above equation.\n    eigenvector는 정방행렬 $A$ 에 대하여 정의되는데 이때 $A$ 는 Linear Transform (scale, rotate, etc)으로 볼 수도 있다. 즉 어떤 선형변환 $A$ 의 eigenvector 는 선형변환 $A$를 적용했을때 방향은 변하지 않고 크기만 변하는 vector들을 의미하며 이 때, eigenvector 크기의 변화량을 eigenvalue라고 한다. For a given linear Transformation A, we can find vectors with the same direction and scaled length before and after transformation. we call these vectors as eigenvector, and the scaled factor $\\lambda$ as eigenvalues.   \n4.2 Null Space  Null Space : The null space of a matrix $A \\in \\mathbb{R}^{m\\times n}$ is the set of all solutions of a homogeneous linear system, $A \\mathbf{x = 0}$ For $A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \u0026hellip; \\\\ \\mathbf{a_m^T} \\end{bmatrix}$, $\\mathbf{x}$ should satify the following $ \\mathbf{ a_1^Tx =a_2^Tx= \u0026hellip; =a_m^Tx }=0$, That is, $\\mathbf{x} $ should be orthogonal to every row vector in $A$ Orthogonal Complement : The set of all vectors $\\mathbf{z}$ that are orthogonal to $W$ is called the orthogonal complement of $W$ and is denoted by $W ^ \\perp$.    Null Space 란 선형방정식 $Ax = 0$ 의 해들이 이루는 공간. 즉, A 의 모든 열벡터에 대하여 orthogonal인 ($A\\mathbb{x}=0$) 벡터 $\\mathbf{x}$의 집합을 null space 라고 한다. Orthogonal Complement 는 주어진 subspace와 수직인 벡터들의 공간   \n4.3 Characteristic equation $$(A - \\lambda I) \\mathbf{x = 0}$$\n  The set of all solutions of the above equation = null space of the matrix $(A - \\lambda I)$ = eigenspace of $A$ corresponding to $\\lambda$\n  The eigensapce consists of the zero vector and all the engienvectors corresponding to $\\lambda$\n  How can we find the eigenvalues?\n  If $(A-\\lambda I)\\mathbf{x = 0}$ has a nontrivial solution, then the columns of $(A - \\lambda I)$ should be noninvertible.\n  If it is invertible, $\\mathbf{x}$ cannot be a nonzero vector since\n  $$ (A - \\lambda I)^{-1} (A - \\lambda I) \\mathbf{x} = (A - \\lambda I)^{-1} \\mathbf{0} \\longrightarrow \\mathbf{x = 0} $$\n  Thus, we can obtain eignevalues by solving characteristic equation :   $$ det(A - \\lambda I) = 0$$\n  Also, the solution is not unique, and thus $A-\\lambda I$ has linearly dependent columns.\n  Once obtaining eigenvalues, we compute the eigenvectors for each $\\lambda$ by solving : $$ (A-\\lambda I) \\mathbf{x = 0}$$\n  Eigenspace : Note that the dimension of the eigenspace (corresponding to a particular $\\lambda$) can be more than one. In this case, any vector in the eigenspace satisfies\n  $$ T\\mathbf{(x)} = A \\mathbf{(x)} = \\lambda \\mathbf{(x)}$$\n  In summary, we can find all the possible eigenvalues and eignevectors, as follows.\n  First, find all the eigenvalue by solving the characteristic equation : $ det(A-\\lambda I) = 0$\n  Second, for each eigenvalue $\\lambda$, solve for $(A-\\lambda I) \\mathbf{x=0}$ and obtain the set of basis vectors of the corresponding eigenspace.\n    우선 characteristic equation $ det(A-\\lambda I) = 0$ 를 풀어 eigenvalue를 찾습니다. 이후 각각의 eigenvalue $\\lambda$ 에 대하여 $(A-\\lambda I) \\mathbf{x=0}$ 를 풀어 eigenvector를 찾습니다.    즉, eigenvector가 존재하려면 Characteristic Equation이 Linearly Dependent 한 column을 가져야하며 이는 곧 $ det(A-\\lambda I) = 0$ 가 역행렬을 갖지 않는다는 말과 같고 다시말해, Determinant가 0이 되어야합니다.\n 4.4 Diagonalization   Diagonal matrix : matrix in which the entries outside the main diagonal are all zero\n  Diagonalization : We want to change a given square matrix $A \\in \\mathbb{R}^{n\\times n}$ into a diagonal matrix via the following form : $D = V^{-1} A V $\n  For $A$ to be diagonalizable, an invertible $V$ should exist such that $V^{-1}AV$\n  How can we find an invertible $V$ and the resulting diagonal matrix $D$\n  $V = [\\mathbf{v_1, v_2 , \u0026hellip; , v_n}]$ where $\\mathbb{v_i}$ are column vectors of $V$\n  $D = diag[\\lambda_1, \\lambda_2 , \u0026hellip; , \\lambda_n ]$\n  $$ D = V^{-1}AV $$\n$$ VD = AV$$\n$$AV = A[ \\mathbf{v_1, v_2 , \u0026hellip; v_n }] = [A\\mathbf{v_1}, A\\mathbf{v_2},\u0026hellip;, A\\mathbf{v_n}], (\\text{column vector matmul})$$\n$$VD = [\\lambda \\mathbf{v_1},\\lambda \\mathbf{v_2}, \u0026hellip; ,\\lambda \\mathbf{v_n}]$$\n$$AV=VD \\longleftrightarrow [A\\mathbf{v_1}, A\\mathbf{v_2},\u0026hellip;, A\\mathbf{v_n}] = [\\lambda \\mathbf{v_1},\\lambda \\mathbf{v_2}, \u0026hellip; ,\\lambda \\mathbf{v_n}]$$\n  From above, we obtain\n$$A \\mathbf{v_1} = \\lambda \\mathbf{v_1}, A \\mathbf{v_2} = \\lambda \\mathbf{v_2}, \u0026hellip; , A \\mathbf{v_n} = \\lambda \\mathbf{v_n}$$\n   Thus $\\mathbf{v_1, v_2, \u0026hellip; v_n} $ should be eigenvectors and $\\lambda_1, \\lambda_2, \u0026hellip;. \\lambda_n$ should be eigenvalues. \n  Then, For $VD = AV \\longrightarrow D = V^{-1}AV$ to be true, $V$ should invertible.\n  In this case, the resulting diagonal matrix D has eigenvalues as diagonal entries\n   즉 대각화를 통해 얻어진 대각행렬은 해당행렬의 eigenvalue를 diagonal entries로 갖게됩니다.\n  Diagonalizable matrix : For $V$ to be invertible, $V$ should be a square matrix in $\\mathbb{R}^{n \\times n}$, and $V$ should have $n$ linearly independent columns : Hence, $A$ should have $n$ linearly independent eigenvectors.   즉, 3x3 matrix 가 diagonalizable 하다면 3개의 linearly independent한 eigenvector를 찾을 수 있다.\n 4.5 Eigen-Decomposition and Linear Transfomation   Eigendecomposition : If $A$ is diagonalizable, we can write $A = V^{-1}DV$, and we can also write $A = VDV^{-1}$, which we call eigendecomposition of $A$. So, $A$ being diagonalizable is equilvalent to $A$ having Eigendecomposition.\n  Suppose $A$ is diagonalizable, thus having eigendecomposition $ A = VDV^{-1}$, Consider the linear transformation $T(\\mathbf{x}) = A \\mathbf{x}$, it can be seen to be a series of following transformation. (4.5.1+ 4.5.2 + 4.5.3)\n$$T(\\mathbf{x}) = A\\mathbf{x} = VDV^{-1}\\mathbf{x} = V(D(V^{-1}\\mathbf{x}))$$\n   Diagonalizable $A$ 는 $VDV^{-1}$ 로 eigendecompostion이 가능하고, 이렇게 분해된 $V^{-1}, D, V$ 와의 matmul은 각각 기하학적으로 change of basis, Scaling, Back to original basis 를 순차적으로 시행하는것과 같다.\n 4.5.1 Change of Basis\n Let $\\mathbf{y} = V^{-1}\\mathbf{x}$ then $V \\mathbf{y=x}$ $\\mathbf{y}$ is a new coordinate of $\\mathbf{x}$ with respect to a new basis of eigenvectors {$\\mathbf{v_1, v_2}$} https://angeloyeo.github.io/2020/12/07/change_of_basis.html  4.5.2 Element-wise and Dimension-Wise Scaling\n  $T(\\mathbf{x}) = V(D(V^{-1}\\mathbf{x})) = V(D\\mathbf{y})$\n  Let $ z=D\\mathbf{y}$. This computation is a simple Element-wise scaling of $\\mathbf{y}$\n   즉 diagonal matrix 와의 곱은 계산상의 이점이 있다.\n 4.5.3 Back to Original Basis\n  $T(\\mathbf{x}) = V(D\\mathbf{y}) = Vz$\n  $z$ is still a coordinate based on the new basis $\\mathbf{v_1, v_2}$\n  $Vz$ converts $z$ to another coordinates based on the original standard basis.\n  That is, $Vz$ is a linear combination of $\\mathbf{v_1}$ and $\\mathbf{v2}$ using the coefficient vector $z$.\n  That is,\n  $$Vz = \\begin{bmatrix} \\mathbf{v_1} \\ \\mathbf{v_2} \\end{bmatrix} \\begin{bmatrix} z_1 \\ z_2 \\end{bmatrix} = \\mathbf{v_1}z_1 + \\mathbf{v_2}z_2$$\nFig3. Eigendecomposition as a series of transformation  즉, $T(x) = Ax$ 는 $V(D(V^{-1}x))$ 와 같이 다시 쓸 수 있고 이는 아래와 같이 해석할 수 있다\n $y=V^{-1}x$ : eigenvector들을 basis로 하는 새로운 좌표계를 구하고 $z = Dy = D(V^{-1}x)$ : Element 별로 따로따로 eigenvalue를 곱하여 간단하게 계산하는 scaling 을 해주고 $Vz = V(D(V^{-1}x))$ : 다시 원래 좌표계로 복원함  이렇게 복잡한 작업을 굳이 하는 이유는 4.5.4와 같이 계산상의 이점 때문입니다.\n  Example : $A = VDV^{-1} = \\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} -1\u0026amp;0\\\\ 0\u0026amp;2 \\end{bmatrix}\\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix}^{-1}, \\text{ and } \\bf{x}=\\begin{bmatrix} 4\\\\ 3 \\end{bmatrix} $\n$$ y = \\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix}^{-1}\\begin{bmatrix} 4\\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} $$\n$$ z = \\begin{bmatrix} -1 \u0026amp; 0\\\\ 0 \u0026amp;2 \\end{bmatrix} \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -2\\\\ 2 \\end{bmatrix}$$\n$$ Vz = \\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix}\\begin{bmatrix} -2\\\\ 2 \\end{bmatrix} = \\begin{bmatrix} -10\\\\ 0 \\end{bmatrix} $$\n 4.5.4 Linaer Transformation via $A^k$\n  Now, consider recursive transformation $A \\times A \\times A \u0026hellip; \\times A \\mathbf{x} = A^k \\mathbf{x} $\n  If $A$ is diagonalizable, $A$ has Eigendecomposition\n  $$ A = VDV^{-1}$$\n$$ A^k = (VDV^{-1})(VDV^{-1})(VDV^{-1})\u0026hellip;(VDV^{-1}) = (VD{^k}V^{-1})$$\n  Here, $D^k$ is simply computed as $diag[\\lambda_1^k, \\lambda_2^k\u0026hellip; , \\lambda_n^k]$\n  It is much faster to compute $V(D^k(V^{-1}\\mathbf{x}))$ than to compute $A^k \\mathbf{x}$\n  4.6 Summary So Far   Eigenvalue and Eigenvectors\n  Null space, Column space, and orthogonal complement in $\\mathbb{R}^n$\n  Diagonalization and Eigendecomposition\n  Linear transforamtion via eigendecomposition\n  ","id":44,"section":"Mathematics","summary":"4.0 Introduction Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource. The above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$ $D$ is a diagonal matrix with eigenvalues in diagonal entries $V$ is a matrix whose column vectors","tags":null,"title":"Linear Algebra for ML #4 | Eigen Decomposition ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/2019-07-07-linear-algebra-for-ml-lec4/","year":"2019"},{"content":"Summary   Least Square Problem : No solution exists for the linear equation $Ax=b$\n=\u0026gt; Approximate the solution $\\hat{x}$ minimizes $ b - A \\hat{x}$.\n=\u0026gt; And $ b - A \\hat{x}$ should be orthogonal to Column Space =\u0026gt; $ A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = 0$\n=\u0026gt; From this equation, We get the normal equation $A^T A\\hat{\\mathbf{x}}= A^T\\mathbf{b}$\n=\u0026gt; And if $A^T A$ is invertible, the solution $\\hat{\\mathbf{x}}= (A^T A)^{-1}A^T\\mathbf{b}$\n=\u0026gt; On the other view, $\\hat{b}$ is the orthogonal projection of $b$\n=\u0026gt; Now, we can find orthogonal projection of a vector\n=\u0026gt; we can find orthogonal vector set using given linearly independ vectors (in that vector space)\n  3.0 Least Square   Inner Product : Given $ \\mathbf{u,v} \\in \\mathbb{R}^n$, we can consider $ \\mathbf{u,v} $ as $n \\times 1$ matrices. The number $\\mathbf{u^Tv}$ is called inner product or dot product, and it is written as $ \\mathbf{u \\cdot v} $.\n  Vector Norm : The length or magnitude of $\\mathbf{v}$, can be calculated as $ \\sqrt{ \\mathbf{v} \\cdot \\mathbf{v} }$\n  $L_p$ Norm : $ \\Vert \\mathbf{x} \\Vert_p = ( \\Sigma_{i=1}^n \\vert x_i \\vert^p)^{1/p}$\n  Euclidean Norm : $L_2$ norm\n  Frobenius Norm : can be seen as an expansion of $L_2$ norm to apply to the matrix : $ \\Vert A \\Vert_F = \\sqrt{\\Sigma_{i=1}^m \\Sigma_{j=1}^n} \\vert a_{ij}\\vert^2$\n  Unit Vector : A vector whose length is 1\n  Normalizing : Given a nonzero vector $\\mathbf{v}$, if we divide it by its length, we obtain a unit vector.\n  Distance between vectors : dist($\\mathbf{u,v}$) = norm($\\mathbf{u-v}$) = $\\Vert \\mathbf{u-v} \\Vert$\n  Inner Product and Angle Between Vectors : $\\mathbf{u \\cdot v} = \\mathbf{\\Vert u \\Vert \\Vert v \\Vert} cos \\theta $\n  Orthogonal Vectors : $\\mathbf{u \\cdot v} = \\mathbf{\\Vert u \\Vert \\Vert v \\Vert} cos \\theta = 0$. That is $ (\\mathbf{u \\perp v})$\n  3.1 Introduction to Least Squares Problem   Over-Determined : number of equations \u0026gt; number of variables (we have much more data examples), usually no solution exists\n  Motivation for Least Squares : Even if no solution exists, we want to approximately obtain the solution for an over-determined system.\n   일반적으로 n개 변수의 해를 구하려면 n개 방정식이 필요하다. 이때 방정식의 개수가 변수의 개수보다 많아지는 경우를 Over-determined 라고 한다. 일반적으로 이런경우 해가 존재하지 않으나, 해가 없다라고 끝내지 않고 근사적으로 best approximate solution을 찾아보자.\n  The example below shows how to determine which solution is better between $\\begin{bmatrix} -0.12 \\\\ 16 \\\\ -9.5 \\end{bmatrix}$ and $\\begin{bmatrix} -0.4 \\\\ 20 \\\\ -20 \\end{bmatrix}$ for given Over-Determined System.   Least Squares Problem : Given an over-determined system, $A \\mathbf{x \\simeq b}$, a least squares solution $\\hat{x}$ is defined as  $$\\mathbf{\\hat{x}} = argmin_x | \\mathbf{b} - A \\mathbf{x} | $$\n  The most important aspect of the least-squares problem is that no matter what $\\mathbf{x}$ we select, the vector $A \\mathbf{x}$ will necessarily be in the column space Col $A$\n  Thus, we seek for $\\mathbf{x}$ that makes $A \\mathbf{x}$ as the closest point in Col $A$ to $\\mathbf{b}$\n  3.2 Geometric Interpretation of Least squares   Consider $ \\mathbf{\\hat{x}} $ such that $ \\hat{b} = A\\mathbf{\\hat{x}} $ is the closest point to $\\mathbf{b}$ among all points in Column Space of $A$.\n  That is, $\\mathbf{b}$ is closer to $\\mathbf{\\hat{b}}$ than to $A \\mathbf{x}$ for any other $\\mathbf{x}$.\n  To satisfy this, the vector $\\mathbf{b}- A\\mathbf{\\hat{x}}$ should be orthogonal to Col $A$\n  This means $\\mathbf{b} - A \\mathbf{\\hat{x}}$ should be orthogonal to any vector in Col A:\n  $$ \\mathbf{b} - A\\mathbf{\\hat{x}} \\perp (x_1\\mathbf{a}_1 + \u0026hellip;. x_p\\mathbf{a}_n)$$\n Or equivalently,  $$ A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = 0$$\n $\\mathbf{b} - A \\mathbf{\\hat{x}} \\perp \\mathbf{a_1} \\rightarrow \u0026gt;\\mathbf{a}_1^T(\\mathbf{b} - A \\mathbf{\\hat{x}})=0 $\n$\\mathbf{b} - A \\mathbf{\\hat{x}} \\perp \\mathbf{a_2} \\rightarrow \u0026gt;\\mathbf{a}_2^T(\\mathbf{b} - A \\mathbf{\\hat{x}})=0 $ \u0026hellip;\n$\\mathbf{b} - A \\mathbf{\\hat{x}} \\perp \\mathbf{a_m} \\rightarrow \u0026gt;\\mathbf{a}_m^T(\\mathbf{b} - A \\mathbf{\\hat{x}})=0 $\n  same with the equation below which is called a normal equation  $$A^T A\\hat{\\mathbf{x}}= A^T\\mathbf{b}$$\n3.3 Normal Equation  given a least squares problem, $Ax \\simeq \\mathbf{b}$ , we obtain normal equation  $$A^T A\\hat{\\mathbf{x}}= A^T\\mathbf{b}$$\n if $A^T A$ is invertible, then the solution is computed as  $$\\hat{\\mathbf{x}}= (A^T A)^{-1} A^T\\mathbf{b} $$\n  If $A^T A$ is not invertible, the system has either no solution or infinitely many solutions. However, the solution always exist for this \u0026ldquo;normal\u0026rdquo; equation, and thus infinitely many solutions exist.\n  If and only if the columns of $A$ are linearly dependent, $A^TA $ is not invertible. However, $A^T A$ is usually invertible.\n  3.4 Orthogonal Projection consider the orthogonal projection of $\\mathbf{b}$ onto Col $A$ as\n$$ \\mathbf{\\hat{b}} = f(\\mathbf{b})= A{\\mathbf{\\hat{x}}} = A(A^TA)^{-1}A^T\\mathbf{b} $$\n  Orthogonal set : A set of vectors {$ \\mathbf{u_1, \u0026hellip; u_p}$} in $\\mathbb{R^n}$ if each pair of distinct vectors from the set is orthogonal. That is, if $\\mathbf{u_i \\cdot u_j}=0$ whenever $i \\neq j$. So, All vectors in the orthogonal set are orthogonal to each other.\n  Orthonormal set : A set of vectors {$ \\mathbf{u_1, \u0026hellip; u_p}$} in $\\mathbb{R^n}$ if it is an orthogonal set of unit vectors (norm=1) .\n  Orthogonal and Orthonormal Basis : Consider basis {$\\mathbf{v_1, \u0026hellip; , v_p}$} of a p-dimensional subspace $W $ in $\\mathbb{R}^n$. We can make it as an orthogonal(or orthonormal) basis using Gram-Schmidt process.\n   Orthogonal Projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto Line : Consider the orthogonal projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto Line (1D subspace $L$). From the above picture, $\\hat{\\mathbf{y}}$ can be represented by multiplication of the norm (length) for $\\hat{\\mathbf{y}}$ (=$ | | \\hat{\\mathbf{y}} | |$) and the unit vector of $\\mathbf{u}$ ( = $\\mathbf{\\frac{u}{||u||}} $ ). And we can calculate $\\hat{\\mathbf{y}}$ from the inner product of $\\mathbf{y} \\text{ and } \\mathbf{u}$  $$ \\hat{\\mathbf{y}} = proj_L(\\mathbf{y})$$\n$$ = | | \\hat{\\mathbf{y}} | | \\cdot \\mathbf{\\frac{u}{||u||}} = \\mathbf{\\frac{y \\cdot u}{||u||}} \\cdot \\mathbf{\\frac{u}{||u||}} $$\n$$= \\mathbf{\\frac{y \\cdot u}{u \\cdot u}} \\cdot \\mathbf{u}, ( \\because \\mathbf{||u||}^2 = \\mathbf{u \\cdot u})$$\n Orthogonal Projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto Plane : Consider the orthogonal projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto two-dimensional subspace $W$  $$ W = span ( \\mathbf{u_1, u_2} ) $$\n$$ \\hat{\\mathbf{y}} = proj_L(\\mathbf{y}) = \\mathbf{\\frac{y \\cdot u_1}{u_1 \\cdot u_1}} \\cdot \\mathbf{u_1} +\\mathbf{\\frac{y \\cdot u_2}{u_2 \\cdot u_2}} \\cdot \\mathbf{u_2} $$\n Orthogonal Projection as a Linear Transformation : Consider a transformation of orthogonal projection $\\mathbf{\\hat{b}}$ of $\\mathbf{b}$. If orthonormal basis {$\\mathbf{u_1, u_2}$} of a subspace $W$ is given, both $\\mathbf{u_1, u_2}$ are unit vector and we can get following equation :  $$ \\mathbf{\\hat{b}} = f(\\mathbf{b}) = \\mathbf{ (b \\cdot u_1)u_1 + (b \\cdot u_2 )u_2 } $$\n$$ \\mathbf{ = (u_1^Tb)u_1 + (u_2^T b)u_2 = (u_1u_1^T + u_2 u_2^T)b = \\begin{bmatrix} u_1^T \\\\ u_2^T \\end{bmatrix} \\begin{bmatrix} u_1 \\ u_2 \\end{bmatrix} = UU^T b } $$\n3.5 Gram-Schmidt Orthonomalization  If two (or more) vectors are linearly independent, these vectors create an vector space. And we want to represent this vector space using a orthogonal (or orthonormal) set in many cases. In the previous chapter, we\u0026rsquo;ve learned orthogonal projection of one vector to the other. From this we can represent a given vector space (linearly independent vector set) with a orthogonal vector set : Gram-Schmidt   Gram-schmidt Orthogonalization : 추출하고자 하는 Feature의 방향성들이 서로 수직이 되지 않으면 중복이 되는 정보들을 추출하게 됨. 이 경우 후처리를 통해 linearly-independent 한 feature들을 수직인 형태의 vector들로 바꾸어 orthogonal vector set으로 만드는 과정\n  Example: Let $W=span( \\mathbf{x_1, x_2})$, where $\\mathbf{x_1} = \\begin{bmatrix} 3\\\\ 6\\\\ 0\\end{bmatrix}$, $\\mathbf{x_2} = \\begin{bmatrix} 1\\\\ 2\\\\ 2\\end{bmatrix}$. Construct an orthogonal basis {$\\mathbf{v_1, v_2}$} for $W$\nSolution:\n Let $\\mathbf{v_1} = \\mathbf{x_1}$. And, let $\\mathbf{v_2}$ the component of $\\mathbf{x_2}$ is orthogonal to $\\mathbf{x_1}$, i.e. $$ \\mathbf{v_2 = x_2 - proj_{v1}(x2) = x_2 - \\frac{x_2 \\cdot x_1}{x_1 \\cdot x_1} x_1 }= \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} - \\frac{15}{45}\\begin{bmatrix} 3 \\\\ 6 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 2 \\end{bmatrix} $$ Now, we get orthogonal basis for $W$. That is, $$\\mathbf{v_1} = \\begin{bmatrix} 3\\\\ 6\\\\ 0\\end{bmatrix}, \\mathbf{v_2} = \\begin{bmatrix} 0\\\\ 0\\\\ 2\\end{bmatrix}$$ We can get the orthonormal set $\\mathbf{(u_1,u_2)}$ of these vectors by dividing the norm of each vectors $$\\mathbf{u_1} = \\frac{1}{\\sqrt{45}} \\begin{bmatrix} 3\\\\ 6\\\\ 0\\end{bmatrix}, \\mathbf{u_2} = \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix}$$   3.6 QR Factorization (QR Decomposition)  QR Decomposition : is a decomposition of a matrix $A$ into a orthogonal matrix $Q$ (with Gram-Schmidt) and upper triangular matrix $R$ .  For a given lineary independent matrix, we can find orthogonal basis of the vector space using Gram-Schumidt. There should be another matrix that undo the above orthogonalization tranfrom Here, the orthogonalized matrix is represented as $Q$ and the \u0026lsquo;un-doing\u0026rsquo; matrix is represented as $R$.    $$A \\rightarrow \\text{Gram-Schumidt} \\rightarrow Q \\rightarrow \\times R \\rightarrow A $$\n Example : Consider the decomposition of linearly independeny matrix $A = \\begin{bmatrix} 12\u0026amp;-51\u0026amp;4\\\\ 6\u0026amp;167 \u0026amp;-68\\\\ -4\u0026amp;24\u0026amp;-41\\end{bmatrix}$\nSolution :\nThen, we can calculate $Q$ by means of Gram-Schmidt as follows:\n$$U = [\\mathbf{u1, u2, u3}] = \\begin{bmatrix} 12\u0026amp;-69\u0026amp;-58/5\\\\ 6\u0026amp;158 \u0026amp;-6/5\\\\ -4\u0026amp;30\u0026amp;-33\\end{bmatrix}$$\n$$ Q = [\\mathbf{\\frac{u_1}{|u_1|}, \\frac{u_2}{|u_2|}, \\frac{u_3}{|u_3|}}] = \\begin{bmatrix} 6/7\u0026amp;-69/175\u0026amp;-58/175\\\\ 3/7\u0026amp;158/175 \u0026amp;-6/175\\\\ -2/7\u0026amp;30/175\u0026amp;-33/35\\end{bmatrix} $$\nThus, we have\n$$R = Q^TA = \\begin{bmatrix} 14\u0026amp;21\u0026amp;-14\\\\ 0\u0026amp;175 \u0026amp;-70\\\\ 0\u0026amp;0\u0026amp;35\\end{bmatrix}$$\n ","id":45,"section":"Mathematics","summary":"Summary Least Square Problem : No solution exists for the linear equation $Ax=b$ =\u0026gt; Approximate the solution $\\hat{x}$ minimizes $ b - A \\hat{x}$. =\u0026gt; And $ b - A \\hat{x}$ should be orthogonal to Column Space =\u0026gt; $ A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = 0$ =\u0026gt; From this equation, We get the normal equation $A^T A\\hat{\\mathbf{x}}= A^T\\mathbf{b}$ =\u0026gt; And if $A^T A$ is invertible, the solution $\\hat{\\mathbf{x}}= (A^T A)^{-1}A^T\\mathbf{b}$ =\u0026gt; On","tags":null,"title":"Linear Algebra for ML #3 | Least Square ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/2019-07-01-linear-algebra-for-ml-lec3/","year":"2019"},{"content":"0. Introduction   What is a graph?\n  A graph is a data structure consisting of two components : vertices and edges.\n  Typically, a graph is defined as $G = (V,E)$, where $V$ is a set of nodes and $E$ is the edges between them.\n  A graph is often represented by an Adjacency matrix, $A \\in \\mathbb{R}^{N \\times N}$ and feature matrix $X \\in \\mathbb{R}^{N \\times F}$ to describe the nodes in the graph. $N,F$ is the number of nodes and features.\n    Why is a graph difficult to analyze?\n  A graph does not exist in a Euclidean space, which means it can not be represented by any coordinate systems like images or time series data.\n  A graph does not have a fixed information\n    Graph Neural Network : is a neural net that can directly be applied to graphs that provieds a convenient way for node level, edge level, and graph level prediction task.\n    Recurrent Graph Neural Network\n  Graph Convolutional Network (Spatial)\n  Graph Convolutional Network (Spectral)\n    What can GNN do?\n  Node classification : is to perdict the node embedding for every node in a graph.\n  Link prediction : is to understand the relationship between entities in graphs and predict if two entities have a connection in between.\n  Graph Classification : classify the whole graph into different categories\n    \n1. Recurrent Graph Network  Rec GNN is built with an assuption of Banach Fixed-Point Theorem : let $(X,d)$ be a complete metric space and let $T:X \\rightarrow X$ be a contraction mapping.  \n2. Graph Convolutional Network (Spatial)   The idea of convolution on an image is to sum the neighboring pixels around a center pixel, specified by a kernel size.\n  GCN adopts the same idea by aggregate the features of neighboring nodes into a center node.\n  \n3. Graph Convtolutional Network (Spectral)  As compared to other types of GNN, this type of gcn has a very strong mathematics foundation  \nReference An Introduction to GNN for analysing structured data\n","id":46,"section":"Research","summary":"0. Introduction   What is a graph?\n  A graph is a data structure consisting of two components : vertices and edges.\n  Typically, a graph is defined as $G = (V,E)$, where $V$ is a set of nodes and $E$ is the edges between them.\n  A graph is often represented by an Adjacency matrix, $A \\in \\mathbb{R}^{N \\times N}$ and feature matrix $X \\in \\mathbb{R}^{N \\times F}$ to describe the nodes in the graph.","tags":null,"title":"ML Basic #8 | Graph Neural Nets","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml08-graph-neural-networks/","year":"2019"},{"content":"2.1 Linear Equation and Linear System  Linear Equation is an equation that can be written in the form $$ a_1x_1 + \u0026hellip;. a_nx_n = b $$ The above equation can be written as $ \\textbf{a}^T \\textbf{x} = b $. Linear System is a collection of one or more linear equations # 일종의 연립방정식  몸무게(60,65,55), 키(1.7, 1.6, 1.8), 흡연여부(0,1) 에 대한 기대수명 (66,80,70) 샘플로 부터 각각의 요인들이 기대수명에 미치는 영향을 알고자 할 때   연립 방정식 $ \\begin{bmatrix} 60x_1 + 1.7x_2 + 1\\cdot{x_3}= 66 \\\\\\ 65x_1 + 1.6x_2 + 0\\cdot{x_3}= 80 \\\\\\ 55x_1 + 1.8x_2 + 1\\cdot{x_3}= 70 \\end{bmatrix} $ 의 해를 구하여 해결 할 수 있다.  동시에 이를 $ \\begin{bmatrix} 60\u00261.7\u00261 \\\\\\ 65\u00261.6\u00260 \\\\\\ 55\u00261.8\u00261 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\\\ x_2 \\\\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 66\\\\\\ 80\\\\\\ 70 \\end{bmatrix}$ 와 같이 바꾸어 역행렬을 구할 수도 있다.   즉 Linear System $A \\textbf{x} = \\textbf{b}$ 에서 $A$의 역행렬을 구함으로서 해당 Linear System을 풀 수 있다. --   Identity Matrix : $I$ is a square matrix whose diagonal entries are all 1\u0026rsquo;s and all the other entries are zeros.\n  Inverse Matrix : $A^{-1}$ is defined such that $A^{-1}A = AA^{-1} = I$\n  $$A^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d\u0026amp;{-b}\\\\ -c\u0026amp;a \\end{bmatrix}$$\n  Determinant : $det(A) = ad-bc$ determines whether $A$ is invertible\n  If $A$ is non invertible, $A \\textbf{x} = \\textbf{b}$ will have either no solution or infinitely many solutions.\n    행렬식(determinant)은 기하학적으로 넓이(2x2) 혹은 부피(3x3) 를 의미함. 예로 벡터 (3,0) 과 (2,1)이 만들어내는 평행사변형의 넓이는 3으로 이 행렬의 Det와 같다. 좀 더 나아가, Det가 0이라면 행렬을 구성하는 벡터들이 영벡터이거나 서로 동일선상에 있음.   2.2 Linear Combination   For given vectors $\\textbf{v}_1,\\textbf{v}_2, \u0026hellip; \\textbf{v}_p $ and given scalars $ c_1, c_2, \u0026hellip; c_p $,\n$ c_1\\textbf{v}_1 + c_2 \\textbf{v}_2, \u0026hellip; + c_p \\textbf{v}_p $ is called Linear Combination of vectors with weights $c$.\n  이를 활용하여 앞의 2.1에서의 Matrix equation을 다음 Vector Equation으로 바꿀 수 있다.   $\\begin{bmatrix} 60\\\\\\ 65\\\\\\ 55\\end{bmatrix} x_1 + \\begin{bmatrix} 1.2 \\\\\\ 1.6 \\\\\\ 1.8 \\end{bmatrix} x_2 + \\begin{bmatrix} 1 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} x_3 = \\begin{bmatrix} 66 \\\\\\ 80 \\\\\\ 70\\end{bmatrix} \\Longleftrightarrow \\textbf{a}_1 x_1 + \\textbf{a}_2 x_2 + \\textbf{a}_3 x_3 = b $   Determinant를 계산하는 대신 span 이라는 개념을 활용하여 위 vector equation 이 해를 갖고 있는가를 확인 할 수 있다. --  Span : Span{$\\textbf{v}_1, \u0026hellip; \\textbf{v}_p$} is defined as the set of all linear combinations of $\\textbf{v}_1, \u0026hellip; \\textbf{v}_p$. That is, Span is the collection of all vectors that can be written in the form $ c_1\\textbf{v}_1 + c_2 \\textbf{v}_2, \u0026hellip; + c_p \\textbf{v}_p $. Geometric Description of Span : $ \\textbf{v}_1 $ and $\\textbf{v}_2$ are in $\\mathbb{R}^3$ then Span is the plane in $\\mathbb{R}^3$ that contains $ \\textbf{v}_1 $ and $\\textbf{v}_2$   위의 Span의 정의에 따라 3차원 벡터 $\\textbf{v}_1 , \\textbf{v}_2$ 의 span 은 $\\textbf{v}_1 , \\textbf{v}_2$ 의 선형결합의 집합입니다. 즉 $\\textbf{v}_1 , \\textbf{v}_2$로 만들 수 있는 모든 선형결합이 되므로 위 그림에 나타난 평행사변형이 Span{$\\textbf{v}_1 , \\textbf{v}_2$} 이 됩니다.\n  Geometric Interpretation of Vector Equation : we can find whether the solution of vector equation exists using the knowledge of span. The solution of below vector equation exists only when $ \\textbf{b} \\in Span(\\textbf{a}_1, \\textbf{a}_2, \\textbf{a}_3) $.  $$\\textbf{a}_1 x_1 + \\textbf{a}_2 x_2 + \\textbf{a}_3 x_3 = b $$\n Span{$\\textbf{a}_1, \\textbf{a}_2, \\textbf{a}_3$} 이란 결국 $\\textbf{a}_1, \\textbf{a}_2, \\textbf{a}_3$ 로 만들 수 있는 모든 선형결합이므로, 이 안에 $\\textbf{b}$ 가 없다면 어떠한 $x_1, x_2, x_3$ 로도 $\\mathbf{b}$ 를 만들 수 없다는 말과 같습니다. 즉, 이 경우 해가 존재하지 않습니다.\n  Matrix Multiplications as Linear Combinations of Vectors :  $$ \\begin{bmatrix} 60\u0026amp;1.7\u0026amp;1 \\\\ 65\u0026amp;1.6\u0026amp;0 \\\\ 55\u0026amp;1.8\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 60\\\\ 65\\\\ 55\\end{bmatrix} x_1 + \\begin{bmatrix} 1.2 \\\\ 1.6 \\\\ 1.8 \\end{bmatrix} x_2 + \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} x_3 $$\n Matrix Multiplication as Column Combinations  $$ \\begin{bmatrix} 1\u0026amp;1\u0026amp;0 \\\\ 1\u0026amp;0\u0026amp;1 \\\\ 1\u0026amp;-1\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 1\\\\ 1\\end{bmatrix}1 + \\begin{bmatrix} 1\\\\ 0\\\\ -1\\end{bmatrix}2 + \\begin{bmatrix} 0\\\\ 1\\\\ 1\\end{bmatrix}3 $$\n Matrix Multiplication as Row Combinations  $$ \\begin{bmatrix} 1\u0026amp;2\u0026amp;3 \\end{bmatrix} \\begin{bmatrix} 1\u0026amp;1\u0026amp;0 \\\\ 1\u0026amp;0\u0026amp;1 \\\\ 1\u0026amp;-1\u0026amp;1 \\end{bmatrix} = 1 \\times \\begin{bmatrix} 1\u0026amp;1\u0026amp;0 \\end{bmatrix} + 2\\times \\begin{bmatrix} 1\u0026amp; 0\u0026amp;1 \\end{bmatrix} + 3 \\times \\begin{bmatrix} 1\u0026amp; -1\u0026amp; 1\\end{bmatrix}$$\n Matrix Multiplication as Sum of Outer Products  $$\\begin{bmatrix} 1\u0026amp;1 \\\\ 1\u0026amp;-1 \\\\ 1\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} 1\u0026amp; 2\u0026amp; 3 \\\\ 4\u0026amp;5\u0026amp;6 \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 1\\\\ 1\\end{bmatrix} \\begin{bmatrix} 1\u0026amp; 2\u0026amp; 3\\end{bmatrix} + \\begin{bmatrix} 1\\\\ -1\\\\ 1\\end{bmatrix} \\begin{bmatrix} 4\u0026amp; 5\u0026amp; 6\\end{bmatrix} $$\n 지금까지의 Linear Combination 내용과 관련하여 행렬의 곱을 위의 세 가지 새로운 시각으로 바라볼 수 있습니다.\n 2.3 Linear Independence   The solution for $A \\textbf{x} = \\textbf{b} $ is unique, when $\\textbf{a}_1, \\textbf{a}_2, \\textbf{a}_3 $ are linearly independent\n  Infinitely many solutions exists when $\\textbf{a}_1, \\textbf{a}_2, \\textbf{a}_3 $ are linearly dependent.\n   앞의 절에서 span의 개념을 통해 vector equation 의 해가 존재하는지 아닌지를 알아보았습니다. 여기서는 유일한(unique) 해가 존재하는지 무수히 많은 해가 존재하는지 알아보기위해 Linear Independence 라는 개념을 알아보겠습니다.\n  Linear Independence (practical) : Given a set of vectors $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ , check if $ \\textbf{v}_j$ can be represented as a linear combination of the previous vectors. If at least one such $ \\textbf{v}_j$ is found then $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ is linearly dependent.   3차원 공간에서 네개의 벡터가 주어지는 경우, 이 네 벡터는 반드시 Linear Dependent 하다. 이미 앞의 세가지 벡터로 만든 span 이 전체 집합과 같아져 버리기에 네번째 벡터는 이전까지의 span 에 반드시 포함된다.\n  Linear Independence (Formal) : Consider $ x_1 \\textbf{v}_1 + \u0026hellip; + x_p \\textbf{v}_p = \\textbf{0} $. Obviously, one solution is $\\textbf{x} = [0\u0026hellip;0]$, which we call a trivial solution. If this is the only solution $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ is the only solution. if this system also has other nontrivial solutions, $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ are linearly dependent.   $A \\textbf{x} = \\textbf{b}$ 의 vector equation 에서 solution이 존재하려면 $\\textbf{b}$가 반드시 span에 포함되어야 했습니다. 위의 정의에서 $\\textbf{0} $ 벡터의 경우 모든 span 에 반드시 포함됩니다. 따라서 최소 하나의 solution이 반드시 발생하고 이런 solution을 trivial solution이라고 합니다.\n 2.4 Basis of a Subspace (부분공간의 기저와 차원)   Subspace : is defined as a subset of $\\mathbb{R}^n$ closed under linear combination\n  A subspace is always represented as Span {$\\mathbf{v}_1 , \u0026hellip; , \\mathbf{v}_p$}\n   선형결합 이라는 연산에 닫혀있는 부분집합을 subspace 라고 부릅니다. 여기서 뽑은 두 벡터를 어떻게 선형결합을 해도 해당 space 안에 존재.\n  Basis of a subspace : a set of vectors that satisfies both of the following    Fully spans the given subspace $H$\nLinearly independent (i.e., no redundancy)     Non-Uniqueness of Basis : In the subspace $H$ (green plane), there other set of linearly independent vectors that span the subspace $H$   주어진 subspace $H$ 에 대하여 해당 평행사변형을 만드는 방법은 여러가지입니다. $\\mathbf{v}_1$, $\\mathbf{v}_2$ 외의 다른 벡터와 coefficient 를 조합하여 위 그림의 초록색 평행사변형을 만들 수도 있습니다.\n  Dimension of Subspace : Even though different basis exist for $H$, the number of vectors in any basis for $H$ will be unique. This number is the dimension of $H$. Column Space of Matrix : the subspace could be spanned by the column vector of $A$. Rank of Matrix : the dimension of the column space of $A$    Details  $$ A = \\begin{bmatrix} 1\u0026amp;0\u0026amp;2 \\\\ 0\u0026amp;1\u0026amp;1 \\\\ 1\u0026amp;0\u0026amp;2 \\end{bmatrix}, \\ \\ \\text{ Column Vector } a_1, a_2, a_3 = \\begin{bmatrix} 1\\\\ 0\\\\ 1\\end{bmatrix} , \\begin{bmatrix} 0\\\\ 1\\\\ 0\\end{bmatrix}, \\begin{bmatrix} 2\\\\ 1\\\\ 2\\end{bmatrix} $$\n  Column Space is subspace of column vectors from matrix $A$. And we need to find basis vector to get column space.\n  $a_1, a_2$ is independent each other. $a_3$ is represented as linear combination of $a_1, a_2$ . As a result, $a_3$ is dependent on $a_1, a_2$. So, $a_1, a_2$ become basis vector for colum space of $A$\n  Here, Rank of matrix $A$ is two. Because we have only two vectors $a_1, a_2$ that are linearly independent.\n  If a solution for $A \\textbf{x} = \\textbf{b} $ exists, then $b$ must be on the column space of $A$. Because, $b$ is linear combination of column vectors\n  $$ Ax = \\begin{bmatrix} 1\u0026amp;0\u0026amp;2 \\\\ 0\u0026amp;1\u0026amp;1 \\\\ 1\u0026amp;0\u0026amp;2 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = \\begin{bmatrix} 1\\\\ 0\\\\ 1\\end{bmatrix} x_1 + \\begin{bmatrix} 0\\\\ 1\\\\ 0\\end{bmatrix} x_2 + \\begin{bmatrix} 2\\\\ 1\\\\ 2\\end{bmatrix} z_3 = \\begin{bmatrix} b_1\\\\ b_2\\\\ b_3\\end{bmatrix} $$\n \n2.5 Chage of Basis (기저의변환)  Generally, vectors are defined on cartesian coordinate (표준좌표계) , and it means they are based on standard basis (1,0), (0,1). But sometimes, we need vectors on different coordinate (basis) for some reasons like computational efficiency. (e.g. eigendecomposition) When the coordinate changes, the vector itself does not changed, but the representation of vector has been changed from (2,2) to (2,0) as in figure below    위와 같이 새로운 좌표계의 도입은 기하학적으로 새로운 기저의 도입이라 볼 수 있다. 일반적인 cartesian coordinate 에서 사용하는 기저벡터는 (0,1), (1,0) 이며 이들을 standard basis라 부른다 새로운 기저를 이용한 좌표표현 : standard basis가 아닌 새로운 기저로 임의의 벡터를 표현하려면 어떻게 해야하나    Example. standard basis 에서 (2,2) 로 나타내어지던 vector를 (1,1), (-1,1) 이라는 새로운 기저로 표현한 결과 $(k1,k2)$ 를 구하면 $$ 2\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 2\\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix} = k_1\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + k_2\\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} $$ 위 식의 우변은 행렬곱의 또다른 시각에서 보았듯 열벡터의 선형결합을 이용해 아래와 같이 다시 쓸 수 있다 $$ \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1\u0026amp;-1 \\\\ 1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} k_1 \\\\ k_2 \\end{bmatrix} $$ 이를 정리하면, $$ \\begin{bmatrix} k_1 \\\\ k_2 \\end{bmatrix} = \\begin{bmatrix} 1\u0026amp;-1 \\\\ 1 \u0026amp; 1 \\end{bmatrix} ^{-1} \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} $$\n \n2.6 Linear Transformation   A transformation, function, or mapping maps an input $x$ to an output $y$\n  Domain : Set of all the possible values of $x$ (정의역)\n  Co-Domain : Set of all the possible values of $y$ (공역)\n  Range : Set of all the output values mapped by each $x$ in the domain (치역)\n    Linear Transforamtion : A transformation (or mapping) $T$ is linear if : $$ T(c \\mathbf{u}+ d \\mathbf{v}) = cT (\\mathbf{u}) + dT(\\mathbf{v})$$\n  Matrix of Linear Transformation : In general, let $T$ : $ \\mathbb{R}^n \\rightarrow \\mathbb{R}^m $ be a linear transformation. Then $T$ is always written as a matrix-vector multiplication. i.e., $T(\\mathbf{x}) = A \\mathbf{x}$\n  Here, the matrix $A$ is called the standad matrix of the linear transformation.\n  2.7 Linear Transformation in Neural Networks   Fully Connected Layers can be thought as a linear transformation\n  Fully Connected Layers usually involve a bias term, That\u0026rsquo;s why we call it an affine layer, not a linear Layers\n  Reference : colah\u0026rsquo;s blog\n  2.8 ONTO and ONE-TO-ONE   ONTO : A mapping $T$ : $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is said to be onto $\\mathbb{R}^m$ if each $\\mathbf{b} \\in \\mathbb{R}^m$ is the image of at least one $x \\in \\mathbb{R}^n$. That is, the range is equal to the co-domain. (전사)\n  ONE-TO-ONE : A mapping $T$ : $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is said to be one-to-one if each $\\mathbf{b} \\in \\mathbb{R}^m$ is the image of at most one $x \\in \\mathbb{R}^n$. That is, each output vector in the range is mapped by only one input vector, no more than that. (일대일함수)\n  ","id":47,"section":"Mathematics","summary":"2.1 Linear Equation and Linear System Linear Equation is an equation that can be written in the form $$ a_1x_1 + \u0026hellip;. a_nx_n = b $$ The above equation can be written as $ \\textbf{a}^T \\textbf{x} = b $. Linear System is a collection of one or more linear equations # 일종의 연립방정식 몸무게(60,65,5","tags":null,"title":"Linear Algebra for ML #2 | Linear System \u0026 Linear Transform ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/2019-05-08-linear-algebra-for-ml-lec2/","year":"2019"},{"content":"1.1 Scalars, Vectors, Matrices, and Tensors   Scalars : just a single number, italics in this book, lower-case variable name, such as $x$ (loswercase)\n  Vectors : an array of numbers, in order, bold lower case name, such as $\\bf x$ (bold lowercase)\n  Matrices : 2-D array of numbers, with two indices, bold uppercase variable name, such as $A$ (uppercase)\n  Tensors : an array of numbers arranged on a regular grid with a variable number of axes.\n  1.2 Matrix Additions and Multiplications   Transpose : operation on matrices. We can define a vector using the transpose operator e.g. ${\\bf x} = [x_{1}, x_{2}, x_{3}]^{T}$ , for scalar $a = a^{T}$\n  Add : we can add matrices to each other as long as they have the same shape.\n  To define the matrix product of matrices $A$ and $B$, $A$ must have the same number of columns as the number of rows in $B$\n  The matrix product is not commutative ( $AB ≠ BA$ )\n  $$C_{i,j} = \\sum_k A_{i,k}B_{k,j}$$\n  element-wise product (product of the individual elements) is denoted as $A\\odot B$\n  dot-product between two vectors $x$ and $y$ is the matrix product $x^{T}y$\n  1.3 Reference  인공지능을 위한 선형대수, edwith 주재걸 교수님 Lay et al. Linear Algebra and Its applications, 5th editionS Ian Good Fellow. Deep Learning Book Gilbert Strang\u0026rsquo;s MIT Lecture Summary of Gilbert LA  ","id":48,"section":"Mathematics","summary":"1.1 Scalars, Vectors, Matrices, and Tensors Scalars : just a single number, italics in this book, lower-case variable name, such as $x$ (loswercase) Vectors : an array of numbers, in order, bold lower case name, such as $\\bf x$ (bold lowercase) Matrices : 2-D array of numbers, with two indices, bold uppercase variable name, such as $A$ (uppercase) Tensors : an array of numbers arranged on a regular grid with","tags":null,"title":"Linear Algebra for ML #1 | Introductions ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/2019-05-07-linear-algebra-for-ml-lec1/","year":"2019"},{"content":"0. Introduction  one to one : Vanilla mode of processing without RNN, from fixed sized input to fixed-sized output one to many : Sequence output (e.g. image captioning) many to one : Sequence input (e.g. sentiment analysis) many to many : Sequence input and sequence output (e.g. Machine Translation)  Figure 1. Overview of Sequence Processing 1. Recurrent Neural Networks   Introduction : Imagine you want to classify what kind of action is happening at every point in a movie. It\u0026rsquo;s unclear how traditional neural nets could use its reasoning about previous events in the film to predict later ones.\n  Method : RNN address this issue. They are networks with loops in them, allowing information to persist.\n  Limitations : Basic RNN design struggles with long-term dependencies (i.e. longer data sequences, paragrph-level rather than sentence-level)\n  Figure 1. (Above) A standard RNN contains a single layer 2. Long Short Term Memory (1997)   Introduction : LSTMs are explicitly designed to deal with long-term dependency problem.\n  Methods: Internally, LSTMs run with Cell state, which can be thought of as the main stream, with three gates that controlls Cell state (forget gate, input gate, output gate).\n  Forget Gate : The first step in our LSTM is to decide what infromation we\u0026rsquo;re going to throw away from the cell state. This decision is made by a sigmoid layer called forget gate.\n$$ G_f(h_{t-1}, x_t) = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n  Input Gate : The next step is to decide what new information we\u0026rsquo;re going to store in the cell state. This has two parts. First, a sigmoid layer called \u0026ldquo;input gate\u0026rdquo; decides which values we\u0026rsquo;ll update. Next, a tanh layer creates a vector of new candadate values $C_t$, that could be added to the state\n$$G_i(h_{t-1},x) = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n$$\\tilde{C} = tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n  Update Cell State: It\u0026rsquo;s time to update the old cell state, $C_{t-1}$, into the new cell state $C_t$.\n$$C_t = f_t * C_{t-1} + i_t * \\tilde{C_t} $$\n  Output Gate : Finall, we need to decide what information we\u0026rsquo;re going to return. This output will be based on our cell state, but will be a filtered version\n$$ G_o(h_{t-1}, x) = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) $$\n$$ h_t = G_o * tanh(C_t)$$\n    Figure 2. An LSTM contains four interacting layers Figure 3. Step1: decide what information to thow away (L), Step2\u00263: decide what to store (M), Step4: decide what to output (R) 3. Seq2Seq (2014)   Introduction : DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality. And this is a significant limitation for sequential problems such as speech recognition and machine translation.\n  Methods : use one LSTM to read the input sequence, and use another LSTM to extract the output sequence\n  Encoder : tokenized input sentences are fed into encoder that result in context vector\n  Context Vector\n  Decoder : Basically same with RNN Language Model\n    References  Understanding LSTM Networks  ","id":49,"section":"Research","summary":"0. Introduction one to one : Vanilla mode of processing without RNN, from fixed sized input to fixed-sized output one to many : Sequence output (e.g. image captioning) many to one : Sequence input (e.g. sentiment analysis) many to many : Sequence input and sequence output (e.g. Machine Translation) Figure 1. Overview of Sequence Processing 1. Recurrent Neural Networks Introduction : Imagine you want to classify what kind of action","tags":null,"title":"ML Basic #7 | Sequential Modeling","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml07-sequential-models/","year":"2019"},{"content":"1. Introduction   NFS, Network File System, is a distributed file system protocol that allows you to mount remote directories on your server.\n  In situation that you have big data in PC-1 and need to process them with PC-2\n  \n2. Methods   On the host (192.168.205.183)\n# install nfs-kernel-server $ sudo apt update $ sudo apt install nfs-kernel-server # configure the NFS Exports on the Host Server $ sudo vi /etc/exports # add line in this format : directory_to_share client(share_option1, ... ) # ex) mnt/HW1 192.168.205.0(rw,sync,no_subtree_check) $ sudo systemctl restart nfs-kernel-server    On the client (192.168.205.184)\n# install nfs client $ sudo apt update $ sudo apt install nfs-common # mount NFS to wherever you want $ sudo mount 192.168.205.183:/mnt/HW1 /mnt/HW1    \n3. Reference  How to set up an NFS Mount on Ubuntu 18.04  \n","id":50,"section":"Engineering","summary":"1. Introduction   NFS, Network File System, is a distributed file system protocol that allows you to mount remote directories on your server.\n  In situation that you have big data in PC-1 and need to process them with PC-2\n  \n2. Methods   On the host (192.168.205.183)\n# install nfs-kernel-server $ sudo apt update $ sudo apt install nfs-kernel-server # configure the NFS Exports on the Host Server $ sudo vi /etc/exports # add line in this format : directory_to_share client(share_option1, .","tags":null,"title":"How to set up an NFS mount on Ubuntu 16.04","uri":"https://koreanbear89.github.io/engineering/2.-linux/how-to-set-up-an-nfs-mount-on-ubuntu-16.04/","year":"2019"},{"content":"Introduction   Action Classification : The task classfying an action in video sequences according to its spatio-temporal content.\n  Benchmark Set\n UCF-101 : is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. HMDB-51 Kinetics : has 400 human action classes with more than 400 examples for each class, each from a unique YouTube video.    Methods\n  CNN + RNNs\n  3D Convolutional Networks\n ResNeXt-101 : 6GFLOPs for 112x112x16    Two Stream Network (RGB + Optical Flow)\n  Two Stream 3D ConvNets\n  Feature Engineering with pre-extracted frame-level featue using CNN\n  Skeleton Based Recognition using GCN\n ST-GCN : 16 GFLOPs for one action sample        (2014) LRCN : Long-term Recurrent Convolutional Networks    Introduction : previous models assume a fixed visual representation or perform simple temporal averaging for sequential processing (such as action recog, image captioning, or etc).\n  Method : Long-term Recurrent Convolutional Networks that can learn compositional representations in space and time.\n    (2014) C3D    Introduction : 3D Convolutional Network for learning spatiotemporal feature from a large scale video dataset\n  Method : 3D ConvNets are just like standard convolutional networks, but with spatio-temporal filters (3x3x3)\n    (2014) Two Stream Network    Introduction : investigate architectures to capture the complementary information on appearance from still frames and motion between frames (optical flow).\n  Method : averaging the predictions from a single RGB frame and a stack of 10 externally computed optical flow frames, after passing them through two replicas of an ImageNet pre-trained ConvNet.\n    (2017) I3D   Introduction : A number of successful image classification architectures have been developed over the years through painstaking trial and error. Instead of repeating the process for spatio-temporal models, authors propose to simply convert successful image(2D) classification models into 3D ConvNets.\n  Method : Two-Stream Infalted 3D ConvNet (I3D) that is based on 2D ConvNet inflation\n  Inflating 2D into 3D : filters and pooling kernels of 2D ConvNets for image classification are just expanded into 3D.\n  Two 3D Streams : with one I3D network trained on RGB inputs and another on optical flow inputs. Authors trained two networks separately and averaged their predictions at test time.\n      (2017) ActionVLAD    Introduction : 3D CNN or two stream architectures disregard the long-term temporal structure of video. For example, a basketball shoot, can be confused with other actions such as running, dribbling, jumping, throwing, with only few consecutive frames. So we need a global descriptor for the entire video.\n  Methods:\n  sample frames from the entire video and get top-conv features using a pretrained CNN from RGB and flow each.\n  ActionVLAD : is a learnable spatio temporal aggregation layers. While max or average pooling are good for similar features, actionVLAD aggregates their residuals from nearest cluster centers.\n$$ V = \\sum_{t=1}^{T} \\sum_{i=1}^{N} {\\frac{e^{-\\alpha || x_{it}-c_k||^2}}{ \\sum_{k\u0026rsquo;} {e^{-\\alpha || x_{it} - c_{k\u0026rsquo;}||^2}}}} (x_{it}[j] - c_k[j]) $$\n  combine VLADs from each stream (get video-level fixed length vector) and pass it through a classifier that outputs the final classification scores.\n  Different pooling strategies for a collection of diverse features. Points correspond to features from a video and colors correspond to different sub-actions in the video.     (2017) LOUPE : 1st place at 2017 Youtube-8M    Introduction : Current method for video analysis often extract frame-level features using pre-trained CNNs. Such features are then aggregated over time e.g., by simple temporal averaging or more sophisticated recurrent neural networks such as LSTM or GRU. This work first explore clustering-based aggregation layers.\n  Method :\n  CNN Feature Extraction: The input features (frame-level) are extracted from video and audio signals.\n  Create Local feature: The pooling module (e.g. netVLAD) aggregates the extracted features into a single compact (e.g. 1024 dim) representation for the entire video.\n  Feature Enhancing: The aggregated representation is then enhanced by the Context Gating Layer.\n  Classification: Classification module takes the resulting representation as input and output scores for a pre-defined set of labels.\n      (2017) 3D ResNext    Introduction : Conventional research has only explored relatively shallow 3D architectures. Authors examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets.\n  Method : training 3D CNNs such as ResNet, ResNext, DenseNet on UCF101, HMDB-51 and so on.\n    (2018) SlowFast Networks   Introduction : The recognition of the categorical semantics (colors, textures, lighting etc.) can be refreshed relatively slowly. On the other hand, the motion being performed can evolve much faster. So authors present a two-pathway SlowFast model for video recognition\n  Method : simply can be described as a single stream architecture that operates at two different framerates.\n  Slow pathway : can be any spatiotemporal conv model. key concept is a large temporal stride τ (typically 16) on input frames, i.e., it processes only one out of τ frames.\n  Fast pathway : another conv model which have a small temporal stride\n  Lateral Connections : The information of the two pathways is fused by lateral connections which have been used to fuse optical flow based, two-stream networks.\n      (2018) ST GCN   Introduction : propose a novel model of dynamic skeletons called ST-GCN\n  Methods:\n  Pose Estimation : construct a spatiotemporal graph with the joints as graph nodes and natural connectivities in both human structures and times as graph edges.\n  Skeleton Graph Construntion : The node set $V$ has all the joints in a sequence including estimated coordinates and estimation confidence. The edge set $E$ is composed of two subset, that depicts the intra skeleton connetcion and inter-frame edges.\n  Spatial GCN : The feature map $f^t_{in} : V_t \\rightarrow R^c $ has a vector on each node of the graph.\n  on image, convolution opertion can be written as below with sampling function $p$ and weight function $w$.\n$$ f_{out}(\\mathbb{x}) = \\sum_h \\sum_w f_{in} (p(\\mathbb{x},h,w)) \\cdot w(h,w) $$\n  sampling function : On image, neigboring pixels are defined by x as center using kernel size. On graph, neighbor nodes are defined by the minimum length of path from x.\n  weight function : is similart to the kernel of 2d convolution. But we have a mapper\n$$ f_{out}(v_{ti}) = \\sum_{v_{tj} \\in B(v_{ti})} \\frac{1}{Z_{ti}(v_{tj})} f_{in}(v_{tj}) \\cdot \\mathbb{w}(l_{ti}(v_{tj})) $$\n  spatiotemporal modeling : Until now, we formulated spatial GCN, this can be expanded in temporal dimension simply. By extending the concept of neighborhood to also include temporally connected joints\n    Partition strategies : design a partitioning strategy to implement the label map $l$.\n(d) spatial configuration partitioning. The nodes are labeled according to their distances to the skeleton gravity center (black cross), root(green), near(blue), longer(yellow)     Limitations : cannot model the correlation between the joints that located further away than the maximum distance D. (left hand and right foot) : resolved by Actional Structural GCN\n    (2020) Shift GCN   Introduction : propose a novel shift graph convolutional network to overcome conventional shortcomings\n  Computational complexity of GCN based methods are pretty heavy.\n  The receptive fields of both spatial graph and temporal graph are inflexible\n      (2021, Goog Res) ViViT : A Video Vision Transformer    Introduction : We propose a pure-transformer architecture for video classification, inspired by the recent success of such models for images like ViT.\n  Methods :\n  Embedding video clips : two simple methods for mapping a video to a sequence of tokens $\\hat{z}$ (Uniform, Tubelet) and then add the positional embedding and reshape into $z$, the input to the transformer\n$$ \\mathbf{V} \\in \\mathbb{R}^{T\\times H\\times W \\times C} \\mapsto \\hat{z} \\in \\mathbb{R}^{n_t \\times n_h \\times n_w \\times n_d} \\rightarrow z \\in \\mathbb{R}^{N \\times d} $$\n  Uniform frame sampling (Figure2) : simply sample $n_t$ frames, and embed each 2D frame independently following ViT (Conv2D + Concat)\n  Tubelet Embedding (Figure3) : extension of ViT\u0026rsquo;s embedding to 3D and corresponds to a 3D convolution.\n    Transformer Models for VIdeo\n  Model 1) Spatio-temporal attention : simply forwards all spatio-temporal tokens extracted from the video\n  As it models all pairwise interactions, Multi-Headed Self Attention has quadratic complexity with respect to the number of tokens.\n  motivates the development of more efficient architectures\n    Model 2) Factorised encoder : consists of two separate transformer encoders\n  spatial encoder : only models interactions between tokens extracted from the same temporal index.\n  temporal encoder: consisting ofLttransformer layers to model in-teractions between tokens from different temporal indices.\n        Results\n  Input Encoding : tubelet embedding initialised using the “central frame” method (Eq. 9) performs well, outperforming the others (Table1)\n  Model Variants : The unfactorised model (Model 1) performs the best on Kinetics 400. However, it can also overfit on smaller datasets such as Epic Kitchens, where we find our “Factorised Encoder” (Model 2) to perform the best\n      (2021) TimeSformer : Is Space-Time Attention All You Need for Video Understanding?   Introduction : We present a convolution-free approach to video classification\n  Methods : TimeSformer (Time-Space Transformer)\n  Preprocessing\n  Input Clip : TimeSformer takes input clip $X \\in \\mathbb{R}^{H \\times W \\times 3 \\times F}$\n  Decomposition into patches ; each frame is decomposed into $N$ non-overlapping patches of $\\mathbf{x_{(p,t)}} \\in \\mathbb{R}^{3P^2}$\n  Linear embedding : embedding vector $\\mathbb{z}{(p,t)} \\in \\mathbb{R}^D$ by means of a learnable matrix $E \\in \\mathbb{R}^{D \\times 3P^2}$ with trainable positional embedding $e^{pos}{(p,t)} \\in \\mathbb{R}^D$ : $\\mathbf{z}{(p,t)} = E\\mathbf{x}{(p,t)} + e^{pos}_{(p,t)}$\n  Classification Embedding : The final clip embedding is obtained from the final block for the classification token, On top of this representation we append a 1-hidden-layer MLP, which is used to predict the final video classes.\n output을 다시 aggregation 해서      Modelling\n  Query Key Value computation : At each block $l$, a query/key/value vector is computed for each patch from the representation $z^{(l−1)}_{(p,t)}$encoded by the preceding block\n  Self-attention computation : via dot-product of query and key vector\n  Encoding : The encoding is obtained by computing the weighted sum of value vectors using self-attention coefficients from each attention head\n    Space-Time Self Attention Models : temporal attention and spatial attention are separately applied one after the other\n we first compute temporal attention by comparing each patch(p,t) with all the patches at the same spatial location in the other frames      Conclusion : conceptually simple, achieves state of the art results on major action recognition tasks, has low training and inference cost, adn can be applied to clips of over one minute, thus enabling long-term video modeling.\n  class PatchEmbed(nn.Module): self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+1, embed_dim)) ... def forward_features(self, x): x, T, W = self.patch_embed(x) cls_tokens = self.cls_token.expand(x.size(0), -1, -1) x = torch.cat((cls_tokens, x), dim=1) x = x + self.pos_embed ... class Attention(nn.Module): self.qkv = nn.Linear(dim, dim * 3) self.norm1 = norm_layer(dim) ... def forward(self, x): q, k, v = qkv[0], qkv[1], qkv[2] attn = (q @ k.transpose(-2, -1)) * self.scale attn = softmax(dim=-1) x = (attn @ v).transpose(1, 2).reshape(B, N, C)  ","id":51,"section":"Research","summary":"Introduction Action Classification : The task classfying an action in video sequences according to its spatio-temporal content. Benchmark Set UCF-101 : is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. HMDB-51 Kinetics : has 400 human action classes with more than 400 examples for each class, each from a unique YouTube video. Methods CNN + RNNs 3D Convolutional Networks ResNeXt-101 : 6GFLOPs","tags":null,"title":"MLCV #7 | Action Classification","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv07-video-classification/","year":"2018"},{"content":"1. K-Nearest Neighbors   Introduction : a method that predicts a given data based on the nearest K neigbors in existing data.\n  Method : If regression, return mean value of k nearest neibors, if classification, return the mode class of k nearest neibors ,\n  Hyperparameter : k(odd number, if k is too big, underfit, if k is too small, overfit), distance metric(L1, Manhattan, L2, Euclidean)\n  \n2. Linear Regression   Introduction : Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data.\n  Methods : The most common method fitting a regression line is least squares. This method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line. And then we can optimize the squared error using gradient descent.\n  \n3. Logistic regression   Introduction : Linear Regression과 달리 입력데이터가 범주형(categorical) 데이터가 되면 문제가 발생, Logistic Regression은 regression 문제를 풀기보다 classification 문제를 풀기 위함이라고 봐야함.\n  Methods : Logistic Regression의 경우 sigmoid를 이용하기에 mse를 사용하면 우리가 생각하는 2차함수의 convex 형태가 아닌 여러 local minimum을 갖게됨. 따라서 Logistic Regression에 대하여는 Cross Entropy 를 사용하고, Gradient Descent를 사용함.\n  \n4. RBM : Restricted Boltzmann Machine (RBM)   Introduction : RBMs are two layered NN with generative capabilities, that have the ability to learn a prob dist over its input. RBMs can be used for dimensionality reduction, classification, regression, and so on.\n  Architecture : input layer, hidden layer 각 한층씩, 전체 두개 층으로 이루어져있으며, Boltzmann Machine이 같은 층의 unit끼리도 연결되어있는 반면 같은층의 unit과는 연결 되어있지않음.\n  Training : 두개 이상의 확률 변수의 결합 확률 분포로부터 일련의 표본을 생성하는 확률적 알고리즘인 Gibbs Sampling을 이용해 학습 (Unsupervised Learning)\n  입력을 넣어주고 그대로 다시 backward 하여 recon한 결과가 (마치 autoencoder 처럼) 입력과 같아지도록 학습하는듯\n  forward : $ y_{out} = \\sigma (wx + b)$\n  backward (reconstruct the input) : $ y_{recon} = y_{out} x + b$\n  KL Divergence can be considered as the reconstruction error: $ KL(x|y_{recon}) $\n  Reduce KLD in iterative manner : common training algorithms for RBMs approximate the log-likelihood gradient given some data and perform gradient ascest on these approximations.(Gibbs Sampling)\n    \n5. DBN : Deep Belief Network   Architecture : Multiple RBMs can also be stacked and can be fine tuned through the process of gradient descent and backpropagation.\n  Train : 첫번째 RBM의 학습이 완료되면 첫번째 RBM의 parameter를 고정하고, 첫번쨰 RBM의 hidden unit을 입력으로 하는 다음 RBM을 학습한다. 이렇게 쌓인 DBN을 gradient descent 와 backpropagation으로 finetune 할 수 있는듯.\n  joint probability 를 잘 표현해서라기 보다는, 이 모델로 다른 DNN을 pretrain 하고 학습했을때, Overfit이 완화되어 성능향상을 보였기때문에 당시 주목을 받음\n  그러나, 데이터가 충분한 경우 위 DBN을 이용한 Weight Init 보다 Random Init의 성능이 좋다고 알려지며 Practical 한 목적으로는 거의 사용하지 않음.\n  \n6. HDBSCAN (Hierarchical DBSCAN, 2013)   Introduction : HDBSCAN extends DBSCAN by converting it into a hierarchical clustering algorithm, and then using a technique to extract a flat clustering based in the stability of clusters.\n  Methods :\n  Transform the space according to the density/sparsity.\n  clustering의 core인 single linkage algorithm 은 noise에 굉장히 민감한데, 이는 잘못된 위치에 있는 노이즈가 cluster들 사이를 연결하여 둘을 하나의 cluster로 묶어버리는 경우가 많기 때문. 따라서 좀 더 robust한 distance가 필요\n  Mutual Reachability Distance : a 와 b의 MRD 는 { distance(a, k-nearest neighbor of a), distance(b, knn of b), distance (a,b) } 중 가장 큰 값 으로 정의. 이로써 Dense한 지점에서는 a,b 사이의 거리를 metric으로 사용하고 Sparse한 지점에서는 a,b 각각의 k 이웃까지의 거리를 사용하여 robust 하게 적용가능 [참고]\n$$ MRD = max[core_k(a), core_k(b), d(a,b)] $$\n    Build the minimum spanning tree of the distance weighted graph : 이제 각 데이터들을 node로 하고 데이터들 사이의 MR_distance 를 score로 marking한 (weighted) edge(선) 로 데이터들을 이은 그래프를 만듦. 어느 점에서 시작하든 distance는 변하지 않으므로 최종적으로는 동일한 그래프가 만들어짐 (fig1)\n  Construct a cluster hierarchy of connected components. : 이제 앞서 marking한 distance가 먼 edge부터 threshold를 낮춰가며 연결을 끊어줌\n  Condense the cluster hierarchy based on minimum cluster size : minimum cluster size에 근거하여 적당한 부분에서 잘라주고\n  Extract the stable clusters from the condensed tree : we want the choose clusters that persist and have a longer lifetime;\n     \u0026nbsp \u0026nbsp \u0026nbsp \u0026nbsp  ","id":52,"section":"Research","summary":"1. K-Nearest Neighbors Introduction : a method that predicts a given data based on the nearest K neigbors in existing data. Method : If regression, return mean value of k nearest neibors, if classification, return the mode class of k nearest neibors , Hyperparameter : k(odd number, if k is too big, underfit, if k is too small, overfit), distance metric(L1, Manhattan, L2, Euclidean) 2. Linear Regression Introduction : Linear","tags":null,"title":"ML Basic #6 | Conventional Models","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml06-conventional-models/","year":"2018"},{"content":"1. How to tune Batch Size   Small batch를 사용하는 것이 generalization 측면에서 더 좋은 영향을 끼친다고 알려져있고 적은 메모리에도 network을 올릴 수 있음\n  Large Batch를 사용하면 학습과정에서의 loss, train_acc 등의 fluctuation을 줄일 수 있고, gpu를 활용한 parallel computation resource 를 줄일 수 있어 더 빠르게 학습이 가능함. 즉 1개 데이터의 loss로 100번 backprop 하는 것 보다 100개 data의 loss로 1번 backprop 하는게 계산효율이 좋다.\n  일반화 성능이 떨어지는 large batch size는 32-512 개의 일반적인 batch size에 비해 훨씬 높아야하기에 대부분의 경우 그냥 gpu에 올라가는 최대한의 batch size를 사용해도 일반화 성능이 떨어질 걱정을 크게하지는 않아도 됨\n  Revisiting Small Batch Training for DNNS (2017)\n  On Large-Batch Training for DL : Generalizaiton Gap and Sharp Minima (2017)\n  \n2. Regularization   L1, L2 모두 cost function의 크기를 키워 점진적으로 parameter의 크기가 커지는것을 방지함.\n  w가 작아지도록 학습을 한다는 것은 local noise가 학습에 큰 영향을 끼치지 않는다는 것을 의미한다, 이로인해 outlier의 영향을 적게 받도록 함.\n  L1 norm은 경우에 따라 특정 feature를 0으로 처리해버리는 feature selection의 기능을 갖는다고도 함.\n  Dropout, EarlyStopping\n  \n3. Batch Normalization   Introduction : DL 학습과정에서 $\\frac{x-\\mu}{\\sigma}$ 형태의 Normalization은 다음과 같은 이점을 갖는다. [참고]\n Make training process less senstive to scale of features. Regularization behaves differently for different scaling. Makes optimization well-conditioned    Methods:\n 주어진 Batch에 대하여 평균을 뺴고 분산으로 나누어 정규화하고 학습가능한 parameter인 $\\gamma$ 를 곱하고 $\\beta$를 더함 : scale and shift (기울기와 평행이동을 통해 표현력을 늘림)    \n4. Weight Initialization   Introduction : DL Model의 초기 가중치 설정 방법\n  Xavier Initialization : 이전 layer와 다음 layer의 node 개수에 따라 Normal Distribution의 분산을 조절, Sigmoid, TanH 와 같은 비선형 activation function에서 효과적인 결과를 보여줌, ReLU와 같이 사용하면 출력값이 0으로 수렴하는 문제가 있다고는 함.\n$$ W \\sim N(0, Var(W)), \\quad Var(W)=\\sqrt{\\frac{2}{n_{in} + n_{out}}}$$\n  He Initialization : Xavier 가 ReLU와 함께 사용되면 비효율적인 결과가 나타남.\n$$ W \\sim N(0, Var(W)), \\quad Var(W)=\\sqrt{\\frac{2}{n_{in}}}$$\n    \n5. Multitask Learning   Introduction : is a subfield of ML in which multiple tasks are simultaneously learned by a shared model. Such approaches offer advantages like improved data efficiency, reduced overfitting through shared representations, and fast learning by leveraging auxiliary information. However, the simultaneous learning of multiple tasks presents new design and optimization challenges, and choosing which tasks should be learned jointly is in itself a non-trivial problem.\n  Methods:\n Hard Parameter Sharing : is generally applied by sharing the hidden layers between all tasks, whilekeeping several task-specific output layers Soft Parameter Sharing : each task has its own model with its own parameters.The distance between the parameters of the model is then regularized in order to encourage theparameters to be similar    References\n An Overview of Multitask Learning in Deep Neural Nets      일반적으로는 하나의 모델로 여러가지 task를 동시에 수행한 뒤 sum of weighted loss로 다시 이 모델을 학습   Imbalanced Dataset  Introduction : An imbalanced classification problem is an example of a classification problem where the distribution of examples across the known classes is biased or skewed. The distribution can vary from a slight bias to a severe imbalance where there is one example in the minority class for hundreds, thousands, or millions of examples in the majority class or classes. Methods :  Data Replication : Replicate the available data until the number of samples are comparable Synthetic Data: Images: Rotate, dilate, crop, add noise to existing input images and create new data Modified Loss: Modify the loss to reflect greater error when misclassifying smaller sample set Change the algorithm: Increase the model/algorithm complexity so that the two classes are perfectly separable (Con: Overfitting)ML    ","id":53,"section":"Research","summary":"1. How to tune Batch Size Small batch를 사용하는 것이 generalization 측면에서 더 좋은 영향을 끼친다고 알려져있고 적은 메모리에도 network을 올릴 수 있음 Large Batch를 사용하면","tags":null,"title":"ML Basic #5 | Training Techniques","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml05-training-techniques/","year":"2018"},{"content":"0. Introduction   Image Retrieval : aims to find similar images to a query image among an image dataset.\n  Tech Trend :\n  Conventional Methods relying on local descriptor matching (scale invariant features - local image descriptors - reranking with spatial verifications)\n  using FC layers after several conv layers as global descriptors [A Babenko et al, A Gordo et al.]\n  using global pooling methods from the activations of conv layers.\n  boost the performance by combining different global descriptors which are trained individually.\n    \n1. BoF, BoW (Bag of Features, Bag of Visual Words)   Introduction : BoW is a simplifying representation used in NLP and information retrieval.\n  Methods : BoF groups local descriptors.\n  Local Feature Extraction : Extract local features from image (SIFT, SURF, small img patches)\n  Clustering : Cluster (k-means) extracted features and find center features (codeword) of each cluster\n  Image representation : Represent each image using histogram of codeword.\n  Learning and Recognition : BoW 기반의 학습 및 인식방법은 크게 Bayesian 확률을 이용한 Generative방법과, SVM등의 분류기를 이용한 Discriminative 방식이 있다.Bayesian은 물체 클래스별 히스토그램 값을 확률로 보고 물체를 분류하는 것이고, Discrim 방식은 Histogram을 feature vector로 보고 SVM등의 분류기에 넣고 클래스 경계를 학습시키는 방법이고,\nFigure 1. Overview of BoW     \n1. VLAD (Aggregating Local Descriptors) (2010)   Introduction : propose a simple yet efficient way to aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation.\n  Fisher Vector : transform a input variable-size set of independent samples into a fixed size vector representation\n  A Gaussian Mixture Model (GMM) is used to model the distribution of features(e.g. SIFT) extracted over the image.\n  The Fisher Vector encodes the gradients of the log-likelihood of the features under the GMM, with respect to the GMM parameters.\n    VLAD (Vector of Locally Aggregated Descriptor) : is a feature pooling method, which can be seen as a simplification of the Fisher Kernel. VLAD encodes a set of local feature descriptors extracted from an image using a clustering method such as GMM or K-means.\n  accumulate the differences $x-c_i$ for each visual word $c_i$.\n  subsequently $L_2$ normalized by $v = v / ||v||_2$\n  Can be written using $a_k$ that assigns descriptor $x_i$ to specific cluster centres $c_k$.\n$$ v_{i,j} = \\sum_{x \\in C} x_j-c_{i,j} = \\sum_{i=1}^N a_k(x_i)(x_i(j)-c_k(j))$$\n    \n2. NetVLAD (2016)   Introduction : develop a cnn architecture that aggregates mid-level conv features into a compact single vector representation using generalized VLAD layer, NetVLAD.\n  Methods : (i) extract top conv featues using pretrained CNN (ii) and pool these features using netVLAD\n  netVLAD : The source of discontinuous in VLAD is hard assignment $a_k(x_i)$ of descriptor $x_i$ to specific cluster centres $c_k$. (If $c_k$ is the closest cluster, $a_k=1$, else, $a_k=0$). Authors replace it to soft assignment (softmax of -distances to each clusters).\n$$ a_k(x_i) = softmax( -|x_i - c_k |^2) = \\frac{e^{-\\alpha | x_i| ^2 + 2\\alpha c_k x_i + |c_k |^2}}{\\sum_{k\u0026rsquo;} e^{-\\alpha | x_i| ^2 + 2\\alpha c_k x_i + |c_{k\u0026rsquo;} |^2}} = \\frac{e^{2\\alpha c_k x_i + |c_k |^2}}{\\sum_{k\u0026rsquo;} e^{ 2\\alpha c_k x_i + |c_{k\u0026rsquo;} |^2}} = \\frac{e^{w_k^T x_i + b_k}}{\\sum_{k\u0026rsquo;} e^{w_{k\u0026rsquo;}^T x_i + b_{k\u0026rsquo;}}}$$\n$$V(j,k) = \\sum_{i=1}^{N} a_k (x_i)(x_i(j) - c_k(j))$$\n  \n1. Global Descriptors (~2018)   SIFT, SPoC : sum pooling from the feature map which performs well mainly due to the subsequent descriptor whitening.\n  MAC , regional MAC : performs max pooling (MAC) over regions then sum over the regional MAC descriptor at the end.\n  GeM: generalizes max and average pooling with a pooling parameter\n  weighted sum pooling, weighted GeM, multiscale RMAC, etc.\n  The performance of each global descriptor varies by dataset as each descriptor has different properties. For example, SPoC activates larger regions on the image representation while MAC activates more focused regions\n  \n2. SPoC, Sum Pooling of Convolution (2015)   Introduction : investigate possible ways to aggregate local deep features to produce compact global descriptors for image retrieval.\n  Methods :\n  Sum pooling : The construction of the SPoC descriptor starts with the sum pooling of the deep features.\n$$ \\psi_1(I) = \\sum_{y=1}^{H} \\sum_{x=1}^{W} f(x,y)$$\n  Centering prior : objects of interest ted to be located close to the geometrical center of an image. So, incorporate such centering prior using coefficients $\\alpha(w,h)$ , (Gaussian)\n$$ \\psi_2(I) = \\sum_{y=1}^{H} \\sum_{x=1}^{W} \\alpha(x,y)f(x,y)$$\n  Post processing : The obtained representation $\\psi(I)$ is subsequently l2 normalized, then PCA compression and whitening are performed.\n    \n3. MAC, RMAC, Maximum Activation of Convolution (2015)   Introduction : revisit both retrieval stages, namely initial search and reranking\n  Method :\n  Maximum Activation of Convolutions (MAC) : the feature vector constructed by a spatial max-pooling over all feature map $\\chi_i$ from last conv.\n$$ \\mathbb{f_{\\Omega}} = [f_{\\Omega,1}, f_{\\Omega,2}, \u0026hellip; ,f_{\\Omega,K}]^T, with f_{\\Omega,i} = max \\chi_i(p)$$\n  regional MAC : divide conv feature map to multiple regions(for WxH dim not C) and apply MAC for each regions and post-process it. (l2 and PCA-whitening)\n  Two images are compared with the cosine similarity of the K-dim vector produced as described above.\n    \n4. GeM, Generalized Mean Pooling (2017)   Introduction : propose a novel trainable Generalized Mean Pooling layer that generalizes max and average pooling and show that it boosts retrieval performance\n  Method :\n  ConvNet Backbone : given an input image, the output is a 3D tensor $\\chi$ of $W \\times H \\times K$ dimensions\n  GeM : add a pooling layer that takes $\\chi$ as an input and produces a vector $\\mathbb{f}$ as an output of the pooling process.\n$$ \\mathbb{f_{\\Omega}} = [f_{\\Omega,1}, f_{\\Omega,2}, \u0026hellip; ,f_{\\Omega,K}]^T, f_{\\Omega,k} = ( \\frac{1}{|\\chi_k|} \\sum_{x \\in \\chi_k}x^{p_k})^{\\frac{1}{p_k}}$$\n    \n5. Combination of Multiple Global Descriptors (2019)   Introduction : Ensembling different models and combining multiple global descriptors lead to performance improvement. However, these processes are not only difficult but also inefficient with respect to time and memory. Here, authors propose a novel framework that exploits multiple global descriptors to get an ensemble effect while it can be trained in an end-to-end manner.\n  Method : Proposed framework consists of a CNN backbone and two modules. The first main module learns an image representation, which is a combination of multiple global descriptors. Next, an auxiliary module to fine-tune a CNN with a classification loss.\n  Backbone Network : can use any CNN such as Inception, ShuffleNet, Resnet. authors use ResNet50 as a baseline backbone.\n  Main Module - Multiple Global Descriptors : main module has multiple branches that output each image representation by using different global descriptors (SPoC, MAC, GeM) on the last conv layer. And these discriptions are concatenated after whitening(PCA, FC) and l2 normalization .\n  Auxiliary Module : finetunes the CNN backbone based on the first global descriptor of the main module by using a classification loss (train a CNN backbone with a classification loss and then fine-tune the network with a triplet loss). Additional temperature scaling and label smoothing for performance improvement.\n    \n","id":54,"section":"Research","summary":"0. Introduction Image Retrieval : aims to find similar images to a query image among an image dataset. Tech Trend : Conventional Methods relying on local descriptor matching (scale invariant features - local image descriptors - reranking with spatial verifications) using FC layers after several conv layers as global descriptors [A Babenko et al, A Gordo et al.] using global pooling methods from the activations of conv layers. boost the","tags":null,"title":"MLCV #6 | Image Retrieval","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv06-image-retrieval/","year":"2018"},{"content":"0. Introduction  Image Style Transfer : The task of migrating a style from one image (Style Image) to another (Content Image).   1. Image Style Transfer using CNNs (2016)   Introduction : Introduce a algorithm that can separate and recombine the image content and style of natural images.\n  Method : Extract feature maps $F_l$ from each input image $I_{content} $ and $I_{style}$ using pretrained networks at $l_{th}$ layer. Then, optimize $I_{output}$ to have similar contents with $I_{content}$ and similar style with $I_{style}$.\n  The content loss between $I_{content}$ and $I_{output}$ is calculated using Frobenius norm at $l_{th}$ layer :\n$$L_{content} = \\Sigma(F_{output} - F_{content})^2$$\n  The style loss between $I_{style}$ and $I_{output}$ at $l_{th}$ layer is calculated using Frob. norm and Gram matrix. The style loss is defined by weighted sum of $L_{style}^l$ :\n$$L_{style} = \\sum w_l \\cdot L_{style}^l = \\sum w_l (\\sum(Gram(F_{output}) - Gram(F_{style})))$$\n     The final obejective function is defined as :\n$$ L_{total} = \\alpha L_{content} + \\beta L_{style}$$\n   2. pix2pix (2016)  Introduction : Conditional adversarial networks as a general-purpose solution to image-to-image translation problems.   Method : The generator translate the input image (gray-scale) to target domain(color). And, the discrimator distinguishes between the converted image and real image.  $$ \\text{GAN objective} = arg \\min_G \\max_D L_{cGAN}(G,D) + \\lambda L_{L1}(G) $$\n  Adversarial Loss , the first term, is from cGAN loss :\n$$ L_{cGAN}(G,D) = \\mathbb{E}_y[log(D(x,y))] + \\mathbb{E}_x[log(1-D(G(x)))]$$\n  Reconstruction Loss, the second term, is from traditional CNN based loss, which means pixel-wise differences between $y$ and $G(x)$.\n$$ L_{L1}(G) = \\mathbb{E}_{x,y}[| y-G(x) |] $$\n  Generator architecture is based on U-Net and discriminator architecture is based on PatchGAN(Markovian discrimator).\n  Generator is fed on real satellite image instead of latent vector and the pair of images are fed into the discriminator.\nFigure. Overview of pix2pix Architecture   기존의 방법대로 CNN (U-Net)이 합성한 칼라이미지와 실제 칼라이미지의 L1 distance 정도만을 loss로 network을 학습시켜보면 대충 blur된? 얼버무려진? (un-realistic한 ) 영상) 나옴. 이를 보완하기 위해 Discriminator를 추가하고 loss function에 discriminator 관련 항을 추가하여 generator가 더 realistic한 영상을 합성하는 방향으로 학습하게 함. --  3. cycleGAN (2017)   Introduction : For many tasks, paired training data will not be available. Authors present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples.\n  Method : using two discriminator (one for discriminating real $y$ and synthesized $G(x)$, the other for real $x$ and synthetic $F(y)$ ). And additional cycle consistency loss for preventing mode collapse that always return same output but very realistic.\n  $$ L(G,F,D_X,D_Y) = L_{GAN}(G,D_Y,X,Y) + L_{GAN}(F,D_X,Y,X) + \\lambda L_{cyc}(G,F) $$\n  Adversarial Loss : For the mapping function $G: X \\rightarrow Y $ and its discrimator $D_Y$, we express the objective as :\n$$ L_{GAN} (G,D_Y,X,Y) = \\mathbb{E}{y~p{data}(y)}[(logD_Y(y))] + \\mathbb{E}{x~p{data}(x)}[(1-logD_Y(G(x)))] $$\n$$ L_{GAN} (F,D_X,Y,X) = \\mathbb{E}{x~p{data}(x)}[(logD_X(x))] + \\mathbb{E}{y~p{data}(y)}[(1-logD_X(F(y)))] $$\n  Cycle Consistency Loss : Adversarial Losses alone cannot guarantee that the learned function can map an individual input $x_i$ to a desired output $y_i$. So authors argue that the learned mapping functions should be cycle-Consistent :\n$$ \\text{Forward cycle consistency} = \\mathbb{E}{x~p{data}(x)}[| F(G(x))-x |_1 ] $$\n$$ \\text{Backward cycle consistency} = \\mathbb{E}{y~p{data}(y)}[| G(F(y))-y | _ {1} ] $$\n$$ L_{cyc} (G,F) = \\text{forward} + \\text{backward} $$\n  Figure. Overview of cycleGAN Architecture ","id":55,"section":"Research","summary":"0. Introduction Image Style Transfer : The task of migrating a style from one image (Style Image) to another (Content Image). 1. Image Style Transfer using CNNs (2016) Introduction : Introduce a algorithm that can separate and recombine the image content and style of natural images. Method : Extract feature maps $F_l$ from each input image $I_{content} $ and $I_{style}$ using pretrained networks at $l_{th}$ layer. Then, optimize $I_{output}$ to","tags":null,"title":"MLCV #5 | Image Style Transfer","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv05-image-style-transfer/","year":"2018"},{"content":"0. Introduction   Image Segmentation : The process of assigning a label to every pixel in the image.\n  Semantic Segmentation : treats multiple objects of the same class as a single entity.\n  Instance Segmentation : treats multiple objects of the same class as distinct individual objects.\n  1. FCN (2015)   Introduction : The first end-to-end pixel-wise prediction model based only on convolutional layers.\n  Method:\n Feature Extraction : using convolution layers like conventional Image Classification Tasks (layer 1,2,3,4,5) Convolutionalizing : Downsampling using 1x1 conv rather than FC layer(layer 6,7,8) Pixel Wise Classification : Last conv1x1 layer performs pixel wise classification for 21 classes. Upsampling : using deconvolution layer, also called transposed convolution Fusing Output : x32 upsample from pool5 (FCN-32S) + x16 upsample from pool4 (FCN16S) + x8 upsample from pool3 (FCN8S)     Figure1. Overview of FCN Architecture  Figure2. Overview of upsampling process \n2. Mask R-CNN (2017)   Introduction : to detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance.\n  Method : just add a third branch that outputs the object mask.\n  Feature Extraction is same as Faster RCNN\n  RPN is same as Faster RCNN\n  The 3rd tails outputs (class + box offset + a binary mask) for each ROI in parallel.\n 3.1 : Class labels are collapsed into a short output vectors by FC layers, same as Faster RCNN 3.2 : Box offset is collapsed into a short output vectors by FV layers, same as Faster faster_RCNN 3.3 : $m \\times m $ masks are predicted for each ROI using an FCN    ROI Align : If we use ROI pool at the above process, there would be small difference between the real ROI and extracted feature map. It does not matter in classification task, but does in segmentation. To address this problem, authors proposed ROI Align.\n    \n","id":56,"section":"Research","summary":"0. Introduction Image Segmentation : The process of assigning a label to every pixel in the image. Semantic Segmentation : treats multiple objects of the same class as a single entity. Instance Segmentation : treats multiple objects of the same class as distinct individual objects. 1. FCN (2015) Introduction : The first end-to-end pixel-wise prediction model based only on convolutional layers. Method: Feature Extraction : using convolution layers like conventional","tags":null,"title":"MLCV #3 | Semantic Segmentation","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv03-image-segmentation/","year":"2017"},{"content":"0. Introduction  Object Detection : a task of finding the different objects in an image and classifying them Salient Object Detection : a task based on a visual attention mechanism, in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or RGB images. AP or mAP is generally used as the primary metrics metric.click here for details Non Maximum Suppression (NMS) : 하나의 사물에 대하여 여러개의 bbox를 그려낸 경우, 가장 confidence가 높은 bbox만 놔두고, 이와 IoU가 높은 다른 bbox들을 제거함.  1. R-CNN (2013)   Introduction : An early application of CNNs to Object Detection tasks\n  Method\n  Region Proposals : Generate a set of proposals ($n=2000$) for bounding boxes using selective search algorithm.\n  Resize Regions : resize ROI patches to 224x224 for pretrained AlexNet\n  Classification : Run the images in b-boxes through a pre-trained AlexNet and SVM to see what object the image in the box is.\n  b-box regressor : Run the b-box through a linear regression model to output tighter coordinates\n     cf. Selective search looks at the images through windows of different sizes and for each window, tries to group together adjacent pixels by texture, color, or intensity to identify objects\n 2. Fast R-CNN (2015)   Introduction : RCNN was quite slow because of 2000 (number of Region patches) forward passes per image (for all proposed regions). And also it need to train three different models separately (CNN, SVM, regression). Fast-RCNN tried to solve these problems.\n  Method\n  pretrained CNN : get top-conv feature map using pretrained CNN\n  Region Proposal : just get ROI coordinates from input image using selective search, and does not make image patches.\n  ROI pooling : conv-features for each proposed ROI are obtained by selecting a corresponding region from the feature map of input image instead of running CNN for every ROI patches. Then, these conv-features are pooled adaptively.\n  Classification \u0026amp; B-Box regressor\n    쉽게 말하면, 일단 input image를 CNN에 넣고 feature map을 얻은 다음에 이 feature map에서 proposed b-box 와 matching 되는 부분만 따로 떼어내면 CNN 을 2000번 돌리지 않고도 2000개 proposal의 top-conv feature를 얻어낼 수 있음. -- 3. Faster R-CNN (2016)   Introduction : There was still one remaining bottleneck in the Fast R-CNN : the region proposer based on selective search.\n  Method : adds a Fully Convolutional Network which is called Region Proposal Network between the top-conv feature map and ROI pooling. The RPN slides a window over the top-conv features. At each window location, the network ouputs a score and a bbox per anchor.\n  Pretrained CNN : Run the image through a CNN to get a (top-conv) feature-map. (returns 14x14x512)\n  Region Proposal Network: slide a small conv-net over the extracted feature-map which maps the input window to lower-dimensional feature (256-dim). Then this lower-dim feature is fed into two sibling FC layers, one for box-classification and the other for box-regression.\n  2.1 Classifier : returns 14x14x9x2 (9 for anchor and 2 for object/background)\n  2.2 Box Regressor : returns 14x14x9x4 (9 for anchor and 4 for dx, dy, w, h)\n    ROI Pooling \u0026amp; Classification : same as faster RCNN\n     Figure 3. Architecture of Region Proposal Network (RPN)  4. Mask R-CNN (2017)   Introduction : to detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance.\n  Method : just add a third branch that outputs the object mask.\n  Feature Extraction is same as Faster RCNN\n  RPN is same as Faster RCNN\n  The 3rd tails outputs (class + box offset + a binary mask) for each ROI in parallel.\n 3.1 : Class labels are collapsed into a short output vectors by FC layers, same as Faster RCNN 3.2 : Box offset is collapsed into a short output vectors by FV layers, same as Faster faster_RCNN 3.3 : $m \\times m $ masks are predicted for each ROI using an FCN    ROI Align : If we use ROI pool at the above process, there would be small difference between the real ROI and extracted feature map. It does not matter in classification task, but does in segmentation. To address this problem, authors proposed ROI Align.\n    \n5. SpineNet (2020)   Introduction : In past few years, most networks follow the design that encodes input image into intermediate features with monotonically decreased resolutions. Most improvements of network architecture design are in adding network depth and connections within features resolution group.\n  Authors demonstrate that SpineNet can also be used as backbone model in Mask-RCNN Detector and improve both box detection and instance segmentation\n  ","id":57,"section":"Research","summary":"0. Introduction Object Detection : a task of finding the different objects in an image and classifying them Salient Object Detection : a task based on a visual attention mechanism, in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or RGB images. AP or mAP is generally used as the primary metrics metric.click here for details Non Maximum Suppression (NMS) :","tags":null,"title":"MLCV #2 | Object Detection","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv02-object-detection/","year":"2017"},{"content":"0. Introduction   Confusion Matrix 는 TP,TN,FP,FN의 네가지 요소로 구성되며, 앞의 True/False는 모델이 정답을 맞추었는지를 나타내고 뒤의 Positive/Negative는 모델의 예측값을 나타낸다.\n  Positive, Negative는 모델러의 주관을 제외하고 모델의 관점에서의 예측값으로 해석해야한다. 예로, 암이 있다라는 예측값은 부정적인 결과지만, 모델의 입장에서는 단지 0 (negative) 보다 1(positive)에 가까울 뿐이다.\n  영문과 국문의 어순차이에 따라 뒤에서부터 읽는것이 직관적이다. 어순을 바꾸어 암의 여부를 판단하는 모델을 예로 아래와 같이 해석할 수 있다.\n TP (True Positive) : 암이 있다고 예측했고, 정답과 같다. (True라고 했는데, True인 경우) TN (True Negative) : 암이 없다고 예측했고, 정답과 같다. (False라고 했는데, False인 경우) FP (False Positive) : 암이 있다고 예측했고, 정답과 다르다. (True라고 했는데, False인 경우) FN (False Negative) : 암이 없다고 예측했고, 정답과 다르다. (False라고 했는데, True인 경우)    Multiclass의 경우도 크게 다르지 않다. 각각의 class에 대하여 그대로 Positive, Negative를 찾을 수 있다. 예로 Apple Class에 대하여,\n TP (True Positive) : 7 TN (True Negative) : 2+3+2+1 FP (False Positive) : 8+9 FN (False Negative) : 1+3    \n1. True Positive Rate (TPR, Sensitivity, Recall)   실제 암이 있는 환자들 중에 암이 있다고 예측한 비율\n  실제 Positive 중에서 Positive라 예측한 비율\n  $$\\frac{TP}{P} = \\frac{TP}{TP+FN}$$\n\n2. True Negative Rate (TNR, Specificity)  실제 정상인들 중 정상인이라고 예측한 비율  $$\\frac{TN}{N} = \\frac{TN}{TN+FP}$$\n\n3. False Positive Rate (FPR, Fall-out)  실제 정상인 중에 암이 있다고 예측한 비율  $$ FPR = \\frac{FP}{N} = 1-TNR = 1-Specificity $$\n\n3. Positive Predictive Value (PPV, Precision)  암이 있다고 예측한 환자들 중에 실제 암이 있는 비율 Sensitivity 가 정답의 관점이라면 Precision은 모델 관점 Positive 라고 예측한 것 중에서 실제 Postive의 비율  $$ \\frac{TP}{TP+FP}$$\n\n5. F1 Score  Precision 과 Recall 의 조화평균, 평균 계산에 사용된 값이 불균형할수록 페널티가 가해져서 작은 값에 가깝도록 평균이 계산 참고 : https://gaussian37.github.io/ml-concept-ml-evaluation/ Micro (Averaged) F1 : is calculated by considering the total TP, total FP and total FN of the model. It does not consider each class individually, It calculates the metrics globally. Macro (Averaged) F1 : calculates metrics for each class individually and then takes unweighted mean of the measures. Weighted F1: it takes a weighted mean of the measures. The weights for each class are the total number of samples of that class.  $$ 2 \\times \\frac{Precision \\times Recall}{Precision + Recall } $$\n\n5. ROC (Receiver Operating Characteristic) Curve  Threshold 를 바꿔가며 Recall (TPR) 과 Fall-out (FPR, FP/(FP+TN))의 변화를 시각화한것 x축, FPR은 실제 정상인 중에 암이 있다고 예측한 비율 (FP/N) y축, TPR은 실제 암인 환자중에 암이 있다고 예측한 비율 (TP/P) 즉, test data에 대하여 threshold를 0~1로 바꿔가며, (FPR,TPR) 의 좌표를 구하고 이를 그래프위에 나타내면 ROC커브.  \n6. AUC (Area under Curve)  ROC Curve 는 그래프이기에 명확한 수치로 비교하기 어렵고 때문에 그래프 아래의 면적값을 계산  \n7. Precision-Recall Curve   precision 과 recall은 상호 보완적인 metric으로 두개를 동시에 보아야함.\n  예로, 영상에 10명의 사람이 있는데 이중 모델이 5개 물체를 검출해냈고 그중 4개가 사람이 맞으면 Precision은 0.8, Recall은 0.4.\n  그러나 일반적으로 이 둘은 Trade-Off의 관계에 있음. 가령, 이것저것 다 검출하고 보는 검출기의 경우 recall은 높으나 precision은 낮을것이고, 굉장히 strict 하게 검출하는 검출기의 경우 precision은 높으나 Recall은 낮을수밖에.\n  그래서 ROC 처럼 Precision-Recall도 Curve를 그려 함께 비교하는것이 일반적임\n  \n8. AP or mAP  Computer Vision 분야에서 Object Detection 알고리즘의 성능 평가에 주로 사용되는 metric으로 AUC의 Recall-Precision 버전이라고 생각하면 될 듯  $$ AP = \\int_0^1 p(r)dr $$\n Interpolated AP : Recall Precision 그래프의 넓이를 계산하는게 일반적이나, 계산의 편리성을 위하여 0부터 1까지 0.1간격으로 열한개 recall값의 maximum precision value를 average 하여 11로 나누어 사용하기도 함.  $$ AP = \\frac{1}{11} (P_r(0) + P_r(0.1) + P_r(0.2) + \u0026hellip; + P_r(1.0)) $$\n  만일 검출하고자하는 물체 클래스가 여러개인 경우 각 클래스당 AP 를 구한 다음 이 AP들의 평균을 계산한 mAP를 사용하기도 함.\n  Area를 계산할때 True/False를 판단하는 기준으로 IoU (교집합/합집합)을 주로 사용하는데 AP_50, AP_75 와 같이 뒤에 붙은 숫자는 일반적으로 IoU의 threshold를 의미함. 가령 AP_50의 경우 IoU가 0.5 d이상인 경우만 검출에 성공했다고 하는 경우.\n  Recall은 0~1 사이의 값을 갖으므로, Recall Precision 그래프를 $\\frac{1}{n}$ 크기의 작은 사각형으로 나누어 넓이를 계산하면 아래와 같고, 이는 각 recall값 n에 대한 precision 값의 평균과 같다.\n  $$ Area = \\sum (\\frac{1}{n} \\times P(n)), \\quad AP = \\frac{1}{n} {\\sum P(n)}$$\n\n9. nDCG ","id":58,"section":"Research","summary":"0. Introduction Confusion Matrix 는 TP,TN,FP,FN의 네가지 요소로 구성되며, 앞의 True/False는 모델이 정답을 맞추었는지를 나타내고 뒤의 Positive/Ne","tags":null,"title":"ML Basic #4 | Evaluation Metrics","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml04-evalutaion-metrics/","year":"2017"},{"content":"0. Introduction  Optimizer : updates the weight parameters to minimize the loss function.  \n1. Gradient Descent  Introduction : calculate the gradient $\\frac{\\partial c}{\\partial w} $ of cost function for weight w, and update the weights using calcuated gradient  $$ \\theta_{new} = \\theta - \\eta \\bigtriangledown J (\\theta;x,y)$$\n  Batch Gradient Descent : use the entire dataset to compute gradient of the cost function for each iteration\n  Stochastic Gradient Descent : use a single datapoint or example to calculate the gradient and update the weights with every iteration\n  Mini Batch Gradient Descent : instead of single training example, mini-batch of sample is used.\n  \n2. Momentum   Introduction : For updating the weights it takes the gradient of the current step as well as the gradient of the previous time steps.\n  $\\eta$ is learning rate, $\\gamma$ is coefficient for momentum\n  $$ v_t = \\gamma v_{t-1} + \\eta \\bigtriangledown J(\\theta;x,y) \\ \\theta = \\theta - v_t$$\n\n3. AdaGrad   Introduction : We need to tune the lr in momentum. Adagrad is an adaptive learning rate method that perform larger updates for infrequent parameters and smaller updates for frequent parameters.\n  지금까지 많이 변화했던 param들은 optimum에 가까울것이라 가정하고 step size를 작게, 아닌 param들은 step size를 크게하여 업데이트함.\n  학습해야할 param이 $k$ 개 일때 $G_t$ 는 $k$ 차원 벡터로 time step $t$ 까지 각 param가 이동한 gradient의 제곱의 합을 저장하고 있다. 따라서 실제 $\\theta$를 업데이트 할 때는 $G_t$에 반비례하도록 learning rate $\\eta$ 를 조정함.\n  $$ G_t = G_{t-1} + (\\bigtriangledown_\\theta J(\\theta))^2 \\ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot \\bigtriangledown_\\theta J(\\theta) $$\n\n4. RMSProp   Adagrad의 경우 학습을 계속 진행할 경우 lr이 너무 작아져서 학습이 되지않음\n  이 해결하기 위한 방법으로 RMSProp은 매 스텝마다 gradient의 제곱을 더했던 $G_t$를 합이 아니라 지수평균으로 바꿈\n  이를 통해 $G_t$ 가 무한정 커지지는 않으면서 각 params 사이의 lr 차이를 줄 수 있음.\n  $$ G = \\gamma G + (1-\\gamma)\\bigtriangledown_\\theta J(\\theta_t))^2 \\ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot \\bigtriangledown_\\theta J(\\theta)$$\n\n5. Adam  Adam은 RMSProp과 Momentum 을 합친 방식이라고 생각 할 수 있다. Momentum과 같이 지금까지 계산해온 gradient의 지수평균을 저장하며 ($m_t$), RMSProp과 유사하게 gradient 의 제곱값의 지수평균을 저장한다.  $$ m_t = \\beta_1 m_{t-1} + (1- \\beta_1)\\bigtriangledown_\\theta J(\\theta_t) \\ v_t = \\beta_2 v_{t-1} + (1- \\beta_2)(\\bigtriangledown_\\theta J(\\theta_t))^2$$\n  단, $m,v$ 가 0으로 초기화 되어있기에 학습초기에는 $m_t, v_t$가 0에 가깝게 bias 되어있을 것이라고 판단하여 이를 unbiased 보정해주는 작업을 거친다.[details in paper]\n  $m_t, v_t$ 를 $\\Sigma$ 형태로 펼친 후 양변에 expectation을 씌워서 정리하면 unbiased expectation을 얻을 수 있다. 이 보정된 expectation을 가지고 업데이트한다.\n  $$\\hat{m_t} = \\frac{m_t}{1-\\beta^t_1} \\ \\hat{v_t} = \\frac{v_t}{1-\\beta^t_2} \\ \\theta = \\theta - \\frac{\\eta}{\\sqrt{\\hat{v_t} + \\epsilon}}\\hat{m_t}$$\n  보통 $\\beta_1$은 0.9 $\\beta_2$는 0.999 $\\epsilon$은 $10^{-8}$을 사용한다.\n  그렇다면 스스로 lr을 조정하는 adam을 사용하는데 왜 lr decay를 추가로 사용할까. 여기서 lr decay로 조절된 lr은 upper limit으로 작동한다.\n  The learning rates adapt themselves during train steps, it\u0026rsquo;s true, but if you want to be sure that every update step do not exceed lambda you can than lower lambda using exponential decay or whatever. It can help to reduce loss during the latest step of training, when the computed loss with the previously associated lambda parameter has stopped to decrease.\n  \n","id":59,"section":"Research","summary":"0. Introduction Optimizer : updates the weight parameters to minimize the loss function. 1. Gradient Descent Introduction : calculate the gradient $\\frac{\\partial c}{\\partial w} $ of cost function for weight w, and update the weights using calcuated gradient $$ \\theta_{new} = \\theta - \\eta \\bigtriangledown J (\\theta;x,y)$$ Batch Gradient Descent : use the entire dataset to compute gradient of the cost function for each iteration Stochastic Gradient Descent : use","tags":null,"title":"ML Basic #3 | Optimization","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml03-optimization/","year":"2016"},{"content":"0. Introduction   Loss Function : is a function that evaluate how well the algorithm models the target dataset\n  MSE : 확률론적 관점에서 MSE는 Gaussian Distrib 의 negative log-likelihood와 같다. 즉, MSE는 Gaussian Distrib. 의 MLE와 같다.\n  Cross Entropy : 확률론적 관점에서 CE는 Multinomial Distrib 의 negative log-likelihood와 같다. 즉, CE는 Multinomial Distrib. 의 MLE와 같다.\n  MSE : 확률론적 관점에서 MSE는 Gaussian Distrib 의 negative log-likelihood와 같다. 즉, MSE는 Gaussian Distrib. 의 MLE와 같다.\n   1. Mean Squared Error (MSE)   introduction : easily computable quantity that measures the average of the squares of the errors.\n$$ MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - t_i)^2 $$\n  L1 Loss (LAD : Least Absolute Deviation), L2 Loss (LSE : Least Square Error)\n$$ L_{L1} = \\sum_{i=1}^n | y_i - t_i |$$\n$$ L_{L1} = LSE = \\sum_{i=1}^n (y_i - t_i)^2$$\n  확률론적 관점 에서 MSE 는 Gaussian Distribution의 Negative log likelihood와 같다. (모델의 예측결과를 $y_i = \\theta^T x_i= \\mu$로 하는 정규분포라 가정)\n  $$ \\text{log likelihood} , logP(t|x;\\theta) = log \\prod_i P(t_i | x_i;\\theta) = \\sum_i log \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{- \\frac{(t_i - \\theta^T x_i)^2}{2\\sigma^2} } \\ = \\sum_i \\frac{1}{\\sqrt{2 \\pi \\sigma}} + \\sum_i (- \\frac{(t_i - \\theta^T x_i)^2}{2\\sigma^2} ) \\ = C_1 - C_2 \\sum_i (t_i - \\theta^T x_i)^2$$\n 2. Cross Entropy Loss   정보이론의 관점에서 : Cross Entropy 는 학습데이터의 정보량의 평균값인 Entropy와 모델의 예측값의 Entropy 사이의 KLD\n  Information content : surprisal of a random variable or signal is the amount of information\n  동전을 던져 앞면이 나오는 사건의 정보량 : $ -log_2 (0.5) = 1$\n  주사위를 던져 눈이 1이 나오는 사건의 정보량 : $ -log_2 (1/6) = 2.5849$\n  정보량 = 놀람의 정도 : 즉, 발생확률이 더 낮은 사건이 더 높은 정보량을 보인다.\n  그렇다면 단순히 이를 확률의 역수인 $\\frac{1}{p(x)}$가 아니라 log를 취하는 이유는 무엇일까, log를 취함으로서 놀람의 정보를 표현하는데 필요한 최소한의 자원을 나타낼 수 있게된다. 예로 1/8로 발생하는 어떤사건을 2진수로 표현한다면 밑이 2인 로그함수를 이용하여 $log2(8)$ 최소 3개의 비트가 필요함을 알 수 있다.\n$$ I(E) = -log P(E)$$\n Entropy (Shannon Entropy) : average(expectation) information content of discrete random variable X\n  즉 엔트로피는 이러한 정보량의 평균이며, 동시에 사건을 표현하기 위해 요구되는 리소스의 평균이라고도 할 수 있다.\n$$ Entropy = -\\sum_{i=1}^{N} p_i log{p_i} $$\n    Relative Entropy (Kullback-Leibler Divergence) : measure of how one probability distribution is different from second, reference probability distribution.\n$$ KL(p|q) = \\sum_i p_i log(\\frac{p_i}{q_i}) = \\sum_i (p_i log p_i - p_i log q_i) = (-\\sum_{i=1}^{N} p_i log{q_i}) - (-\\sum_{i=1}^{N} p_i log{p_i})$$\n  Cross-Entropy: To minimize the dissimilarity of distributions, we should find the $q$ that minimize the first term (second term is independent to the distrib. $q$). From this we can get cross entropy error.\n$$ CE = -\\sum_{i=1}^{N} p_i log{q_i} $$\n    확률론적 관점에서 : Multinomial Distrib 의 Negative Log likelihood\n$$ \\text{log likelihood} , logP(t|x;\\theta) = log \\prod_i \\prod_k \\pi_k^{t_k} = \\sum_i \\sum_k t_k log \\pi_k$$\n       3. Binary Cross Entropy  위의 Cross Entropy 식에서 단순하게 $N=2$, Binary classification이므로 $p_2 = 1-p_1$  $$BCE = -(p_1 logq_1 + p_2 logq_2) = - (p_1 log q_1 + (1-p_1) log (1-q_1))$$\n 확률론적 관점에서 : binomial distrib 의 log likelihood 는 BCE와 같다  $$ \\text{log likelihood} , logP(y|x;\\theta) = log \\prod_i p_i^{y_i} (1-p_i)^{1-y_i} \\ = \\sum_i log( p_i^{y_i} (1-p_i)^{1-y_i}) \\ = \\sum_i ( y_i log p_i + (1-y_i)log(1-p_i) ) $$\n 4. Dice-Coefficient Loss $$\\text{Dice Coefficient} = \\frac{2*|X \\cap Y|}{|X| + |Y|} $$\n  Where $ |X| $ is the cardinality (i.e. the number of elements in each set) of the set $X$.\n  It has been used as a metric in the field of image segmentation. And sometimes it can be used as a loss function.\n  In the field of image segmentation, Dice Loss can be used as a loss function when the foreground is smaller than the background, causing imbalance problem\ndef dice_loss(pred, target): smooth = 1. # have to use contiguous since they may from a torch.view op iflat, tflat = pred.contiguous().view(-1).cuda(), target.contiguous().view(-1).cuda() intersection = (iflat * tflat).sum() A_sum, B_sum = torch.sum(iflat * iflat), torch.sum(tflat * tflat) return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth)     5. Noise Contrastive Learning  Intuitive explanation :  If we want to find the next words (you) appropriate for a given context ([Nice, to, meet]) . We can train the network with the softmax CE loss function, which it will return the probabilities for all candidate words. This means that the output \u0026ldquo;scores\u0026rdquo; for each class have to be normalized, converted into actual probabilities for each class. =\u0026gt; computationally expensive Let\u0026rsquo;s simplify this multinomial classification problem to binary classification (logistic regression) by transformation inputs with \u0026ldquo;set of words and answer\u0026rdquo; to positive and negative pairs. So the model only needs to predict whether a pair is positive or negative rather than find which word would be the answer    ","id":60,"section":"Research","summary":"0. Introduction Loss Function : is a function that evaluate how well the algorithm models the target dataset MSE : 확률론적 관점에서 MSE는 Gaussian Distrib 의 negative log-likelihood와 같다. 즉, MSE는 Gaussian Distrib. 의 MLE와 같다.","tags":null,"title":"ML Basic #2 | Loss Functions","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml02-loss-funtions/","year":"2016"},{"content":"0. Introduction   Activation Function : is a function to induce non-linearity into the output of the neuron for the given input.\n  Non-Linearity: 아래의 선형성을 만족하지 않는 함수들에 대하여 비선형성을 보인다고함. $ f(x+y) = f(x)+f(y), \\quad f(\\alpha x) = \\alpha f(x)$\n  왜 비선형함수를 사용하는가 : $y = wx+b$ 형태로 선형 output을 낸다면 layer의 깊이가 깊어진다한들, 결국 $w_2x +b_2$ 의 또다른 선형 함수를 모델링할 뿐이기 때문. (becaues composites of linear functions are again linear)\n  bias 의 역할 : activation 함수의 x축 평행이동을 위하여. 가령 어떤 feature x가 2보다 작을때 Relu output을 0으로 만들고 싶은데, weight만으로는 이게 불가함. (It\u0026rsquo;s similar to the constant b of a linear function y=ax+b. It allow you to move the line up and down to fit the prediction with the data better. Without b, the line always goes through the origin (0,0) and you may get a poorer fit.)\n  \n1. Sigmoid / Logistic   Advantages :\n  Smooth gradient : preventing \u0026ldquo;jumps\u0026rdquo; in output values\n  Clear predictions : for $x$ above 2 or below -2, output tends to be at the edge of the curve, very close to 0 or 1.\n    Disadvantage:\n  Vanishing gradient : for very high or low values of X, there\u0026rsquo;s no change to the prediction, causing a vanishing gradient problem. This can result in the network being too slow to reach an accurate prediction.\n  Outputs not zero centered\n  Computationally expensive\n  $$ \\sigma (x) = (1+e^{-x})^{-1} \\ \\sigma\u0026rsquo;(x) = \\sigma(x) (1-\\sigma(x)) $$\n  \n2. Tan H   Properties :\n  Zero Centered : make it easier to model inputs that have strongly negative,neutral, and strongly positive values\n  Same with Sigmoid funtion\n  $$ tanh(x) = \\frac{sinh(x)}{cosh(x)}=\\frac{e^x-e^{-x}}{e^x+e^{-x}} \\ tanh\u0026rsquo;(x) = 1 - tanh^2(x) $$\n  \n3. ReLU (Rectified Linear Unit)   Advantages :\n  Computationally efficient : allows the network to converge very quickly\n  Non-Linearity: although it looks like a linear function, ReLU has a derivate fuction and allows for backpropagation.\n    Disadvantage:\n Dying ReLU : when inputs approach zero, or are negative, the gradient of the function becomes zero, the network cannot perform backpropagation and cannot learn.  $$ ReLU = \\begin{cases} :x :: (x\u0026gt;0) \\\\ 0 :: (x\u0026lt;0) \\end{cases} $$\n  \n4. Leaky ReLU   Properties :\n alleviate Dying ReLU : To resolve dying ReLU problem, leaky ReLU has non zero outputs with negative input, thus the gradient does not become zero.  $$ \\text{Leaky ReLU} = \\begin{cases} :x :: (x\u0026gt;0) \\\\ 0.01x :: (x\u0026lt;0) \\end{cases} $$\n  \n5. ELU (Exponential Linear Units)   Properties :\n  alleviate Dying ReLU : To resolve dying ReLU problem, ELU has non zero outputs with negative input, thus the gradient does not become zero.\n  more computation : ELU has exponential function that ReLU doesn\u0026rsquo;t have, thus it costs bit more than ReLU.\n  $$ \\text{Leaky ReLU} = \\begin{cases} :x :: (x\u0026gt;0) \\\\ \\alpha(e^x -1) :: (x\u0026lt;0) \\end{cases} $$\n  \n6. Swish (Scaled Exponential Linear Units)   Properties :\n  Unboundeness : Unlike sigmoid and Tanh functions, Swish is unbounded above which makes it useful near the gradients with values near to zero. This feature avoids saturation as training becomes slow near zero gradient value.\n  Smoothness of the curve : plays an important role in generalization and optimization. Unlike ReLU, Swish is a smooth function which makes it less sensitive to initializeing weights and learning rate.\n  Bounded Below : Like most of the activation functions out there, Swish is also bounded below which helps in strong regularization effects. Like ReLU and softplus, Swish produces negative outputs for small negative inputs due to its non-monotonicity. The non-monotonicity of Swish increases expressivity and importves gradient flow, Which is important considering that many preactivation fall into this range.\n  $$ Swish = x * sigmoid(x) = x * (1+e^{-x})^{-1} $$\n  \n6. SELU (Scaled Exponential Linear Units)   Properties :\n  Similar to ReLUs, SELUs enable deep neural networks since there is no problem with vanishing gradients.\n  In contrast to ReLUs, SELUs cannot die\n  SELUs on their own learn faster and better than other activation functions, even if they are combined with batch normalization\n  $$ selu = \\lambda \\begin{cases}x :: (x\u0026gt;0) \\ \\alpha(e^x-1) :: (x\u0026lt;0) \\end{cases} $$\n  \n6. Soft-argmax   To get the position where the intensity is maximal in a vector, we usually use an argmax func. But the problem is, this func has no derivative.\n$$ Softmax(x) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\n  Using Sofmax, we get normalized probabilities for each $x_i$, and the expectation of this is the sum of the indices multiplied by their respective probabilities\n$$ \\mathbb{E[x]} = \\sum_i \\frac{e^{x_i}}{\\sum_j e^{x_j}} i $$\n  However, this mean value is weak if there\u0026rsquo;s multiple modes. To raise the max and lower the others, we can multiply $x$ by an arbitrarily big $\\beta$.\n$$ \\mathbb{E[x]} = \\sum_i \\frac{e^{\\beta x_i}}{\\sum_j e^{\\beta x_j}} i $$\n  def soft_arg_max_khw(A, dim=1): A_softmax = torch.softmax(A, dim=dim).cuda() indices = torch.arange(start=0, end=A.size()[dim]).float().cuda() return torch.matmul(A_softmax, indices).cuda()  ","id":61,"section":"Research","summary":"0. Introduction Activation Function : is a function to induce non-linearity into the output of the neuron for the given input. Non-Linearity: 아래의 선형성을 만족하지 않는 함수들에 대하여 비선형성을 보인다고함. $ f(x+y) = f(x)+f(y), \\quad f(\\alpha x) = \\alpha f(x)$ 왜 비선형함수를","tags":null,"title":"ML Basic #1 | Activation Functions","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml01-activation-funtions/","year":"2016"},{"content":"0. Introduction   Generative Model: likelihood를 활용하거나 Posterior 를 활용하여 Data Distribution을 Modeling 하여 Decision Boundary를 찾고자 하는 방법론, 오박사스타일. GMM\n  Discriminative Model: Regression 과 같이 데이터를 활용하여 class 를 분류할 수 있는 decision boundary를 찾는데 집중하는 방법론.\n  Supervised Learning : Labeled Training Data 로 부터 함수를 모델링하기 위한 기계학습의 한 방법.\n  Semi-supervised Learning : Labeled 와 Unlabeled Data를 모두 사용하여 모델을 학습하는 방법으로, Labeled Data 가 적고 Unlabeled Data가 많은 경우 주로 사용함.\n  Unsupervised Learning : Unlabeled Data를 바탕으로 모델을 학습하는 방법.\n  \n","id":62,"section":"Research","summary":"0. Introduction Generative Model: likelihood를 활용하거나 Posterior 를 활용하여 Data Distribution을 Modeling 하여 Decision Boundary를 찾고자 하는 방법론, 오박사스타일. GMM","tags":null,"title":"ML Basic #0 | Terminology","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml00-terminologies/","year":"2016"},{"content":"Introduction  Image Classification : The task of classifying an image according to its visual content.   (2022) At first, the goal of works in this summary was to solve the visual classification problems, but now I want to focus on the way to encode visual contents into vectors (embedding, encoding)\n   AlexNet (2012)  Introduction : CNNs have been prohibitively expensive to apply in large scale to high resolution images. Method : Training on Multiple GPUs  def AlexNet(x): out = MP(relu(conv11x11(x))) out = MP(relu(conv5x5(out))) out = relu(conv3x3(out)) out = relu(conv3x3(out)) out = MP(relu(conv3x3(out))) out = FC(relu(FC(relu(FC(out))))) return out    VGG Net (2014)  Introduction : come up with significantly more accurate ConvNet Method : deeper ConvNet  def VGG16(x): out = MP(conv3x3(conv3x3(x))) out = MP(conv3x3(conv3x3(out))) for i in range(3): out = MP(conv3x3(conv3x3(conv3x3(out)))) out = softmax(FC(FC(FC(out)))) return out    GoogleNet (2015)  Introduction : efficient deeper networks (with fewer params than AlexNet) Method : inception module(NIN, Bottleneck)  def inception_block(x): branch_1x1 = conv1x1(x) branch_3x3 = conv3x3(conv1x1(x)) branch_5x5 = conv5x5(conv1x1(x)) branch_pool = conv1x1(MP3x3(x,same)) out = concat([branch_1x1,branch_3x3,branch_5x5,branch_pool]) return out    ResNet (Microsoft, 2015)  Introduction : to solve the degradation problem caused by deeper layer. Method : Residual Block with shortcut(skip) connection defined as :  $$ \\mathbf{x}_{l+1} = \\mathbf{x}_l + F(\\mathbf{x}_l,{W_i}) $$\ndef residual_block(x): out = relu(bn1(conv3x3(x))) out = relu(bn2(conv3x3(out)) + x) return out    DenseNet (2016)  Introduction : information about the input or gradient can vanish and wash out as CNNs become deep Method : Dense Connectivity (not sum, just concat) Result : 77.85% of top-1 accuracy in ImageNet  $$ x_l = H_l([x_0, x_1, \u0026hellip; , x_{l-1} ]) $$\n$[x_0, x_1, \u0026hellip; , x_{l-1}]$ means concatenation of the features-maps produced in previous layers.\ndef dense_block(x): out = conv1x1(relu(bn1(x))) # Bottleneck for comput. efficiency out = conv3x3(relu(bn2(out))) out = concat([x, out]) return out    ResNeXt (2016)  Introduction : present a improved architecture that adopts ResNets strategy of repeating layers. Method : split-transform-merge strategy (cardinality) Result : 80.9% of top-1 accuracy in ImageNet with 83.6M params  $$ \\mathbf{x}_{l+1} = \\mathbf{x}l + \\sum{i=1}^{cardin} F_i(\\mathbf{x}_l) $$\ndef residual_block(x): out = relu(bn1(conv3x3(x, groups=cardinality))) out = relu(bn2(conv3x3(out, groups=cardinality)) + x) return out    ShuffleNet(2017)   Introduction : extremely computation-efficient CNN architecture named ShuffleNet, designed specially for mobile devices with very limited computing power.\n  Methods : utilizes two new operations, pointwise group convolution and channel shuffle\n  divide the channels in each group into several subgroups\n  feed each group in the next layer with difference subgroup\n      FixResNeXt (2019)  Introduction : Existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time. Method : Simple strategy to optimize the classifier performance, that employs different train and test resolution : in face, a lower train resolution improves the classification at test time. Result : 86.4% of top-1 accuracy in ImageNet with 83.6M params  (L) conventional augmentation method (R) proposed augmentation method   ViT : An Image is Worth 16x16 Words (2020, GoogRes)   Introduction :\n Transformer architecture has become the de-facto standard for natural language processing tasks In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks.    Methods : applying a standard Transformer directly to images\n  Patch Embedding $x_i$ : extracts N non-overlapping image patches, performs a linear projection ($E$, is equivalent to a 2D conv) and then rasterises them into 1D token.\n  learnable embedding $z_{cls}$ : an optional learned clasification token (Similar to BERT\u0026rsquo;s [cls]) is prepended to the sequene of embedded patches\n  learnable position embedding $p$ : added to the tokens to retain positional information,\n=\u0026gt; When you have no idea about how to hand-craft positional encoding for your data\n=\u0026gt; Let the transformer figure out for itself what it needs as positional embeddings\n=\u0026gt; simply train the vectors in table of figure at \u0026ldquo;NLP3 \u0026gt; Transformer \u0026gt; Binarized Indexing\u0026rdquo;\n  $$ \\mathbf{z} = [z_{cls}, E_{x_1}, E_{x_1}, \u0026hellip;, E_{x_1}] + \\mathbf{p}\n$$\n  Result:\n When trained on mid-sized datasets such as ImageNet(1.3M) without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size However, the picture changes if the models are trained on larger datasets (JFT-300M, Figure3)     VirTex : Learning Visual Representations from Textual Annotations  introduction : revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining.  (1) Semantic density : Captions provide a semantically denser learning signal than unsupervised contrastive methods and supervised classification. (2) simplified data collection : natural language descriptions do not require an explicit ontology and can easily be written by non-expert workers,   VirTex : a pretraining approach using semantically dense captions to learn visual representations  (1) jointly train a ConvNet and Transformer from scratch to generate natural language captions for images  Visual Backbone : a convolutional network which computes visual features of image Textual Head : receives features from thevisual backbone and predicts captions for images   (2) transfer the learned features to downstream visual recognition tasks   Result  show that natural language can provide supervision for learning transferable visual representations with better data-efficiency than other approaches. VirTex matches or exceeds the performance of existing methods for supervised or unsupervised pre-training on ImageNet, despite using up to 10×fewer images    ","id":63,"section":"Research","summary":"Introduction Image Classification : The task of classifying an image according to its visual content. (2022) At first, the goal of works in this summary was to solve the visual classification problems, but now I want to focus on the way to encode visual contents into vectors (embedding, encoding) AlexNet (2012) Introduction : CNNs have been prohibitively expensive to apply in large scale to high resolution images. Method : Training","tags":null,"title":"MLCV #1 | Image Classification","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv01-image-classification/","year":"2016"},{"content":"1. Windows와 Ubuntu에 사용할 파티션 나누기  컴퓨터관리 - 저장 - 디스크관리  \n2. 멀티부팅을 위한 윈도우의 빠른시작기능 해제  제어판 - 전원옵션 - 전원단추 작동설정 - 현재 사용할 수 없는 설정 변경 - 빠른시작켜기 해제  \n3. Ubuntu 부팅용 USB 만들기  Ubuntu OS 다운로드 및 부팅 usb Setup  \n4. Ubuntu 설치   Install Ubuntu - 한국어 - 무선네트워크 - 업데이트 체크\n  기타 - 스왑영역(4GB, 논리, 시작지점) - 바이오스 부팅영역(1MB, 논리, 시작지점) - EXT4(나머지, 논리, 시작지점, \u0026lsquo;/\u0026rsquo;) - 부트로더는 SSD 제일상단에 위치한 지점으로 설정\n  서울 - 한국어, 한국어(101/104키보드호환)\n  \n5. GPU Driver 설치  NVIDIA로 가서 사용중인 GPU와 OS 버전에 맞는 Driver 다운로드  $ cd Downloads/ $ chmod +x NVIDIA-Linux-x86_64-410.57.run # to get permission to execute the run file $ sudo ./ NVIDIA-Linux-x86_64-410.57.run --no-x-check   Driver 설치 GUI 에 따라 진행하되 automatically updte X configuration file 에서는 No를 선택  \n6. CUDA, cuDNN 설치   참고 링크\n  Ubuntu 16.04, RTX2080ti 의 경우 CUDA 10.0 으로 설치할 것\n  설치하려는 tensorflow-gpu 버전에 맞는 CUDA 버전과 cuDNN 버전을 먼저 확인할 것\n  \n7. Anaconda 설치  Anaconda 설치 후 bashrc 에 PATH 등록하여 기본 python으로 anaconda 사용  \n8. Tensorflow 설치 $ pip install tensorflow-gpu $ pip install opencv-python # if you need main modules   Unofficial Windows Binaries for python Extension Packages Open CV   9. pyTorch  If current cuda version is not matching with torch build cuda version, torch.cuda.is_available would be False  $ pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html  ","id":64,"section":"Engineering","summary":"1. Windows와 Ubuntu에 사용할 파티션 나누기 컴퓨터관리 - 저장 - 디스크관리 2. 멀티부팅을 위한 윈도우의 빠른시작기능 해제 제어판 - 전원옵션 - 전원","tags":null,"title":"How to set up Ubuntu 16.04 for ML ","uri":"https://koreanbear89.github.io/engineering/2.-linux/how-to-set-up-ubuntu-16.04-for-ml/","year":"2016"},{"content":"\n1. Check the number of files in directory. # the number of directories in working directory $ ls -l | grep ^d | wc -l # the number of files in working directory $ ls -l | grep ^- | wc -l # the number of files in working directory $ ls ./ | wc -l # print all the files in dir ls | gawk \u0026quot;BEGIN {\\\u0026quot;pwd\\\u0026quot; | getline cwd} {printf(\\\u0026quot;%s/%s\\n\\\u0026quot;, cwd, \\$0)}\u0026quot;  2. Check resource $ grep -c processor /proc/cpuinfo  3. Anaconda # show the list of envs $ conda info --envs # Create conda ENV using specific python version $ conda create --name myenv python=3.5 # Create conda ENV using the existing ENV $ conda create --name myclone --clone myenv # Remove activated #!/usr/bin/env $ conda remove --name myenv --all # Backup conda env $ conda env export \u0026gt; [filename].yaml # create conda env from yaml file $ conda env create -f [filename].yaml  4. SSH # start ssh connection to remote server $ ssh koreanbear@192.168.0.1 # start ssh connection to remote server using private key $ ssh -i ~/Desktop/key.pem koreanbear@192.168.0.1 # copy test.txt from local PC to remote server $ scp ./test.txt koreanbear@192.168.0.1:~/Desktop $ rsync options source destination  5. screen # Create new session with name $ screen -S name # Create new session $ Ctrl+a+c # Move to next session $ Ctrl+a+a # Move to previous session $ Ctrl+a+n # quit the current screen session $ Ctrl+d # Detach current screen session $ Ctrl+a+d # Detach screen from remote $ screen -d \u0026lt;SCREENID\u0026gt; # connect to specific screen session using ID $ screen -r \u0026lt;SCREENID\u0026gt; # connect to specific screen session using name $ screen -r name # show the list of screen sessions $ screen -ls # kill the screen session using SCREENID $ screen -X -S SCREENID quit  6. GPU Test # check CUDA Version $ nvcc -V  7. Jekyll $ chcp 65001 # change code page $ bundle exec jekyll serve # serve jekyll for local environment  8. Process kill # show the list of process, you can find Process ID using command below $ ps -ef | grep python # 실행중인 process 개수를 보려면 -ic $ ps -ef | grep -ic palantir # kill은 프로세스로 signal을 보냄 -15 종료, -9 강제종료 $ kill -15 PID $ kill -9 PID # pkill 은 프로세스 이름으로 kill, -f 는 모든 argument를 비교 $ pkill -f PROCESS_NAME $ sudo pkill -f palantir  10. Caps Lock to Hangul $ xmodmap ~/.Xmodmap  11. Hardware $ xset dpms force off # turn off monitor, with crontab!  12. crontab # edit crontab setting $ crontab -e # write in format of \u0026quot;***** command\u0026quot; # first * for minute (0~59) # second * for hour (0~23) # third * for day (1-31) # fourth * for month (1~12) # fifth * for day of the week (0~6)  13. chmod $ chmod -R [777] [file name or folder name]  14. HDFS $ hdfs dfs -ls $ hdfs dfs -put [LOCAL_FILE_NAME] [Dest] $ hdfs dfs -get [FILE_NAME or FOLDER_PATH] # 특정 폴더 혹은 파일 삭제 # -r : 특정 디렉토리 이하의 폴더 모두 제거 # -skipTrash : 즉시 완전 삭제 $ hdfs dfs -rm [-f] [-r|-R] [-skipTrash] URI [URI ...]  15. tar # tar 로 압축 $ tar -cvf [파일명.tar] [폴더명] # tar 압축해제 $ tar -xvf [파일명.tar] # tar.gz 압축 $ tar -zcvf [파일명.tar.gz] [폴더명] # tar.gz 압축해제 $ tar -zxvf name.tgz  # split tar # 1. 우선 tar로 압축한다 $ tar cvzf compressed.tgz target_dir # 2. 분할 압축하기 $ split -b 100M compressed.tgz \u0026quot;compressed.tgz.part\u0026quot; # 3. 분할된 파일 합치기 $ cat compressed.tgz.parta* \u0026gt; backup.tgz  ps # show all processes ps aux | grep PROCESS_NAME  ","id":65,"section":"Engineering","summary":"1. Check the number of files in directory. # the number of directories in working directory $ ls -l | grep ^d | wc -l # the number of files in working directory $ ls -l | grep ^- | wc -l # the number of files in working directory $ ls ./ | wc -l # print all the files in dir ls | gawk \u0026quot;BEGIN {\\\u0026quot;pwd\\\u0026quot; | getline","tags":null,"title":"Cheat Sheet | Linux Commands","uri":"https://koreanbear89.github.io/engineering/2.-linux/cheatsheet-linux-commands/","year":"2016"},{"content":"1. Pickle import pickle # load or save object using pickle try: with open(path_pkl, 'rb') as f: obj_pkl = pickle.load(f) except: obj_pkl = [] with open(path_pkl, 'wb') as f: pickle.dump(obj_pkl, f)  2. Requests import requests # GET url = 'http://localhost/test' params = {'arg1':'1', 'arg2':'2'} response = requests.get(url=url, params=params).json() # POST response = requests.post(url=url, data=json.dumps(params))  3. argparse import argparse if __name__ == \u0026quot;__main__\u0026quot;: parser = argparse.ArgumentParser() parser.add_argument(\u0026quot;--name\u0026quot;, type=str, required=True, help=\u0026quot;help\u0026quot;) args = parser.parse_args() print(args.name)  4. Datetime from datetime import datetime datetime.today().strftime(\u0026quot;%Y%m%d%H%M%S\u0026quot;) # YYYYmmddHHMMSS 형태의 시간 출력  5. Flask from flask import jsonify, make_response @application.route('/inference', methods=[\u0026quot;GET\u0026quot;]) def infer(): summary = {'class' : 'cat', 'score':'0.92'} # make response data res = make_response(jsonify(summary), 200) # make Response object res.headers.add(\u0026quot;Access-Control-Allow-Origin\u0026quot;, \u0026quot;*\u0026quot;) # CORS ERROR 대응 return res  6. Path # Root path # Current Working Directory import os print(os.getcwd())  7. Counter from collections import Counter Counter(['apple','red','apple','red','red','pear']) \u0026gt;\u0026gt;\u0026gt; Counter({'red': 3, 'apple': 2, 'pear': 1})  8. Limiting floats to N demicmal points # 1. round round(1.23456, 4) \u0026gt;\u0026gt; 1.2346 # 2. Python format \u0026quot;{:.2f} / {:.3f}\u0026quot;.format(1.2345, 1.2345) \u0026gt;\u0026gt; \u0026quot;1.23 / 1.235\u0026quot; # 3. Python f-string num1 = 1.23456 f\u0026quot;{num1:.2f}\u0026quot; \u0026gt;\u0026gt; \u0026quot;1.23\u0026quot;  ","id":66,"section":"Engineering","summary":"1. Pickle import pickle # load or save object using pickle try: with open(path_pkl, 'rb') as f: obj_pkl = pickle.load(f) except: obj_pkl = [] with open(path_pkl, 'wb') as f: pickle.dump(obj_pkl, f) 2. Requests import requests # GET url = 'http://localhost/test' params = {'arg1':'1', 'arg2':'2'} response = requests.get(url=url, params=params).json() # POST response = requests.post(url=url, data=json.dumps(params)) 3. argparse import argparse if __name__ == \u0026quot;__main__\u0026quot;: parser = argparse.ArgumentParser() parser.add_argument(\u0026quot;--name\u0026quot;, type=str, required=True, help=\u0026quot;help\u0026quot;) args","tags":null,"title":"CheatSheet | Python Snippet","uri":"https://koreanbear89.github.io/engineering/1.-tools/170102-cheatsheet-python-snippet/","year":"2016"},{"content":"1. Matrix $$ \\begin{pmatrix}1 \u0026amp; 2 \u0026amp; 3 \\4 \u0026amp; 5 \u0026amp; 6 \\7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\begin{bmatrix}a \u0026amp; b \u0026amp; c \\d \u0026amp; e \u0026amp; f \\g \u0026amp; h \u0026amp; i \\end{bmatrix} \\begin{pmatrix} a_{1,1} \u0026amp; a_{1,2} \u0026amp; \\cdots \u0026amp; a_{1,n} \\ a_{2,1} \u0026amp; a_{2,2} \u0026amp; \\cdots \u0026amp; a_{2,n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{m,1} \u0026amp; a_{m,2} \u0026amp; \\cdots \u0026amp; a_{m,n} \\end{pmatrix}$$\n\\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\begin{bmatrix} a \u0026amp; b \u0026amp; c \\\\ d \u0026amp; e \u0026amp; f \\\\ g \u0026amp; h \u0026amp; i \\end{bmatrix} \\begin{pmatrix} a_{1,1} \u0026amp; a_{1,2} \u0026amp; \\cdots \u0026amp; a_{1,n} \\\\ a_{2,1} \u0026amp; a_{2,2} \u0026amp; \\cdots \u0026amp; a_{2,n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m,1} \u0026amp; a_{m,2} \u0026amp; \\cdots \u0026amp; a_{m,n} \\end{pmatrix}  ","id":67,"section":"Engineering","summary":"1. Matrix $$ \\begin{pmatrix}1 \u0026amp; 2 \u0026amp; 3 \\4 \u0026amp; 5 \u0026amp; 6 \\7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\begin{bmatrix}a \u0026amp; b \u0026amp; c \\d \u0026amp; e \u0026amp; f \\g \u0026amp; h \u0026amp; i \\end{bmatrix} \\begin{pmatrix} a_{1,1} \u0026amp; a_{1,2} \u0026amp; \\cdots \u0026amp; a_{1,n} \\ a_{2,1} \u0026amp; a_{2,2} \u0026amp; \\cdots \u0026amp; a_{2,n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{m,1} \u0026amp; a_{m,2} \u0026amp; \\cdots \u0026amp; a_{m,n} \\end{pmatrix}$$\n\\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\begin{bmatrix} a \u0026amp; b \u0026amp; c \\\\ d \u0026amp; e \u0026amp; f \\\\ g \u0026amp; h \u0026amp; i \\end{bmatrix} \\begin{pmatrix} a_{1,1} \u0026amp; a_{1,2} \u0026amp; \\cdots \u0026amp; a_{1,n} \\\\ a_{2,1} \u0026amp; a_{2,2} \u0026amp; \\cdots \u0026amp; a_{2,n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m,1} \u0026amp; a_{m,2} \u0026amp; \\cdots \u0026amp; a_{m,n} \\end{pmatrix}  ","tags":null,"title":"CheatSheet | LaTeX Math Symbol","uri":"https://koreanbear89.github.io/engineering/1.-tools/160101-cheatsheet-latex-math-symbol/","year":"2016"},{"content":"​ with torch.no_grad():\n​ torch.cuda.empty_cache()\n","id":68,"section":"Engineering","summary":"​ with torch.no_grad():\n​ torch.cuda.empty_cache()","tags":null,"title":"","uri":"https://koreanbear89.github.io/engineering/1.-tools/cheatsheet-pytorch/","year":"0001"},{"content":"title: \u0026quot;Cheat Sheet | Mac OS Setup\u0026quot; date: 2022-05-22 09:00:13 categories: [2. Linux, Favorites]  Introduction   어플리케이션 별 단축키 설정\n 시스템 환경설정 \u0026gt; 키보드 \u0026gt; 단축키 \u0026gt; 앱단축키    Automount 해제   diskutil info -all 로 Volume UUID 와 type 확인\n  sudo vi /etc/fstab 맨 아래에 아래와 같이 추가\n  UUID=0D4EE102-A8C3-31A6-A0BD-C82A702697A2 none apfs rw,noauto UUID=BEC5D911-9473-4C3A-97C7-E68999C9ACFD none apfs rw,noauto  ","id":69,"section":"Engineering","summary":"title: \u0026quot;Cheat Sheet | Mac OS Setup\u0026quot; date: 2022-05-22 09:00:13 categories: [2. Linux, Favorites] Introduction 어플리케이션 별 단축키 설정 시스템 환경설정 \u0026gt; 키보드 \u0026gt; 단축키 \u0026gt; 앱단축키 Automount 해제 diskutil info -all 로 Volume UUID 와 type 확인 sudo vi /etc/fstab 맨 아래에 아래와","tags":null,"title":"","uri":"https://koreanbear89.github.io/engineering/6.-mac/set-up-mac/","year":"0001"},{"content":"title: \u0026quot;MLCV #13 | Multimodal Representation\u0026quot; date: 2021-12-07 09:00:13 categories: [2. Machine Learning]  Learing Transferable Visual Models, CLIP (Contrastive Language Image Pretraining  Introduction  Traditional CV-DL models are trained to predict a fixed set of pre-determined object categories =\u0026gt; limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision.   Methods  Extract feature representations of each modality (image, text) Joint multi-modal embedding Scaled pair-wise cosine similarities Symmetric loss function   Conclusion  We have investigated whether it is possible to transfer the success of task-agnostic web-scale pre-training in NLP to another domain.   Reference  OpenAI CLIP: ConnectingText and Images (Paper Explained) - YouTube , [14:40~]      CLIP 은 pretrain 방법의 하나로 NLP 분야에서 BERT 등이 자연어로 pretrain 하여 성능을 대폭 향상시킨 방식으로부터 insight를 얻음 이미지+자연어 데이터셋으로 특정 이미지의 feature $I_f$ 와 대응되는 텍스트의 feature $T_f$ 의 inner product $I_f \\cdot T_f$ 가 최대가 되도록 미리 학습해둔뒤  Inference 할 떄는 추가적인 fine-tuning 없이 test-set의 label들 중에 Inner product가 최대가 되는 label을 선택     # extract feature representations of each modality I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # Joint multimodal embedding [n, d_e] # 아래의 inner-product가 최대가 되도록 W_i, W_t를 학습 I_e = np.dot(I_f, W_i) T_e = np.dot(T_f, W_t) # Scaled pairwise cosine similarities [n,n] logits = np.dot(I_e, T_e.T) # Symmetric loss function loss_i = cross_entropy(Logits, labels, axis=0) loss_t = cross_entropy(logits, labels, axis=1)   A Local-to-Global Approach to Multi-modal Movie Scene Segmentation   Introduction : Recognizing the movie scenes, including the detection of scene boundaries and the understanding of the scene content, facilitates a wide-range of movie understanding tasks such as scene classification, cross movie scene retrieval, human interaction graph and human-centric storyline construction\n  Terminologies :\n shots : captured by a camera that operates for an uninterrupted period of time and thus is visually continuous super-shots : collection of shots, roghly segmented from local (adjacent) features scene : comprises a sequence of shots to present a semantically coherent part of the story  a plot-based semantic unit, where a certain activ-ity takes place among a certain group of character often happens in a fixed place, it is also possi-ble that a scene traverses between multiple places continually      Methods: solve a binary classification problem to determine whether a shot boundary is a scene boundary, by designing a three-level model to incorporate different levels of contextual information based on the shot representation\n Shot Representation, $s_i$  place : ResNet50 pretrained on \u0026ldquo;Places\u0026rdquo; dataset cast : Faster-RCNN pretrained on CIM dataset to detect cast instances and ResNet50 pretrained on PIPA dataset to extract cast features action : TSN pretrained on AVA dataset to get action features audio : NaverNet pretrained on AVA-ActiveSpeaker Dataset to separate speech and background sound and stft   Shot Boundary Representation : propose a Boundary Network to model the shot boundary, $b_i = \\Beta(S_{i-w_b}, \u0026hellip; S_{i+w_b})$  BNet takes a clip of the movie with $2w_b$ shots as input and outputs a boundary representation $b_i$ BNet consists of two branches, namely $B_d$ and $B_r$, that calculate differences and relations betweent adjacent shots   Coarse Prediction at Segment Level : $\\Tau([b_1, \u0026hellip;, b_{n-1}]) = [p_1, \u0026hellip;, p_{n-1}] $  Bi-LSTM predicts a sequence of coarse score $[p_i,\u0026hellip;]$ from sequence of representatives $[b_i,\u0026hellip;]$ with a threshold, we get roughly classified super-shot boundaries $[\\hat{o_i},\u0026hellip;]$   Global Optimal Grouping : $ \\Gamma([\\hat{o_1},\u0026hellip;,\\hat{o_i}]) = [o_1,\u0026hellip;,o_i] $  rmfdabove local segmentation gives us an initial rough scene cut set. Our goal is to merge these super-shots into scenes      Conclusion : this framework is very effective and achieves much better performance than existing methods\n   Multimodal Transformer for Unaligned Multimodal Language Sequences (2019)   Introduction : two major challenges in modeling multimodal human language time-series data\n (1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; (2) long-range dependencies between elements across modalities.    Methods : Multimodal Transformer (MulT) for modeling unaligned multimodal language sequences.\n  Overall Architecture\n Temporal Convolutions $\\hat{X}$ : pass the input seq through 1D Conv layer, project the features of different modalities to the same dimension $d$ Positional Embeddings $Z$ : enable the sequences to carry temporal information, add PE to $\\hat{X}$ Crossmodal Transformers : enables one modality for receiving information from another modality, based on the cross-modal attention blocks    Crossmodal Attention : consider two modalities $\\alpha, \\beta$, calculate attention score using Query from $\\alpha$ and Key from $\\beta$\n$$ \\text{softmax}(\\frac{Q_\\alpha K^T_\\beta}{\\sqrt{d_k}}) V_\\beta $$\n  Result : show that MulT exhibits the best performance when compared to prior methods (Multimodal sentiment analysis, CMU-MOSI \u0026amp; MOSEI)\n     Self-Supervised MultiModal Versatile Network (2020, Deepmind)  Introduction : learn representations using self-supervision by leveraging three modalities naturally present in videos: visual, audio and language streams Methods : multimodal versatile network  Input : unlabelled videos containing different modalities : RGB stream, audio track, linguistic narrations (Automatic Speech Recognition) MultiModal Versatile Networks :  Shared space : all modalities are embedded into a single shared vector space $S_{vat}⊂R^{d_s}$ Disjoint spaces : have different visual-audio $S_{va}$ and visual-text $S_{vt}$ spaces Fine and coarse spaces (FAC) :  visual and the audio (fine-grained) domains are different from the language domain (semantically coarse-grained) in terms of their granularities  fine-grained : there are many visual or sounds of guitars that might be really different to each other coarse-grained : while the textual domain is more coarse as its goal is to abstract away details (e.g. a single “guitar” word)   vision and audio are compared in the fine-grained space ($S_{va}$), while text is compared with vision and audio in the lower dimensional coarse-grained space ($S_{vat}$).   since the text modality is directly obtained from the audio track using ASR, we do not construct the audio-text space nor the loss that puts them in alignment explicitly   Multimodal Contrastive Loss : we construct self-supervised tasks which aim to align pairs of modalities.  positive training pairs across two modalities are constructed by sampling the two streams from the same location of a video. negative training pairs are created by sampling streams from different videos.   Video to image network deflation   Results : exceeds the state-of-the-art for action and audio classification on five challenging benchmarks: HMDB51, UCF101, Kinetics600, ESC-50 and AudioSet   VATT : Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text (2021)   introduction : present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures\n  Methods :\n  (1) Tokenization and Positional Encoding : define a modality-specific tokenization layer\n video : 3D extension of the patching machanism, linear projection on the entire voxels. audio : the raw audio waveform is a 1D input with length $T\u0026rsquo;$, and we partition it to $[T\u0026rsquo;/t\u0026rsquo;]$ segments each containing $t\u0026rsquo;$ waveform amplitudes and then apply a linear projection with a learnable weight $W\\in\\mathbb{R^{t\u0026rsquo; \\times d}}$ to get a $d$ dimensional vector representation. text : construct a vocabulary of size $v$ and map each word to a $v$-dimensional one-hot vetor followed by a linear projection with a learnable weight $W \\in \\mathbb{R}^{v \\times d}$ Drop Token : a simple and yet effective strategy to reduce the computational complexity during training.    The Transformer Architecture : we do not tweak the architecture\n  Common Space Projection\n Video-audio : maps the video and audio Transformers’ outputs to the video-audio common space $S_{va}$ using Linear Projection video-text : the text Transformer’s outputs and the video embedding in the $S_{va}$ space to video-text common space $S_{vt}$ using Linear Projection    Multimodal Contrastive Learning\n  video-audio : Noise Contrastive Estimaiton to align video-audio pairs.\n  video-text : Multiple Instance Learning NCE to align to video-text pairs.\n  overall per-sample objective : for training the entire model end-to-end is as follows:\n$$ L=NCE(z_{v,va},z_{a,va}) +λ MILNCE(z_{v,vt},[z_{t,vt}]),\n$$\n      Results :\n suggests that Transformers are effective for learning semantic video/audio/text representations even if one model is shared across modalities and multi-modal self-supervised pre-training is promising for reducing their dependency on large-scale labeleddata.  Video Action recognition (UCF101, HMDB, Kinetics) Audio Event Classification (ESC50, AudioSet) text to video retrieval (MSR-VTT)       CLIPBERT for Video-and-Language Learning via Sparse Sampling  Introduction CLIPBERT : enables end-to-end learning for video-and-language tasks, by employing sparse sampling   ","id":70,"section":"Research","summary":"title: \u0026quot;MLCV #13 | Multimodal Representation\u0026quot; date: 2021-12-07 09:00:13 categories: [2. Machine Learning] Learing Transferable Visual Models, CLIP (Contrastive Language Image Pretraining Introduction Traditional CV-DL models are trained to predict a fixed set of pre-determined object categories =\u0026gt; limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much","tags":null,"title":"","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml11-multimodal-representation/","year":"0001"}],"tags":[]}