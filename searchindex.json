{"categories":[{"title":"1. CheatSheets","uri":"https://koreanbear89.github.io/categories/1.-cheatsheets/"},{"title":"1. Linear Algebra","uri":"https://koreanbear89.github.io/categories/1.-linear-algebra/"},{"title":"2. Languages","uri":"https://koreanbear89.github.io/categories/2.-languages/"},{"title":"2. Statistics","uri":"https://koreanbear89.github.io/categories/2.-statistics/"},{"title":"3. Computer Vision","uri":"https://koreanbear89.github.io/categories/3.-computer-vision/"},{"title":"3. Mathematics for ML","uri":"https://koreanbear89.github.io/categories/3.-mathematics-for-ml/"},{"title":"3. System Design","uri":"https://koreanbear89.github.io/categories/3.-system-design/"},{"title":"Favorites","uri":"https://koreanbear89.github.io/categories/favorites/"}],"posts":[{"content":"\n1. Introduction and Motivation Machine learning is about designing algorithms that automatically extract valuable information from data. There are three concepts that are at the core of machine learning : data, a model, and learning. Data : Since machine learning is inherently data driven, data is at the core of machine learning. Model : would describe a function that maps inputs to real-valued outputs. Learning : can be understood as a way to automatically find patterns and structure in data by optimizing the parameters of the model 1.1 Finding Words for Intuitions Data as vectors : there are (at least) three different ways to think about vectors: a vector as an array of numbers (computer science view), a vector as an arrow with a direction and magnitude (physics view), a vector as an object that obeys addition and scaling (a mathematical view) Model : A good model can be used to predict what would happen in the real world without performing real-world experiments. Learning : We learn from available data by using numerical optimization methods with the aim that the model performs well on unseen data. 1.2 Two Ways to Read This Book Bottom-up : Building up the concepts from foundational to more ad-vanced. Top-down : Drilling down from practical needs to more basic requirements Part I is about Mathematics : linear algebra : The study of vectors and matrices analytic geometry : the construction of similarity and distances matrix decomposition : Some operations on matrices are extremely useful in ML probability theory : Quantification of uncertainty vector calculus : details concept of gradients optimization : to find maxima/minima of functions Part II is about Machine Learning : linear regression ; to find functions that map inputs $x$ to corresponding observed function values $y$, model fitting via MLE and MAP. dimensionality reduction : to find a compact, lower-dimensional representation of high-dimensional data $x$. density estimation : to find a probability distribution that describes a given dataset. We will focus on Gaussian mixture models for this purpose, and we will discuss an iterative scheme to find the parameters of this model. classification : unlike regression, where the labels were real-valued, the labels in classification are integers, which requires special care. ","id":0,"section":"Mathematics","summary":"1. Introduction and Motivation Machine learning is about designing algorithms that automatically extract valuable information from data. There are three concepts that are at the core of machine learning : data, a model, and learning. Data : Since machine learning is inherently data driven, data is at the core of machine learning. Model : would describe a function that maps inputs to real-valued outputs. Learning : can be understood as a way to automatically find patterns and structure in data by optimizing the parameters of the model 1.","tags":null,"title":"Mathematics for ML #1 | Introduction Part.I ","uri":"https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml01-introduction/","year":"2022"},{"content":"\nIntroduction Tasks:\nImage Classification : The task of classifying an image according to its visual content.\nImage Representation : focus on the way to encode visual contents into vectors (embedding, encoding)\n1. AlexNet (2012) Introduction : CNNs have been prohibitively expensive to apply in large scale to high resolution images. Method : Training on Multiple GPUs def AlexNet(x): out = MP(relu(conv11x11(x))) out = MP(relu(conv5x5(out))) out = relu(conv3x3(out)) out = relu(conv3x3(out)) out = MP(relu(conv3x3(out))) out = FC(relu(FC(relu(FC(out))))) return out 2. VGG Net (2014) Introduction : come up with significantly more accurate ConvNet Method : deeper ConvNet def VGG16(x): out = MP(conv3x3(conv3x3(x))) out = MP(conv3x3(conv3x3(out))) for i in range(3): out = MP(conv3x3(conv3x3(conv3x3(out)))) out = softmax(FC(FC(FC(out)))) return out 3. GoogleNet (2015) Introduction : efficient deeper networks (with fewer params than AlexNet) Method : inception module(NIN, Bottleneck) def inception_block(x): branch_1x1 = conv1x1(x) branch_3x3 = conv3x3(conv1x1(x)) branch_5x5 = conv5x5(conv1x1(x)) branch_pool = conv1x1(MP3x3(x,same)) out = concat([branch_1x1,branch_3x3,branch_5x5,branch_pool]) return out 4. ResNet (Microsoft, 2015) Introduction : to solve the degradation problem caused by deeper layer. Method : Residual Block with shortcut(skip) connection defined as : $$ \\mathbf{x}_{l+1} = \\mathbf{x}_l + F(\\mathbf{x}_l,{W_i}) $$\ndef residual_block(x): out = relu(bn1(conv3x3(x))) out = relu(bn2(conv3x3(out)) + x) return out 5. DenseNet (2016) Introduction : information about the input or gradient can vanish and wash out as CNNs become deep Method : Dense Connectivity (not sum, just concat) Result : 77.85% of top-1 accuracy in ImageNet $$ x_l = H_l([x_0, x_1, \u0026hellip; , x_{l-1} ]) $$\n$[x_0, x_1, \u0026hellip; , x_{l-1}]$ means concatenation of the features-maps produced in previous layers.\ndef dense_block(x): out = conv1x1(relu(bn1(x))) # Bottleneck for comput. efficiency out = conv3x3(relu(bn2(out))) out = concat([x, out]) return out 6. ResNeXt (2016) Introduction : present a improved architecture that adopts ResNets strategy of repeating layers. Method : split-transform-merge strategy (cardinality) Result : 80.9% of top-1 accuracy in ImageNet with 83.6M params $$ \\mathbf{x}_{l+1} = \\mathbf{x}l + \\sum{i=1}^{cardin} F_i(\\mathbf{x}_l) $$\ndef residual_block(x): out = relu(bn1(conv3x3(x, groups=cardinality))) out = relu(bn2(conv3x3(out, groups=cardinality)) + x) return out 7. ShuffleNet(2017) Introduction : extremely computation-efficient CNN architecture named ShuffleNet, designed specially for mobile devices with very limited computing power.\nMethods : utilizes two new operations, pointwise group convolution and channel shuffle\ndivide the channels in each group into several subgroups\nfeed each group in the next layer with difference subgroup\n8. FixResNeXt (2019) Introduction : Existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time. Method : Simple strategy to optimize the classifier performance, that employs different train and test resolution : in face, a lower train resolution improves the classification at test time. Result : 86.4% of top-1 accuracy in ImageNet with 83.6M params (L) conventional augmentation method (R) proposed augmentation method 9. ViT : An Image is Worth 16x16 Words (2020, GoogRes) Introduction :\nTransformer architecture has become the de-facto standard for natural language processing tasks In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. Methods : applying a standard Transformer directly to images\nPatch Embedding $x_i$ : extracts N non-overlapping image patches, performs a linear projection ($E$, is equivalent to a 2D conv) and then rasterises them into 1D token.\nlearnable embedding $z_{cls}$ : an optional learned clasification token (Similar to BERT\u0026rsquo;s [cls]) is prepended to the sequene of embedded patches\nlearnable position embedding $p$ : added to the tokens to retain positional information,\n=\u0026gt; When you have no idea about how to hand-craft positional encoding for your data\n=\u0026gt; Let the transformer figure out for itself what it needs as positional embeddings\n=\u0026gt; simply train the vectors in table of figure at \u0026ldquo;NLP3 \u0026gt; Transformer \u0026gt; Binarized Indexing\u0026rdquo;\n$$ \\mathbf{z} = [z_{cls}, E_{x_1}, E_{x_1}, \u0026hellip;, E_{x_1}] + \\mathbf{p}\n$$\nResult:\nWhen trained on mid-sized datasets such as ImageNet(1.3M) without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size However, the picture changes if the models are trained on larger datasets (JFT-300M, Figure3) 10. VirTex : Learning Visual Representations from Textual Annotations introduction : revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. (1) Semantic density : Captions provide a semantically denser learning signal than unsupervised contrastive methods and supervised classification. (2) simplified data collection : natural language descriptions do not require an explicit ontology and can easily be written by non-expert workers, VirTex : a pretraining approach using semantically dense captions to learn visual representations (1) jointly train a ConvNet and Transformer from scratch to generate natural language captions for images Visual Backbone : a convolutional network which computes visual features of image Textual Head : receives features from thevisual backbone and predicts captions for images (2) transfer the learned features to downstream visual recognition tasks Result show that natural language can provide supervision for learning transferable visual representations with better data-efficiency than other approaches. VirTex matches or exceeds the performance of existing methods for supervised or unsupervised pre-training on ImageNet, despite using up to 10×fewer images ","id":1,"section":"Research","summary":"Introduction Tasks: Image Classification : The task of classifying an image according to its visual content. Image Representation : focus on the way to encode visual contents into vectors (embedding, encoding) 1. AlexNet (2012) Introduction : CNNs have been prohibitively expensive to apply in large scale to high resolution images. Method : Training on Multiple GPUs def AlexNet(x): out = MP(relu(conv11x11(x))) out = MP(relu(conv5x5(out))) out = relu(conv3x3(out)) out = relu(conv3x3(out))","tags":null,"title":"MLCV #1 | Image Classification","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv01-image-classification/","year":"2016"},{"content":"\nIntroduction Tasks\nObject Detection : a task of finding the different objects in an image and classifying them Salient Object Detection : a task based on a visual attention mechanism, in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or RGB images. Metrics:\nAP or mAP is generally used as the primary metrics metric.click here for details Othres\nNon Maximum Suppression (NMS) :\n(1) get multiple bbox for each object\n(2) Leave the most confident bbox and remove other bboxes which has high IoU with it\n1. R-CNN (2013) Introduction : An early application of CNNs to Object Detection tasks\nMethod\nRegion Proposals : Generate a set of proposals ($n=2000$) for bounding boxes using selective search algorithm.\nResize Regions : resize ROI patches to 224x224 for pretrained AlexNet\nClassification : Run the images in b-boxes through a pre-trained AlexNet and SVM to see what object the image in the box is.\nb-box regressor : Run the b-box through a linear regression model to output tighter coordinates\ncf. Selective search looks at the images through windows of different sizes and for each window, tries to group together adjacent pixels by texture, color, or intensity to identify objects\n2. Fast R-CNN (2015) Introduction : RCNN was quite slow because of 2000 (number of Region patches) forward passes per image (for all proposed regions). And also it need to train three different models separately (CNN, SVM, regression). Fast-RCNN tried to solve these problems.\nMethod\npretrained CNN : get top-conv feature map using pretrained CNN\nRegion Proposal : just get ROI coordinates from input image using selective search, and does not make image patches.\nROI pooling : conv-features for each proposed ROI are obtained by selecting a corresponding region from the feature map of input image instead of running CNN for every ROI patches. Then, these conv-features are pooled adaptively.\nClassification \u0026amp; B-Box regressor\n3. Faster R-CNN (2016) Introduction : There was still one remaining bottleneck in the Fast R-CNN : the region proposer based on selective search.\nMethod : adds a Fully Convolutional Network which is called Region Proposal Network between the top-conv feature map and ROI pooling. The RPN slides a window over the top-conv features. At each window location, the network ouputs a score and a bbox per anchor.\nPretrained CNN : Run the image through a CNN to get a (top-conv) feature-map. (returns 14x14x512)\nRegion Proposal Network: slide a small conv-net over the extracted feature-map which maps the input window to lower-dimensional feature (256-dim). Then this lower-dim feature is fed into two sibling FC layers, one for box-classification and the other for box-regression.\n2.1 Classifier : returns 14x14x9x2 (9 for anchor and 2 for object/background)\n2.2 Box Regressor : returns 14x14x9x4 (9 for anchor and 4 for dx, dy, w, h)\nROI Pooling \u0026amp; Classification : same as faster RCNN\nFigure 3. Architecture of Region Proposal Network (RPN) 4. Mask R-CNN (2017) Introduction : to detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance.\nMethod : just add a third branch that outputs the object mask.\nFeature Extraction is same as Faster RCNN\nRPN is same as Faster RCNN\nThe 3rd tails outputs (class + box offset + a binary mask) for each ROI in parallel.\n3.1 : Class labels are collapsed into a short output vectors by FC layers, same as Faster RCNN 3.2 : Box offset is collapsed into a short output vectors by FV layers, same as Faster faster_RCNN 3.3 : $m \\times m $ masks are predicted for each ROI using an FCN ROI Align : If we use ROI pool at the above process, there would be small difference between the real ROI and extracted feature map. It does not matter in classification task, but does in segmentation. To address this problem, authors proposed ROI Align.\n5. SpineNet (2020) Introduction : In past few years, most networks follow the design that encodes input image into intermediate features with monotonically decreased resolutions. Most improvements of network architecture design are in adding network depth and connections within features resolution group.\nAuthors demonstrate that SpineNet can also be used as backbone model in Mask-RCNN Detector and improve both box detection and instance segmentation\n","id":2,"section":"Research","summary":"Introduction Tasks Object Detection : a task of finding the different objects in an image and classifying them Salient Object Detection : a task based on a visual attention mechanism, in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or RGB images. Metrics: AP or mAP is generally used as the primary metrics metric.click here for details Othres Non Maximum Suppression","tags":null,"title":"MLCV #2 | Object Detection","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv02-object-detection/","year":"2016"},{"content":"Introduction Tasks:\nImage Segmentation : The process of assigning a label to every pixel in the image.\nSemantic Segmentation : treats multiple objects of the same class as a single entity.\nInstance Segmentation : treats multiple objects of the same class as distinct individual objects.\n1. FCN (2015) Introduction : The first end-to-end pixel-wise prediction model based only on convolutional layers.\nMethod:\nFeature Extraction : using convolution layers like conventional Image Classification Tasks (layer 1,2,3,4,5) Convolutionalizing : Downsampling using 1x1 conv rather than FC layer(layer 6,7,8) Pixel Wise Classification : Last conv1x1 layer performs pixel wise classification for 21 classes. Upsampling : using deconvolution layer, also called transposed convolution Fusing Output : x32 upsample from pool5 (FCN-32S) + x16 upsample from pool4 (FCN16S) + x8 upsample from pool3 (FCN8S) Figure1. Overview of FCN Architecture Figure2. Overview of upsampling process 2. Mask R-CNN (2017) Introduction : to detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance.\nMethod : just add a third branch that outputs the object mask.\nFeature Extraction is same as Faster RCNN\nRPN is same as Faster RCNN\nThe 3rd tails outputs (class + box offset + a binary mask) for each ROI in parallel.\n3.1 : Class labels are collapsed into a short output vectors by FC layers, same as Faster RCNN 3.2 : Box offset is collapsed into a short output vectors by FV layers, same as Faster faster_RCNN 3.3 : $m \\times m $ masks are predicted for each ROI using an FCN ROI Align : If we use ROI pool at the above process, there would be small difference between the real ROI and extracted feature map. It does not matter in classification task, but does in segmentation. To address this problem, authors proposed ROI Align.\n","id":3,"section":"Research","summary":"Introduction Tasks: Image Segmentation : The process of assigning a label to every pixel in the image. Semantic Segmentation : treats multiple objects of the same class as a single entity. Instance Segmentation : treats multiple objects of the same class as distinct individual objects. 1. FCN (2015) Introduction : The first end-to-end pixel-wise prediction model based only on convolutional layers. Method: Feature Extraction : using convolution layers like conventional","tags":null,"title":"MLCV #3 | Semantic Segmentation","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv03-image-segmentation/","year":"2017"},{"content":"\n0. Introduction Tasks : Image Synthesis : The task of creating new images from some form of image description. 1. GAN (2014) Introduction : A new framework for estimating generative models via an adversarial process\nMethod: simultaneously train two models : a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$.\n$$ \\min_{G} \\max_{D} V(D,G) = \\mathbb{E} _ {x \\sim p_{data}(x)} logD(x) + \\mathbb{E} _ {z \\sim p_z(z)}log(1-D(G(z))) $$\nin terms of Discriminator : $D(x)$ should be 1 and $D(G(z))$ should be 0. So, $D$ is trained to maximize both left and right terms\nin terms of Generator : Left term can be ignored since it\u0026rsquo;s independent of $G$, and $D(G(z))$ should be 1. So, $G$ is trained to minimize right terms.\nin terms of implementation : We need to make two optimizer. Also G_loss and D_loss should be defined respectively.\n2. Conditional GAN (2014) Introduction : The conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y.\nMethod : feeding $y$ into the both the discriminator and generator as additional input layer.\n$$ \\min_{G} \\max_{D} V(D,G) = \\mathbb{E} _ {x \\sim p_{data}(x)} [logD(x,y)] + \\mathbb{E _ {z \\sim p_z(z)}[log(1-D(G(z,y),y))].} $$\ndef generator(x,y): input = concat([x,y],1) layer1 = relu(FC(input, 128)) layer2 = tanh(FC(layer1, 784)) return layer2 def discriminator(x,y): input = concat([x,y],1) layer1 = lrelu(FC(input, 128)) layer2 = tanh(FC(layer1, 1)) return layer2 3. DCGAN (2016) Introduction : Convolutional GANs that make them stable to train in most settings.\nMethod : Following techniques were used for stable Deep Conv GANS.\nReplace any pooling layers with strided convolutions (discriminator) and fractional strided convolutions (generator) Use batchnorm in both the generator and the discriminator Remove fully connected hidden layers for deeper architectures Use ReLU for all layers except for the output which uses Tanh Use LeakyReLU in discriminator for all layers 4. BEGAN (2017) Introduction : A new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based GAN.\nMethod\nuse an auto-encoder as a discriminator as was first proposed in EBGAN. aims to match auto-encoder loss distributions using a loss derived from the Wasserstein distance while typical GANs try to match data distributions directly. This is done using a typical GAN objective with the addition of an equilibrium term to balance the discriminator and the generator. $$ L_D = L(x) - k_t L(G(z_d)) $$\n$$L_G = L(G(z_G))$$\n$$ k_{t+1} = k_t + \\lambda (\\gamma L(x) - L(G(z_G)))$$\n5. GIRAFFE (2021) Introduction : Deep generative models allow for photorealistic image synthesis at high resolutions content, But this is not enough =\u0026gt; creation also needs to be controllable with 3D representation.\nOur key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. GIRAFFE : generating scenes in a controllable and photorealistic manner without additional supervision\n(3.1) model Objects as neural feature fields :\nNeRF (Neural Radiance Fields) : a function f that maps 3D point and viewing direction to a volume density and RGB color value. Input : single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, φ)), Output : the volume density and view-dependent emitted radiance at that spatial location 2D images from different view =\u0026gt; Neural Network =\u0026gt; random view image GRAF (Generative Neural Feature Fields) : unsupervised version of NeRF, trained from unposed image collections with additional latent vector. input : spatial location $\\gamma(x)$, viewing direction, $\\gamma(d)$, shape code $z_s$, appearance code $z_a$ GIRAFFE : replace 3D color output with M-dimensional feature from GRAF represent each object using a separate feature field in combination with an affine transformation (3.2) Scene Compositions\nwe describe scenes as compositions of N entities where the first N−1 are the objects in the scene and the last represents the background (3.3) Scene Rendering\n3D Volume Rendering : For given camera extrinsics, maps above evaluations to the pixel\u0026rsquo;s final feature vector\n2D Neural Rendering : maps the feature image to the final synthesized image (2D CNN)\nResult : By representing scenes as compositional generative neural feature fields, we disentangle individual objects from the background as well as their shape and appearance without explicit supervision. Combining this with a neural renderer yields fast and controllable image synthesis.\n*disentangle : commonly refer to being able to control an attribute of interest, e.g. object shape, size, or pose, without changing other attributes. ","id":4,"section":"Research","summary":"0. Introduction Tasks : Image Synthesis : The task of creating new images from some form of image description. 1. GAN (2014) Introduction : A new framework for estimating generative models via an adversarial process\nMethod: simultaneously train two models : a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$.","tags":null,"title":"MLCV #4 | Image Synthesis","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv04-image-synthesis/","year":"2017"},{"content":"0. Introduction Tasks : Image Style Transfer : The task of migrating a style from one image (Style Image) to another (Content Image). 1. Image Style Transfer using CNNs (2016) Introduction : Introduce a algorithm that can separate and recombine the image content and style of natural images.\nMethod : Extract feature maps $F_l$ from each input image $I_{content} $ and $I_{style}$ using pretrained networks at $l_{th}$ layer. Then, optimize $I_{output}$ to have similar contents with $I_{content}$ and similar style with $I_{style}$.\nThe content loss between $I_{content}$ and $I_{output}$ is calculated using Frobenius norm at $l_{th}$ layer :\n$$L_{content} = \\Sigma(F_{output} - F_{content})^2$$\nThe style loss between $I_{style}$ and $I_{output}$ at $l_{th}$ layer is calculated using Frob. norm and Gram matrix. The style loss is defined by weighted sum of $L_{style}^l$ :\n$$L_{style} = \\sum w_l \\cdot L_{style}^l = \\sum w_l (\\sum(Gram(F_{output}) - Gram(F_{style})))$$\nThe final obejective function is defined as :\n$$ L_{total} = \\alpha L_{content} + \\beta L_{style}$$\n2. pix2pix (2016) Introduction : Conditional adversarial networks as a general-purpose solution to image-to-image translation problems. Method : The generator translate the input image (gray-scale) to target domain(color). And, the discrimator distinguishes between the converted image and real image. $$ \\text{GAN objective} = arg \\min_G \\max_D L_{cGAN}(G,D) + \\lambda L_{L1}(G) $$\nAdversarial Loss , the first term, is from cGAN loss :\n$$ L_{cGAN}(G,D) = \\mathbb{E}_y[log(D(x,y))] + \\mathbb{E}_x[log(1-D(G(x)))]$$\nReconstruction Loss, the second term, is from traditional CNN based loss, which means pixel-wise differences between $y$ and $G(x)$.\n$$ L_{L1}(G) = \\mathbb{E}_{x,y}[| y-G(x) |] $$\nGenerator architecture is based on U-Net and discriminator architecture is based on PatchGAN(Markovian discrimator).\nGenerator is fed on real satellite image instead of latent vector and the pair of images are fed into the discriminator.\nFigure. Overview of pix2pix Architecture 3. cycleGAN (2017) Introduction : For many tasks, paired training data will not be available. Authors present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples.\nMethod : using two discriminator (one for discriminating real $y$ and synthesized $G(x)$, the other for real $x$ and synthetic $F(y)$ ). And additional cycle consistency loss for preventing mode collapse that always return same output but very realistic.\n$$ L(G,F,D_X,D_Y) = L_{GAN}(G,D_Y,X,Y) + L_{GAN}(F,D_X,Y,X) + \\lambda L_{cyc}(G,F) $$\nAdversarial Loss : For the mapping function $G: X \\rightarrow Y $ and its discrimator $D_Y$, we express the objective as :\n$$ L_{GAN} (G,D_Y,X,Y) = \\mathbb{E}{y~p{data}(y)}[(logD_Y(y))] + \\mathbb{E}{x~p{data}(x)}[(1-logD_Y(G(x)))] $$\n$$ L_{GAN} (F,D_X,Y,X) = \\mathbb{E}{x~p{data}(x)}[(logD_X(x))] + \\mathbb{E}{y~p{data}(y)}[(1-logD_X(F(y)))] $$\nCycle Consistency Loss : Adversarial Losses alone cannot guarantee that the learned function can map an individual input $x_i$ to a desired output $y_i$. So authors argue that the learned mapping functions should be cycle-Consistent :\n$$ \\text{Forward cycle consistency} = \\mathbb{E}{x~p{data}(x)}[| F(G(x))-x |_1 ] $$\n$$ \\text{Backward cycle consistency} = \\mathbb{E}{y~p{data}(y)}[| G(F(y))-y | _ {1} ] $$\n$$ L_{cyc} (G,F) = \\text{forward} + \\text{backward} $$\nFigure. Overview of cycleGAN Architecture ","id":5,"section":"Research","summary":"0. Introduction Tasks : Image Style Transfer : The task of migrating a style from one image (Style Image) to another (Content Image). 1. Image Style Transfer using CNNs (2016) Introduction : Introduce a algorithm that can separate and recombine the image content and style of natural images. Method : Extract feature maps $F_l$ from each input image $I_{content} $ and $I_{style}$ using pretrained networks at $l_{th}$ layer. Then, optimize","tags":null,"title":"MLCV #5 | Image Style Transfer","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv05-image-style-transfer/","year":"2018"},{"content":"Introduction Tasks:\nImage Retrieval : aims to find similar images to a query image among an image dataset. Tech Trend :\nConventional Methods : relying on local descriptor matching (scale invariant features - local image descriptors - reranking with spatial verifications)\nusing FC layers : after several conv layers as global descriptors [A Babenko et al, A Gordo et al.]\nusing global pooling methods : from the activations of conv layers.\nboost the performance : by combining different global descriptors which are trained individually.\n1.1 BoF, BoW (Bag of Features, Bag of Visual Words) Introduction : BoW is a simplifying representation used in NLP and information retrieval.\nMethods : BoF groups local descriptors.\nLocal Feature Extraction : Extract local features from image (SIFT, SURF, small img patches)\nClustering : Cluster (k-means) extracted features and find center features (codeword) of each cluster\nImage representation : Represent each image using histogram of codeword.\nLearning and Recognition :\nGenerative ways : based on Bayesian =\u0026gt; classification by histogram of each class\nDiscriminative ways: using classifier like SVM =\u0026gt; put histogram into classifier as a feature vector\nFigure 1. Overview of BoW 1.2 VLAD (Aggregating Local Descriptors) (2010) Introduction : propose a simple yet efficient way to aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation.\nFisher Vector : transform a input variable-size set of independent samples into a fixed size vector representation\nA Gaussian Mixture Model (GMM) is used to model the distribution of features(e.g. SIFT) extracted over the image.\nThe Fisher Vector encodes the gradients of the log-likelihood of the features under the GMM, with respect to the GMM parameters.\nVLAD (Vector of Locally Aggregated Descriptor) : is a feature pooling method, which can be seen as a simplification of the Fisher Kernel. VLAD encodes a set of local feature descriptors extracted from an image using a clustering method such as GMM or K-means.\naccumulate the differences $x-c_i$ for each visual word $c_i$.\nsubsequently $L_2$ normalized by $v = v / ||v||_2$\nCan be written using $a_k$ that assigns descriptor $x_i$ to specific cluster centres $c_k$.\n$$ v_{i,j} = \\sum_{x \\in C} x_j-c_{i,j} = \\sum_{i=1}^N a_k(x_i)(x_i(j)-c_k(j))$$\n2.1 NetVLAD (2016) Introduction : develop a cnn architecture that aggregates mid-level conv features into a compact single vector representation using generalized VLAD layer, NetVLAD.\nMethods : (i) extract top conv featues using pretrained CNN (ii) and pool these features using netVLAD\nnetVLAD : The source of discontinuous in VLAD is hard assignment $a_k(x_i)$ of descriptor $x_i$ to specific cluster centres $c_k$. (If $c_k$ is the closest cluster, $a_k=1$, else, $a_k=0$). Authors replace it to soft assignment (softmax of -distances to each clusters).\n$$ a_k(x_i) = softmax( -|x_i - c_k |^2) = \\frac{e^{-\\alpha | x_i| ^2 + 2\\alpha c_k x_i + |c_k |^2}}{\\sum_{k\u0026rsquo;} e^{-\\alpha | x_i| ^2 + 2\\alpha c_k x_i + |c_{k\u0026rsquo;} |^2}} = \\frac{e^{2\\alpha c_k x_i + |c_k |^2}}{\\sum_{k\u0026rsquo;} e^{ 2\\alpha c_k x_i + |c_{k\u0026rsquo;} |^2}} = \\frac{e^{w_k^T x_i + b_k}}{\\sum_{k\u0026rsquo;} e^{w_{k\u0026rsquo;}^T x_i + b_{k\u0026rsquo;}}}$$\n$$V(j,k) = \\sum_{i=1}^{N} a_k (x_i)(x_i(j) - c_k(j))$$\n3.1 Global Descriptors (~2018) SIFT, SPoC : sum pooling from the feature map which performs well mainly due to the subsequent descriptor whitening.\nMAC , regional MAC : performs max pooling (MAC) over regions then sum over the regional MAC descriptor at the end.\nGeM: generalizes max and average pooling with a pooling parameter\nweighted sum pooling, weighted GeM, multiscale RMAC, etc.\nThe performance of each global descriptor varies by dataset as each descriptor has different properties. For example, SPoC activates larger regions on the image representation while MAC activates more focused regions\n3.2 SPoC, Sum Pooling of Convolution (2015) Introduction : investigate possible ways to aggregate local deep features to produce compact global descriptors for image retrieval.\nMethods :\nSum pooling : The construction of the SPoC descriptor starts with the sum pooling of the deep features.\n$$ \\psi_1(I) = \\sum_{y=1}^{H} \\sum_{x=1}^{W} f(x,y)$$\nCentering prior : objects of interest ted to be located close to the geometrical center of an image. So, incorporate such centering prior using coefficients $\\alpha(w,h)$ , (Gaussian)\n$$ \\psi_2(I) = \\sum_{y=1}^{H} \\sum_{x=1}^{W} \\alpha(x,y)f(x,y)$$\nPost processing : The obtained representation $\\psi(I)$ is subsequently l2 normalized, then PCA compression and whitening are performed.\n3.3 MAC, RMAC, Maximum Activation of Convolution (2015) Introduction : revisit both retrieval stages, namely initial search and reranking\nMethod :\nMaximum Activation of Convolutions (MAC) : the feature vector constructed by a spatial max-pooling over all feature map $\\chi_i$ from last conv.\n$$ \\mathbb{f_{\\Omega}} = [f_{\\Omega,1}, f_{\\Omega,2}, \u0026hellip; ,f_{\\Omega,K}]^T, with f_{\\Omega,i} = max \\chi_i(p)$$\nregional MAC : divide conv feature map to multiple regions(for WxH dim not C) and apply MAC for each regions and post-process it. (l2 and PCA-whitening)\nTwo images are compared with the cosine similarity of the K-dim vector produced as described above.\n3.4 GeM, Generalized Mean Pooling (2017) Introduction : propose a novel trainable Generalized Mean Pooling layer that generalizes max and average pooling and show that it boosts retrieval performance\nMethod :\nConvNet Backbone : given an input image, the output is a 3D tensor $\\chi$ of $W \\times H \\times K$ dimensions\nGeM : add a pooling layer that takes $\\chi$ as an input and produces a vector $\\mathbb{f}$ as an output of the pooling process.\n$$ \\mathbb{f_{\\Omega}} = [f_{\\Omega,1}, f_{\\Omega,2}, \u0026hellip; ,f_{\\Omega,K}]^T, f_{\\Omega,k} = ( \\frac{1}{|\\chi_k|} \\sum_{x \\in \\chi_k}x^{p_k})^{\\frac{1}{p_k}}$$\n4.1 Combination of Multiple Global Descriptors (2019) Introduction : Ensembling different models and combining multiple global descriptors lead to performance improvement. However, these processes are not only difficult but also inefficient with respect to time and memory. Here, authors propose a novel framework that exploits multiple global descriptors to get an ensemble effect while it can be trained in an end-to-end manner.\nMethod : Proposed framework consists of a CNN backbone and two modules. The first main module learns an image representation, which is a combination of multiple global descriptors. Next, an auxiliary module to fine-tune a CNN with a classification loss.\nBackbone Network : can use any CNN such as Inception, ShuffleNet, Resnet. authors use ResNet50 as a baseline backbone.\nMain Module - Multiple Global Descriptors : main module has multiple branches that output each image representation by using different global descriptors (SPoC, MAC, GeM) on the last conv layer. And these discriptions are concatenated after whitening(PCA, FC) and l2 normalization .\nAuxiliary Module : finetunes the CNN backbone based on the first global descriptor of the main module by using a classification loss (train a CNN backbone with a classification loss and then fine-tune the network with a triplet loss). Additional temperature scaling and label smoothing for performance improvement.\n","id":6,"section":"Research","summary":"Introduction Tasks: Image Retrieval : aims to find similar images to a query image among an image dataset. Tech Trend : Conventional Methods : relying on local descriptor matching (scale invariant features - local image descriptors - reranking with spatial verifications) using FC layers : after several conv layers as global descriptors [A Babenko et al, A Gordo et al.] using global pooling methods : from the activations of conv","tags":null,"title":"MLCV #6 | Image Retrieval","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv06-image-retrieval/","year":"2018"},{"content":"Introduction Tasks:\nAction Classification : The task classfying an action in video sequences according to its spatio-temporal content. Benchmark Set\nUCF-101 : is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. HMDB-51 Kinetics : has 400 human action classes with more than 400 examples for each class, each from a unique YouTube video. Methods\nCNN + RNNs\n3D Convolutional Networks\nResNeXt-101 : 6GFLOPs for 112x112x16 Two Stream Network (RGB + Optical Flow)\nTwo Stream 3D ConvNets\nFeature Engineering with pre-extracted frame-level featue using CNN\nSkeleton Based Recognition using GCN\nST-GCN : 16 GFLOPs for one action sample 1. LRCN : Long-term Recurrent Convolutional Networks (2014) Introduction : previous models assume a fixed visual representation or perform simple temporal averaging for sequential processing (such as action recog, image captioning, or etc).\nMethod : Long-term Recurrent Convolutional Networks that can learn compositional representations in space and time.\n2. C3D (2014) Introduction : 3D Convolutional Network for learning spatiotemporal feature from a large scale video dataset\nMethod : 3D ConvNets are just like standard convolutional networks, but with spatio-temporal filters (3x3x3)\n3. Two Stream Network (2014) Introduction : investigate architectures to capture the complementary information on appearance from still frames and motion between frames (optical flow).\nMethod : averaging the predictions from a single RGB frame and a stack of 10 externally computed optical flow frames, after passing them through two replicas of an ImageNet pre-trained ConvNet.\n4. I3D (2017) Introduction : A number of successful image classification architectures have been developed over the years through painstaking trial and error. Instead of repeating the process for spatio-temporal models, authors propose to simply convert successful image(2D) classification models into 3D ConvNets.\nMethod : Two-Stream Infalted 3D ConvNet (I3D) that is based on 2D ConvNet inflation\nInflating 2D into 3D : filters and pooling kernels of 2D ConvNets for image classification are just expanded into 3D.\nTwo 3D Streams : with one I3D network trained on RGB inputs and another on optical flow inputs. Authors trained two networks separately and averaged their predictions at test time.\n5. ActionVLAD (2017) Introduction : 3D CNN or two stream architectures disregard the long-term temporal structure of video. For example, a basketball shoot, can be confused with other actions such as running, dribbling, jumping, throwing, with only few consecutive frames. So we need a global descriptor for the entire video.\nMethods:\nsample frames from the entire video and get top-conv features using a pretrained CNN from RGB and flow each.\nActionVLAD : is a learnable spatio temporal aggregation layers. While max or average pooling are good for similar features, actionVLAD aggregates their residuals from nearest cluster centers.\n$$ V = \\sum_{t=1}^{T} \\sum_{i=1}^{N} {\\frac{e^{-\\alpha || x_{it}-c_k||^2}}{ \\sum_{k\u0026rsquo;} {e^{-\\alpha || x_{it} - c_{k\u0026rsquo;}||^2}}}} (x_{it}[j] - c_k[j]) $$\ncombine VLADs from each stream (get video-level fixed length vector) and pass it through a classifier that outputs the final classification scores.\nDifferent pooling strategies for a collection of diverse features. Points correspond to features from a video and colors correspond to different sub-actions in the video. 6. LOUPE : 1st place at 2017 Youtube-8M (2017) Introduction : Current method for video analysis often extract frame-level features using pre-trained CNNs. Such features are then aggregated over time e.g., by simple temporal averaging or more sophisticated recurrent neural networks such as LSTM or GRU. This work first explore clustering-based aggregation layers.\nMethod :\nCNN Feature Extraction: The input features (frame-level) are extracted from video and audio signals.\nCreate Local feature: The pooling module (e.g. netVLAD) aggregates the extracted features into a single compact (e.g. 1024 dim) representation for the entire video.\nFeature Enhancing: The aggregated representation is then enhanced by the Context Gating Layer.\nClassification: Classification module takes the resulting representation as input and output scores for a pre-defined set of labels.\n7. 3D ResNext (2017) Introduction : Conventional research has only explored relatively shallow 3D architectures. Authors examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets.\nMethod : training 3D CNNs such as ResNet, ResNext, DenseNet on UCF101, HMDB-51 and so on.\n8. SlowFast Networks (2018) Introduction : The recognition of the categorical semantics (colors, textures, lighting etc.) can be refreshed relatively slowly. On the other hand, the motion being performed can evolve much faster. So authors present a two-pathway SlowFast model for video recognition\nMethod : simply can be described as a single stream architecture that operates at two different framerates.\nSlow pathway : can be any spatiotemporal conv model. key concept is a large temporal stride τ (typically 16) on input frames, i.e., it processes only one out of τ frames.\nFast pathway : another conv model which have a small temporal stride\nLateral Connections : The information of the two pathways is fused by lateral connections which have been used to fuse optical flow based, two-stream networks.\n9. ST GCN (2018) Introduction : propose a novel model of dynamic skeletons called ST-GCN\nMethods:\nPose Estimation : construct a spatiotemporal graph with the joints as graph nodes and natural connectivities in both human structures and times as graph edges.\nSkeleton Graph Construntion : The node set $V$ has all the joints in a sequence including estimated coordinates and estimation confidence. The edge set $E$ is composed of two subset, that depicts the intra skeleton connetcion and inter-frame edges.\nSpatial GCN : The feature map $f^t_{in} : V_t \\rightarrow R^c $ has a vector on each node of the graph.\non image, convolution opertion can be written as below with sampling function $p$ and weight function $w$.\n$$ f_{out}(\\mathbb{x}) = \\sum_h \\sum_w f_{in} (p(\\mathbb{x},h,w)) \\cdot w(h,w) $$\nsampling function : On image, neigboring pixels are defined by x as center using kernel size. On graph, neighbor nodes are defined by the minimum length of path from x.\nweight function : is similart to the kernel of 2d convolution. But we have a mapper\n$$ f_{out}(v_{ti}) = \\sum_{v_{tj} \\in B(v_{ti})} \\frac{1}{Z_{ti}(v_{tj})} f_{in}(v_{tj}) \\cdot \\mathbb{w}(l_{ti}(v_{tj})) $$\nspatiotemporal modeling : Until now, we formulated spatial GCN, this can be expanded in temporal dimension simply. By extending the concept of neighborhood to also include temporally connected joints\nPartition strategies : design a partitioning strategy to implement the label map $l$.\n(d) spatial configuration partitioning. The nodes are labeled according to their distances to the skeleton gravity center (black cross), root(green), near(blue), longer(yellow) Limitations : cannot model the correlation between the joints that located further away than the maximum distance D. (left hand and right foot) : resolved by Actional Structural GCN\n10. Shift GCN (2020) Introduction : propose a novel shift graph convolutional network to overcome conventional shortcomings\nComputational complexity of GCN based methods are pretty heavy.\nThe receptive fields of both spatial graph and temporal graph are inflexible\n11. ViViT : A Video Vision Transformer (2021, Goog Res) Introduction : We propose a pure-transformer architecture for video classification, inspired by the recent success of such models for images like ViT.\nMethods :\nEmbedding video clips : two simple methods for mapping a video to a sequence of tokens $\\hat{z}$ (Uniform, Tubelet) and then add the positional embedding and reshape into $z$, the input to the transformer\n$$ \\mathbf{V} \\in \\mathbb{R}^{T\\times H\\times W \\times C} \\mapsto \\hat{z} \\in \\mathbb{R}^{n_t \\times n_h \\times n_w \\times n_d} \\rightarrow z \\in \\mathbb{R}^{N \\times d} $$\nUniform frame sampling (Figure2) : simply sample $n_t$ frames, and embed each 2D frame independently following ViT (Conv2D + Concat)\nTubelet Embedding (Figure3) : extension of ViT\u0026rsquo;s embedding to 3D and corresponds to a 3D convolution.\nTransformer Models for VIdeo\nModel 1) Spatio-temporal attention : simply forwards all spatio-temporal tokens extracted from the video\nAs it models all pairwise interactions, Multi-Headed Self Attention has quadratic complexity with respect to the number of tokens.\nmotivates the development of more efficient architectures\nModel 2) Factorised encoder : consists of two separate transformer encoders\nspatial encoder : only models interactions between tokens extracted from the same temporal index.\ntemporal encoder: consisting ofLttransformer layers to model in-teractions between tokens from different temporal indices.\nResults\nInput Encoding : tubelet embedding initialised using the “central frame” method (Eq. 9) performs well, outperforming the others (Table1)\nModel Variants : The unfactorised model (Model 1) performs the best on Kinetics 400. However, it can also overfit on smaller datasets such as Epic Kitchens, where we find our “Factorised Encoder” (Model 2) to perform the best\n12. TimeSformer : Is Space-Time Attention All You Need for Video Understanding (2021) Introduction : We present a convolution-free approach to video classification\nMethods : TimeSformer (Time-Space Transformer)\nPreprocessing\nInput Clip : TimeSformer takes input clip $X \\in \\mathbb{R}^{H \\times W \\times 3 \\times F}$\nDecomposition into patches ; each frame is decomposed into $N$ non-overlapping patches of $\\mathbf{x_{(p,t)}} \\in \\mathbb{R}^{3P^2}$\nLinear embedding : embedding vector $\\mathbb{z}{(p,t)} \\in \\mathbb{R}^D$ by means of a learnable matrix $E \\in \\mathbb{R}^{D \\times 3P^2}$ with trainable positional embedding $e^{pos}{(p,t)} \\in \\mathbb{R}^D$ : $\\mathbf{z}{(p,t)} = E\\mathbf{x}{(p,t)} + e^{pos}_{(p,t)}$\nClassification Embedding : The final clip embedding is obtained from the final block for the classification token, On top of this representation we append a 1-hidden-layer MLP, which is used to predict the final video classes.\noutput을 다시 aggregation 해서 Modelling\nQuery Key Value computation : At each block $l$, a query/key/value vector is computed for each patch from the representation $z^{(l−1)}_{(p,t)}$encoded by the preceding block\nSelf-attention computation : via dot-product of query and key vector\nEncoding : The encoding is obtained by computing the weighted sum of value vectors using self-attention coefficients from each attention head\nSpace-Time Self Attention Models : temporal attention and spatial attention are separately applied one after the other\nwe first compute temporal attention by comparing each patch(p,t) with all the patches at the same spatial location in the other frames Conclusion : conceptually simple, achieves state of the art results on major action recognition tasks, has low training and inference cost, adn can be applied to clips of over one minute, thus enabling long-term video modeling.\nclass PatchEmbed(nn.Module): self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+1, embed_dim)) ... def forward_features(self, x): x, T, W = self.patch_embed(x) cls_tokens = self.cls_token.expand(x.size(0), -1, -1) x = torch.cat((cls_tokens, x), dim=1) x = x + self.pos_embed ... class Attention(nn.Module): self.qkv = nn.Linear(dim, dim * 3) self.norm1 = norm_layer(dim) ... def forward(self, x): q, k, v = qkv[0], qkv[1], qkv[2] attn = (q @ k.transpose(-2, -1)) * self.scale attn = softmax(dim=-1) x = (attn @ v).transpose(1, 2).reshape(B, N, C) ","id":7,"section":"Research","summary":"Introduction Tasks: Action Classification : The task classfying an action in video sequences according to its spatio-temporal content. Benchmark Set UCF-101 : is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. HMDB-51 Kinetics : has 400 human action classes with more than 400 examples for each class, each from a unique YouTube video. Methods CNN + RNNs 3D Convolutional Networks ResNeXt-101 :","tags":null,"title":"MLCV #7 | Action Classification","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv07-video-classification/","year":"2018"},{"content":"\n8. When Models Meet Data In the first part of the book, we introduced the mathematics that form the foundations of many machine learning methods\nThe second part of the book introduces four pillars of machine learning:\nRegression (Chapter 9) Dimensionality reduction (Chapter 10) Density estimation (Chapter 11) Classification (Chapter 12) 8.1 Data, Models, and Learning Three major components of a machine learning system: data, models, and learning. Good models : should perform well on unseen data. There are two different senses in which we use the phrase “machine learning algorithm”: training and prediction. 8.1.1 Data as Vectors\nWe assume that a domain expert already converted data appropriately, i.e., each input $x_n$ is a D-dimensional vector of real numbers, which are called features, attributes, or covariates by following steps: Raw Data format : In recent years, machine learning has been applied to many types of data that do not obviously come in the tabular numerical format Numerical Representation : Even when we have data in tabular format, there are still choices to be made to obtain a numerical representation. Normalization : Even numerical data that could potentially be directly read into a machine learning algorithm should be carefully considered for units, scaling, and constraint 8.1.2 Models as Functions\nA predictor is a function that, when given a particular input example (in our case, a vector of features), produces an output 8.1.3 Models as Probability Distributions\nInstead of considering a predictor as a single function, we could consider predictors to be probabilistic models, 8.1.4 Learning is Finding Parameters\nThe goal of learning is to find a model and its corresponding parameters such that the resulting predictor will perform well on unseen data. we will consider two schools of machine learning in this book, corresponding to whether the predictor is a function or a probabilistic model. We would like to find good predictors for given training data, with two main strategies : based on some measure of quality (sometimes called finding a point estimate), using Bayesian inference. (requires probabilistic models) 8.2 Empirical Risk Minimization In this section, we consider the case of a predictor that is a function 8.2.1 Hypothesis Class of Functions\nFor given data, we would like to estimate a predictor $f(·,θ):RD →R$, parametrized by $θ$. We hope to be able to find a good parameter $θ^∗$ such that we fit the data well, that is,\n$$ f(x_n,θ^∗) ≈ y_n \\text{ for all } n = 1,\u0026hellip;,N. $$\nAffine functions (linear functions) are used as predictors for conventional ML methods Instead of a linear function, we may wish to consider non-linear functions as predictors. Given the class of functions, we want to search for a good predictor. We now move on to the second ingredient of empirical risk minimization: how to measure how well the predictor fits the training data.\n8.2.2 Loss Function for Training\nEquation below is called the empirical risk and depends on three arguments, the predictor $f$ and the data $X, y$. $$ R_{emp}(f,X,y) = \\frac{1}{N} \\sum_{n}^{N} l(y_n, \\hat{y}_n) $$\nEmpirical risk minimization : This general strategy for learning is called empirical risk minimization 8.2.3 Regularization to Reduce Overfitting\nThe aim of training a ML predictor is so that we can perform well on unseen data Overfitting : the predictor fits too closely to the training data and does not generalize well to new data Regularization : introduce a penalty term that makes it harder for the optimizer to return an overly flexible predictor $$ \\text{min}_θ \\frac{1}{N}∥y − Xθ∥^2 \\rightarrow \\text{min}_θ \\frac{1}{N}∥y − Xθ∥^2 + λ∥θ∥^2 $$\n8.2.4 Cross-Validation to Assess the Generalization Performance\npartitions the data into K chunks, K-1 of which form the trainig set, and the last chunk serves as the validation set. We cycle through all possible partitionings of validation and training sets and compute the average generalization error of the predictor 8.3 Parameter Estimation In this section, we will see how to use probability distributions to model our uncertainty due to the observation process and our uncertainty in the parameters of our predictors. 8.3.1 Maximum Likelihood Estimation\nThe idea behind MLE is to define a function of the parameters that enables us to find a model that fits the data well.\nNegative log-likelihood for data represented by a random variable $x$ and a family of probability densities $p(x|θ)$ is given by :\nThe notation $L_x(θ)$ emphasizes the fact that the parameter $θ$ is varying and the data $x$ is fixed. to find a good parameter vector $θ$ that explains the data well, minimize the negative log-likelihood $L(θ)$ with respect to $θ$. $$ L_x(\\theta) = −logp(x|θ) $$\nMinimizing NLL $L(θ)$ of Gaussian, corresponds to solving the least-squares problem (example 8.5)\nLikelihood detail in section 6.3\n8.3.2 Maximum A Posteriori Estimaion\nIf we have prior knowledge about the distribution of the parameters θ, we can multiply an additional term to the likelihood. using Bayes\u0026rsquo; theorem (6.3) we can compute a posterior distbution $p(θ|x)$, with prior distribution $p(\\theta)$, and likelihood $p(x|θ)$ $$ p(θ|x) = p(x|θ)p(θ) / p(x) $$\n8.3.3 Model Fitting\nOverfitting : refers to the situation where the parametrized model class is too rich to model the dataset Underfitting : we encounter the opposite problem where the model class is not rich enough 8.4 Probabilistic Modeling and Inference We often build models that describe the generative process that generates the observed data. A coin-flip experiment can be described in two steps, (1), we define a parameter μ, which describes the probability of “heads” as the parameter of a Bernoulli distribution (2) we can sample an outcome x ∈ {head, tail} from the Bernoulli distribution $p(x | μ) = Ber(μ)$ In this chapter, we will discuss how probabilistic modeling can be used for this purpose. 8.4.1 Probabilistic Models\nProbabilistic models represent the uncertain aspects of an experiment as probability distributions. Only the joint distribution has this property. Therefore, a probabilistic model is specified by the joint distribution of all its random variables The prior and the likelihood (product rule, Section 6.3) The marginal likelihood p(x), which will play an important role in model selection (Section 8.6), can be computed by taking the joint distribution and integrating out the parameters (sum rule, Section 6.3) The posterior, which can be obtained by dividing the joint by the marginal likelihood. 8.4.2 Bayesian Inference\nIn Section 8.3.1, we already discussed two ways for estimating model parameters θ using maximum likelihood or maximum a posteriori estimation.\nOnce these point estimates θ* are known, we use them to make predictions. More specifically, the predictive distribution will be p(x | θ*), where we use θ* in the likelihood function.\nHaving the full posterior distribution around can be extremely useful and leads to more robust decisions.\n=\u0026gt; Bayesian inference is about finding this posterior distribution\nThe key idea is to exploit Bayes’ theorem to invert the relationship between the parameters θ and the data X (given by the likelihood) to obtain the posterior distribution p(θ | X ).\n8.4.3 Latent Variable Models\nIn practice, it is sometimes useful to have additional latent variables z (besides the model parameters θ) as part of the model. Latent variables may describe the data-generating process, thereby contributing to the interpretability of the model. The conditional distribution $p(x | θ,z)$ allows us to generate data for any model parameters and latent variables. 8.5 Directed Graphical Models In this section, we introduce a graphical language for specifying a probabilistic model, called the directed graphical model provides a compact and succinct way to specify probabilistic models, allows the reader to visually parse dependencies between random variables This section relies on the concepts of independence and conditional independence, as described in Section 6.4.5. Probabilistic graphical models have some convenient properties: They are a simple way to visualize the structure of a probabilistic model. They can be used to design or motivate new kinds of statistical models. Inspection of the graph alone gives us insight into properties, e.g., conditional independence. Complex computations for inference and learning in statistical models can be expressed in terms of graphical manipulations. 8.6 Model Selection In machine learning, we often need to make high-level modeling decisions that critically influence the performance of the model More complex models are more flexible in the sense that they can be used to describe more datasets A polynomial of degree 1 (a line) can only be used to describe linear relations between inputs x and observations y. A polynomial of degree 2 can additionally describe quadratic relationships between inputs and observations. However, we also need some mechanisms for assessing how a model generalizes to unseen test data. =\u0026gt; Model selection is concerned with exactly this problem. Summary In this chapter, We will see how ML algorithms work from a mathematical point of view. Section 8.1, Data, Model, Learning We saw the three main components of ML algorithm : Data, Model, Learning Section 8.2, Empirical Risk Minimization We learned about the perspective of seeing ML model as a functional optimization Loss Function (ex. MSE =\u0026gt; Least Square Problem) Regularization, additional penalty term in loss function Section 8.3, Parameter Estimation We learned about the parameter estimation of data distribution in terms of ML model training. Likelihood (loss function) prior (regularization term) Section 8.4 Probabilistic Modeling and Inference We learned about the Bayesian inference in terms of ML Model inference Section 8.5 Directed Graphical Models Directed Graphical Models can be used to specify a probabilistic model. Section 8.6 Model Selection How can we choose the model that works best for unseen data among trained models. ","id":8,"section":"Mathematics","summary":"8. When Models Meet Data In the first part of the book, we introduced the mathematics that form the foundations of many machine learning methods\nThe second part of the book introduces four pillars of machine learning:\nRegression (Chapter 9) Dimensionality reduction (Chapter 10) Density estimation (Chapter 11) Classification (Chapter 12) 8.1 Data, Models, and Learning Three major components of a machine learning system: data, models, and learning. Good models : should perform well on unseen data.","tags":null,"title":"Mathematics for ML #8 | Introduction Part.II","uri":"https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml08-when-models-meet-data/","year":"2022"},{"content":"Introduction Tasks : Pose Estimation : The task aims to detect the locations of human anatomical keypoints (e.g., elbow, wrist, etc) 1. Deep Pose (2014) Introduction : The first major paper that applied Deep Learning to Human pose estimation\nMethod :\nDNN-based regression : Alexnet backend (7 layers) with an extra final layer that outputs 2k joint coordinates (where $k$ is the number of joints).\nCascade of pose regressors : refinement of the predictions using cascaded regressors.\nSince the ground truth pose vector is defined in absolute image coordinates and poses vary in size from image to image, authors normalize their training set (coordinates)\nlinear regression on top of the last network layer to predict a pose vector by minimizing $L_2$ distance between the prediction and the true pose vector.\n2. Efficient Object Localization Using ConvNets (2015) Introduction : ConvNet architecture which outpus a heatmap, describing the likelihood of a joint occurring in each spatial location\nMethod : Using an additional ConvNet to refine the localization result of the coarse heat-map.\nCoarse Heat-Map Regression Model : Multi-resolution ConvNet that receives multiple input images with the same content but different sizes\nFine Heat Map Regression Model : Siamese Network with $k$ heads($k$ is the number of joint instance)\n*Figure 4. Overview of our Cascaded Architecture 3. Simple Baselines for Human Pose Estimation and Tracking (2018) Introduction : This work provides simple and effective baseline method for human pose estimation(Task1) \u0026amp; pose tracking(Task2).\nMethod :\nModel Architecture : Simply adds a few deconv layers over the last conv layer in the ResNet.\nTraining Strategy : Use the label (heatmap, $H^k$ for joint $k$ is generated by applying a 2D Gaussian centered on the $k^{th}$ joint\u0026rsquo;s ground truth location with std-dev=1 pixel).\nFlow-Based Pose Tracking: Two different kinds of human boxes, one is from a human detector and the other are boxes generated from previous frames using optical flow.\ninput, label = frames, keypoints_to_hmap(keypoints) bbox= Human_Detector(input) keypoints = hmap_to_coord(Pose_Estimator(input, bbox_det)) for i in range(len(input)): bbox_det = Human_Detector(input) bbox_flow = FlowBox_Generator() # Non-maximum suppression : unify detection and flow boxes bbox_unified = NMS(bbox_det, bbox_flow) joints = Pose_Estimator(input[1], bbox_det[1]) sim_matrix = calc_sim(output[i-1], joints) pose = (sim_matrix, id) output.append(pose) # update the output list 4. HR-Net, Microsoft (2019) Introduction : Existing approaches consist of a stem subnetwork, which decreases the resolution based on high-to-low design pattern.\nMethod : Novel network architecture that connects high-to-low subnetworks in parallel that can maintains high-resolution representations through the whole process for spatially precise heatmap estimation.\n*Figure1. Illustrating the architecture of the proposed HRNet. 5. Higher HR-Net Top-Down methods : take a dependency on person detector to detect person instances to reduce the problem.\nHowever, they are normally computationally intensive and not truly end-to-end systems\n","id":9,"section":"Research","summary":"Introduction Tasks : Pose Estimation : The task aims to detect the locations of human anatomical keypoints (e.g., elbow, wrist, etc) 1. Deep Pose (2014) Introduction : The first major paper that applied Deep Learning to Human pose estimation Method : DNN-based regression : Alexnet backend (7 layers) with an extra final layer that outputs 2k joint coordinates (where $k$ is the number of joints). Cascade of pose regressors :","tags":null,"title":"MLCV #8 | Pose Estimation","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv08-pose-estimation/","year":"2019"},{"content":"\n9. Linear Regression In the following, we will apply the mathematical concepts from previous chapters, to solve linear regression (curve fitting) problems.\nIn regression, we aim to find a function $f$ that maps inputs $x∈R^D$ to corresponding function values $f(x)∈R.$\nWe are given a set of training inputs $x_n$ and corresponding noisy observations $y_n=f(x_n) + \\epsilon$,\nwhere $\\epsilon$ is an i.i.d random variable that describes measurement and observation noise =\u0026gt; simply zero-mean Gaussian noise (not further in this chapter)\nSolving a regression function requires a variety of problems:\n(1) Choice of the model (type) and the parametrization of the regression function.\n(2) Finding good parameters.\n(3) Overfitting and model selection.\n(4) Relationship between loss functions and parameter priors.\n(5) Uncertainty modeling\n9.1 Problem Formulation Objective : is to find a function that is close to the unknown function $f$ that generated the observed data (and that generalizes well).\nObservation Noise\nThe functional relationship between input x and y is given as below. Noise $ε∼N(0, σ^2)$ is independent, identically distributed (i.i.d.) Gaussian measurement noise with zero-mean and variance of $σ^2$. $$ y = f (x) + ε $$\nDefine Likelihood function :\nBecause of the presence of observation noise, we will adopt a probabilistic approach and explicitly model the noise using a likelihood function.\nLikelihood function would be Gaussian with mean, $y|f(x)$ and variance, $\\sigma^2$ :\n$$ p(y|x) = N(y|f(x), σ^2) $$\nParametrization :\nWe consider the special case that the parameters $\\theta$ appear linearly in our model. $$ p(y|x,θ) = N(y|x^⊤θ, σ^2) \\newline \\leftrightarrow y=x^⊤θ+ε, ε∼N(0,σ^2) $$\n9.2 Parameter Estimation In the following, we will have a look at parameter estimation by maximizing the likelihood, a topic that we already covered to some degree in Section 8.3. 9.2.1 Maximum Likelihood Estimation (1) Maximum Likelihood Estimation A widely used approach to finding the desired parameters $θ_{ML}$ where we find parameters $θ_{ML}$ that maximize the likelihood. $$ θ_{ML} = \\text{argmax}_\\theta \\ p(Y |X,θ). $$\n(2) Likelihood Factorization : For given training set, $y_i$ and $y_j$ are conditionally independent given their respective inputs $x_i, x_j$ so that the likelihood factorizes according to : $$ p(Y|X,θ) = p(y_1,\u0026hellip;,y_N |x_1,\u0026hellip;,x_N,θ) $$\n$$ = \\prod p(y_n |x_n,θ) = \\prod N(y_n |x^⊤_n θ, σ^2) \\tag{9.5b} $$\n(3) Log-Transformation : Since the likelihood (9.5b) is a product of N Gaussian distributions, the log-transformation is useful To find $\\theta_{ML}$, we can minimize the negative log-likelihood, which is equal to some of squared errors $$ −logp(Y|X,θ) = −log \\prod p(y_n|x_n,θ) = − \\sum logp(y_n|x_n,θ) $$\n$$ logp(y_n|x_n,θ)= − \\frac{1}{2\\sigma^2} (y_n−x^⊤_nθ)^2 +const = L(\\theta) $$\n(4) Global Optimum : We can find the global optimum by computing the gradient of $L$, setting it to 0 and solving for $θ$ $$ \\frac{dL}{d\\theta} = 0^⊤ \\Longleftrightarrow θ_{ML} = (X^⊤X)^{−1}X^⊤y $$\nPolynomial Regression (MLE with Features) : straight lines are not sufficiently expressive when it comes to fitting more interesting data Estimating the Noise Variance : We can also use the principle of MLE to obtain the maximum likelihood estimator $\\sigma_{ML}^2$ for the noise variance 9.2.2 Overfitting in Linear Regression Model Selection : we can use the RMSE (or the negative log-likelihood) to determine the best degree of the polynomial that minimizes the objective.\npolynomials of low degree fit the data poorly\nhigh-degree polynomials oscillate wildly and are a poor representation of the underlying function that generated the data, such that we suffer from overfitting\n9.2.3 Maximum A Posteriori Estimation (1) Prior and Posterior :\nThe posterior over the parameters θ, given the training data $X,Y$ is obtained by applying Bayes’ theorem (Section 6.3) as : $$ p(θ|X,Y)= \\frac{p(Y|X,θ)p(θ)}{p(Y | X )} $$\nSince the posterior explicitly depends on the parameter prior $p(θ)$, the prior will have an effect on the parameter vector we find as the maximizer of the posterior. (2) Log-Transformation :\nTo find the MAP estimate, We start with the log-transform and compute the log-posterior $$ log p(θ | X , Y) = log p(Y | X , θ) + log p(θ) + const , $$\nWith a (conjugate) Gaussian prior $p(θ) = N(0, b^2I)$􏰂on the parameters θ, we obtain the negative log posterior $$ −logp(θ|X,Y)= \\frac{1}{2\\sigma^2}(y−Φθ)^⊤(y−Φθ)+ \\frac{1}{2b^2} θ^⊤θ+const. $$\nAnd then, we minimize the negative log-poterior distribution with respect to $\\theta$\n$$ θ_{MAP} ∈ argmin_\\theta[−logp(Y|X,θ)−logp(θ)]. $$\n(3) Gradient :\nGet gradient of the negative log-posterior with respect to θ, and we can see the first term on the right-had side as the gradient of the negative log-lgikelihood\n$$ − \\frac{dlogp(θ|X,Y)}{d\\theta} = −\\frac{dlogp(Y|X,θ)}{d\\theta} − \\frac{dlogp(θ)}{d\\theta} $$\n(4) Global Optimum : We will find $θ_{MAP}$ by setting this gradient to $0^⊤$ and solving for $θ_{MAP}$\n$$ θ_{MAP} = (Φ^⊤Φ+\\frac{σ^2}{b^2}I)^{-1}Φ^⊤y $$\n9.2.4 MAP Estimation as Regularization Regularized least squares :\nIn Regularized Least Squares, we consider the loss function as below, which we minimize with respect to θ (see Section 8.2.3) $$ ∥y − Φθ∥^2 + λ ∥θ∥^2_2 $$\nthe first term is a data-fit term, which is proportional to the negative log-likelihood; see (9.10b).\nThe second term is called the regularizer, and the regularization parameter λ\u0026gt;0 controls the “strictness” of the regularization =\u0026gt; can be interpreted as a negative log-Gaussian prior\n9.3 Bayesian Linear Regression Bayesian linear regression : pushes the idea of the parameter prior a step further does not even attempt to compute a point estimate of the parameters, but instead the full posterior distribution over the parameters is taken into account when making predictions 9.3.1 Model Prior, $p(θ) = N(m_0, S_0)$\nlikelihood, $p(y | x, θ) = N (y | φ^⊤(x)θ, σ^2)$\nFull probabilistic model : the joint distribution of observed and unobserved random variables, y and θ, respectively, is:\n$$ p(y,θ|x) = p(y|x,θ)p(θ) $$\n9.3.2 Prior Predictions Introduction\nwe are usually not so much interested in the parameter θ themselves.\nInstead, our focus often lies in the predictions we make with those parameter values.\nIn a Bayesian setting, we take the parameter distribution and average over all plausible parameter settings when we make predictions.\nMore specifically, to make predictions at an input x∗, we integrate out θ and obtain :\n$$ p(y_∗| x_∗) = \\int p(y_∗| x_∗, θ)p(θ)dθ = E_θ[p(y_∗|x_∗, θ)] $$\nSummary\nSo far, we looked at computing preds using the parameter prior $p(θ)$.\nHowever, when we have a parameter posterior (given some training data X, Y) =\u0026gt; we just need to replace the prior $p(θ)$ with the posterior $p(θ|X,Y)$.\n9.3.3 Posterior Prediction Bayes\u0026rsquo; theorem : Given a training set of inputs $x_n ∈ R^D$ and corresponding observations $y_n ∈ R$, we compute the posterior over the parameters using Bayes’ theorem : $$ p(θ|X,Y) = \\frac{p(Y|X,θ)p(θ)}{p(Y|X)} $$\nMarginal Likelihood : independent of the parameters θ and ensures that the posterior is normalized, the likelihood averaged over all possible parameter settings\nPosterior Predictions\nIn principle, predicting with the parameter posterior p(θ|X,Y) is not fundamentally different given that in our conjugate model the prior and posterior are both Gaussian (with different parameters).\nTherefore, by following the same reasoning as in Section 9.3.2, we obtain the (posterior) predictive distribution\nSKIP DETAILS\n9.4 Maximum Likelihood as Orthogonal Projection Introduction : we will now provide a geometric interpretation of MLE. Let us consider a simple linear regression setting. $$ y=xθ+ε, ε∼N(0,σ^2), \\tag{9.65} $$\nProblem Definition :\nThe parameter θ determines the slope of the line.\nWe obtained (9.2.1) the MLE for the slope parameter as (9.66)\nThis means, $θ_{ML}$ is the approximation with the minimum least-squares error between y and Xθ. (9.67)\n$$ θ_{ML} = (X^⊤X)^{−1}X^⊤y \\tag{9.66} $$\n$$ Xθ_{ML} = (X^⊤X)^{−1}XX^⊤y = \\frac{XX^⊤}{X^⊤X}y \\tag{9.67} $$\nSolution : As we are looking for a solution of $y = Xθ$, we can think of linear regression as a problem for solving systems of linear equations\nIn particular, looking carefully at (9.67) we see that the $θ_{ML}$ in our example from (9.65) effectively does an orthogonal projection of y onto the one-dimensional subspace spanned by X\nRecalling the results on orthogonal projections from Section 3.8, we identify $\\frac{X X^⊤}{X⊤X}$ as the projection matrix, Therefore, the maximum likelihood solution provides also a geometrically optimal solution by finding the vectors in the subspace spanned by X that are “closest” to the corresponding observations y,\nIn the general (polynomial) linear regression case, we again can interpret the maximum likelihood result as a projection onto a K-dimensional subspace of $R^N$.\nSummary In this chapter, we will see how to find a function that maps input $x$ to $y$ in observed training data set.\nSection 9.1, \u0008Problem Formulation\nFunctional Relationship : Objective is to find a funtion that is close to the unknown function $f$ that generated the observed data\nObservation noise : Set the observation noise $ε∼N(0, σ^2)$ as (i.i.d.) Gaussian measurement with zero-mean and variance of $σ^2$.\nSection 9.2, Parameter Estimation\nMaximum Liklihood Estimation : We find the model parameters $\\theta_{ML}$ that maximize the likelihood.\n(1) Define the likelihood function : with gaussian function\n(2) Likelihood Factorization : training samples are independent to each other, the likelihood factorizes according to $\\prod N(y_n |x^⊤_n θ, σ^2) $\n(3) Log Transformation :\nLikelihood, a product of N Gaussian Distributions, can be transformed to sum of N log gaussian dist. The problem also transformed to minimize the negative log-likelihood (4) Global Minimum : We can find the global optimum by computing the gradient\nMaximum A Posterior : MLE can lead to severe overfitting, MAP addresses this issue by placing a prior on the params that play the role of a regularizer\nSection 9.3, Bayesian Linear Regression\nBayesian Regression : From section 9.2, we focused on estimating appropriate parameters that work well, However in section 9.3, we focus on prediction itself, not parameters, so we just use parameter distribution and average over all plausible parameter settings when we make predictions\nPrior Prediction : to make predictions at an input $x_∗$, we integrate out θ.\nPosterior Prediction : when we have a parameter posterior (given some training data X, Y) then, we just need to replace the prior $p(θ)$ with the posterior $p(θ|X,Y)$\nSection 9.4, Maximum Likelihood as Orthogonal Projection\nThe MLE solution provides also a geometrically optimal solution by finding the vectors in the subspace spanned by X that are “closest” to the corresponding observations y ","id":10,"section":"Mathematics","summary":"9. Linear Regression In the following, we will apply the mathematical concepts from previous chapters, to solve linear regression (curve fitting) problems.\nIn regression, we aim to find a function $f$ that maps inputs $x∈R^D$ to corresponding function values $f(x)∈R.$\nWe are given a set of training inputs $x_n$ and corresponding noisy observations $y_n=f(x_n) + \\epsilon$,\nwhere $\\epsilon$ is an i.i.d random variable that describes measurement and observation noise =\u0026gt; simply zero-mean Gaussian noise (not further in this chapter)","tags":null,"title":"Mathematics for ML #9 | Linear Regression","uri":"https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml09-linear-regression/","year":"2022"},{"content":"Introduction Tasks :\n3D object detection : classifies the object category and estimates oriented 3D bounding boxes of physical objects from 3D sensor data. Applications : By extending prediction to 3D, one can capture an object’s size, position and orientation in the world, leading to a variety of applications in robotics, self-driving vehicles, image retrieval, and augmented reality\nBenchmarks\nKITTI (car, cyclist, pedestrian easy, mod, hard) : for autonomous driving ScanNet : for indoor scene understanding task SUN-RGBD : Scene Understanding Benchmarks YCB-V : 21 objects from the YCB dataset captured in 92 videos with 133,827 frames. Approches\nApproaches based on Point Clouds (LiDAR)\nProjection based methods : projects point clouds to 2D planes such as bird view, front view, etc. + RPN Volumetric methods : voxelization + heavy 3D CNN Point-net based methods : extract features from raw data Graph based methods : construct graphs to learn the order-invariant of point clouds Approaches based on monocular/stereo images\nApproaches based on cheaper monocular or stereo imagery data have resulted in drastically lower accuracies until now. A key ingredient for image-based 3D object detection methods is a reliable depth estimation approach to replace LiDAR. Existing algorithms are largely built upon 2D object detection, imposing extra geometric constraints to create3D proposals. apply stereo-based depth estimation to obtain the true 3D coordinates of each pixel. 1. SUN-RGBD(2017) An RGB-D benchmark suite for all major scene understanding tasks\nDataset is captured by four different sensors and contains 10,335 RGB-D images.\nDensely annotated and includes 146k 2d polygons, and 64k 3D bboxes with accurate object orientations, as well as a 3D room layout and scene category for each images.\n2. ScanNet(2017) An RGB-D Video Dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. 3. Pseudo LiDAR(2019) Introduction : Mono/stereo depth estimation based methods show worse performance than LiDAR based methods because of poor precision of image based depth estimation.\nContrib : convert image-based depth maps to pseudo-LiDAR representations\nMethods :\nconvert the estimated depth map from stereo or monocular imagery into a 3D point cloud, which we refer to as pseudo-LiDAR\nthen take advantage of ex-isting LiDAR-based 3D object detection pipelines\n4. Realtime 3D Obj Detection on Mobile Dev with Media Pipe (Google, 2020) Introduction : detects objects in 2D images, and estimates their poses and sizes through a ML model, trained on a newly created 3D dataset\nObtaining real world 3D Training Data : using ARCore, Google\u0026rsquo;s AR Platform, integrated in most smartphones, which can provide the information of the camera pose, sparse 3D point clouds, estimated lighting, and planar surfaces.\nAR Synthetic Data Generation : places virtual objects into scenes that have AR session data.\nAn ML Pipeline : a single-stage model to predict the pose and physical size of an object from a single RGB image\n5. SVGA-Net(2020) Introduction : graph based methods for point clouds data, 2020 SOTA in KITTI datasets\nMethods : Spars Voxel Graph Attention Network\nAppendix. LiDAR, ToF and Point Clouds Introduction : LiDAR systems send out pulses of light just outside the visible spectrum and register how long it takes each pulse to return. Once the individual readings are processed and organised, the LiDAR data becomes point cloud data. A 3D point cloud is a collection of data points analogous to the real world in three dimensions. Each point is defined by its own position and (sometimes) colour. Photogrammetric point clouds : have an RGB value for each point, resulting in a colourised point cloud (acqutision using digital camera rather than LiDAR, worse than LiDAR in terms of accuracy). Difference between LiDAR and TOF : A ToF(most android phones) is a scannerless LiDAR system, relying on a single pulse of light to map an entire space, while Apple is using scanner LiDAR, which uses multiple points of light to take these readings much more frequently and with greater accuracy. LiDAR \u0026amp; TOF in mobile : LG G8 (2019) , Galaxy Note10, S20, IPAD pro 4, iphone12 , Huawei Mate30, Sony Xperia2 Appendix. AR Applications on smartphones Google AR Core Depth API : Single Camera Depth Estimation\nIKEA Place : allows the user to directly insert furniture into a picture of the room\nmodiFace : virtual beauty counter\nmeasure kit : AR Ruler\npixie : find lost key through the iPhone screen. (with Bluetooth hardware)\nmagic sudoku : Solve sudoku puzzles in AR\nAppendix. Stereo/monocular image based depth Estimation Benchmarks\nNYU Depth (Mono) : includes 1449 densely labeled pairs of aligned RGB and depth images KITTI Eigen Split (Mono) : part of the KITTI dataset proposed by D.Eigen et al. for monocular depth estimation task BTS (monocular) : a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase.\nDORN (monocular) : combine multi-scale features with ordinal regression to predictpixel depth with remarkably low errors\nPSMNet : applies Siamese networks for disparity estimation, followed by 3D convolutions for refinement\nY Wang et al. : has made these methods more efficient, enabling accurate disparity estimation to run at 30 FPS on mobile devices.\nAppendix. 3D Modeling IKEA 3D Models : includes a lot of simple CAD models, but there\u0026rsquo;s no additional information required for detection like RGBD, point clouds, etc. These 3D Models can be used to estimate the 3D pose of specific objects in image ( Lim et al. ICCV2013 ) ","id":11,"section":"Research","summary":"Introduction Tasks : 3D object detection : classifies the object category and estimates oriented 3D bounding boxes of physical objects from 3D sensor data. Applications : By extending prediction to 3D, one can capture an object’s size, position and orientation in the world, leading to a variety of applications in robotics, self-driving vehicles, image retrieval, and augmented reality Benchmarks KITTI (car, cyclist, pedestrian","tags":null,"title":"MLCV #9 | 3D Object Detection","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv09-3d-object-detection/","year":"2020"},{"content":"Introduction API (Application Programming Interface) interface : a connection between two pieces of electronic equipment, or between a person and a computer\nAPI : an application interface made for easy use in another application\nex) Weather App =\u0026gt; call API from weather station server =\u0026gt; Get weather condition =\u0026gt; User HTTP (HyperText Transfer Protocol) Non HTTP : MQTT, CoAP (low-level, low-energy protocol) REST (Representational State Transfer) ","id":12,"section":"Engineering","summary":"Introduction API (Application Programming Interface) interface : a connection between two pieces of electronic equipment, or between a person and a computer\nAPI : an application interface made for easy use in another application\nex) Weather App =\u0026gt; call API from weather station server =\u0026gt; Get weather condition =\u0026gt; User HTTP (HyperText Transfer Protocol) Non HTTP : MQTT, CoAP (low-level, low-energy protocol) REST (Representational State Transfer) ","tags":null,"title":"Networks #1 | API, HTTP, REST","uri":"https://koreanbear89.github.io/engineering/2.-languages/web-server/","year":"2022"},{"content":"\n1. Introduction GlusterFS : is a Scalable Network Filesystem Brick : is the basic unit of storage in GlusterFS, Volume : is a logical collection of bricks. Create volume with bricks from several nodes (peer) And mount volume to the specific location to use volume like virtual Storage References Tutorial: Create a Docker Swarm with Persistent Storage Using GlusterFS – The New Stack # install GlusterFS on CentOS sudo yum install centos-release-gluster sudo yum install glusterfs-server sudo systemctl start glusterd # start daemon sudo systemctl enable glusterd # restart daemon when reboot the machine # Configure Gluster Volume gluster peer probe \u0026lt;HOST_NAME\u0026gt; # connect HOST as peer gluster pool list # show gluster peers # Create Volume sudo mkdir -p /\u0026lt;BRICK_DIR\u0026gt; # run on all machines sudo gluster volume create \u0026lt;VOLUME_NAME\u0026gt; replica \u0026lt;N\u0026gt; \u0026lt;HOST1\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST2\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST3\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; force # run only on the master # ex) sudo gluster volume create gluster_pvs replica 4 pgsca2x0350:/home1/irteam/whome/__oss/gs_pvs pgsca2x0351:/home1/irteam/whome/__oss/gs_pvs pgsca2x0352:/home1/irteam/whome/__oss/gs_pvs pgsca2x0353:/home1/irteam/whome/__oss/gs_pvs force # Start Volume sudo gluster volume start \u0026lt;VOLUME_NAME\u0026gt; # run only on the master sudo mount.glusterfs localhost:/\u0026lt;VOLUME_NAME\u0026gt; \u0026lt;MOUNT_LOCATION\u0026gt; # run on all machines # ex) sudo mount.glusterfs localhost:/gluster_pvs /home1/irteam/mnt_gluster # Manage Gluster Volumes gluster volume info # show information of volume and bricks gluster volume stop \u0026lt;VOLUME_NAME\u0026gt; gluster volume delete \u0026lt;VOLUME_NAME\u0026gt; ","id":13,"section":"Engineering","summary":"1. Introduction GlusterFS : is a Scalable Network Filesystem Brick : is the basic unit of storage in GlusterFS, Volume : is a logical collection of bricks. Create volume with bricks from several nodes (peer) And mount volume to the specific location to use volume like virtual Storage References Tutorial: Create a Docker Swarm with Persistent Storage Using GlusterFS – The New Stack # install GlusterFS on CentOS sudo yum install centos-release-gluster sudo yum install glusterfs-server sudo systemctl start glusterd # start daemon sudo systemctl enable glusterd # restart daemon when reboot the machine # Configure Gluster Volume gluster peer probe \u0026lt;HOST_NAME\u0026gt; # connect HOST as peer gluster pool list # show gluster peers # Create Volume sudo mkdir -p /\u0026lt;BRICK_DIR\u0026gt; # run on all machines sudo gluster volume create \u0026lt;VOLUME_NAME\u0026gt; replica \u0026lt;N\u0026gt; \u0026lt;HOST1\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST2\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST3\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; force # run only on the master # ex) sudo gluster volume create gluster_pvs replica 4 pgsca2x0350:/home1/irteam/whome/__oss/gs_pvs pgsca2x0351:/home1/irteam/whome/__oss/gs_pvs pgsca2x0352:/home1/irteam/whome/__oss/gs_pvs pgsca2x0353:/home1/irteam/whome/__oss/gs_pvs force # Start Volume sudo gluster volume start \u0026lt;VOLUME_NAME\u0026gt; # run only on the master sudo mount.","tags":null,"title":"CheatSheet | GlusterFS","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-glusterfs/","year":"2022"},{"content":"\n1. Setup Jupyter-Lab $ conda install -c conda-forge jupyter jupyterlab nbconvert # nodejs\u0026gt;12.0.0 needed $ conda install -c conda-forge nodejs $ conda install -c conda-forge/label/gcc7 nodejs $ conda install -c conda-forge/label/cf201901 nodejs $ conda install -c conda-forge/label/cf202003 nodejs # v13.10 $ conda update nodejs # v16.13 # if above not workconda uninstall --force nodejs $ conda uninstall --force nodejs # remove all $ conda install nodejs -c conda-forge --repodata-fn=repodata.json $ pip install jupyterlab-unfold # theme $ pip install theme-darcular # do not need new node version $ conda install -c conda-forge theme-darcula $ jupyter labextension install @arbennett/base16-nord # theme ","id":14,"section":"Engineering","summary":"\n1. Setup Jupyter-Lab $ conda install -c conda-forge jupyter jupyterlab nbconvert # nodejs\u0026gt;12.0.0 needed $ conda install -c conda-forge nodejs $ conda install -c conda-forge/label/gcc7 nodejs $ conda install -c conda-forge/label/cf201901 nodejs $ conda install -c conda-forge/label/cf202003 nodejs # v13.10 $ conda update nodejs # v16.13 # if above not workconda uninstall --force nodejs $ conda uninstall --force nodejs # remove all $ conda install nodejs -c conda-forge --repodata-fn=repodata.json $ pip install jupyterlab-unfold # theme $ pip install theme-darcular # do not need new node version $ conda install -c conda-forge theme-darcula $ jupyter labextension install @arbennett/base16-nord # theme ","tags":null,"title":"CheatSheet | Jupyter","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-jupyter/","year":"2021"},{"content":"\nIntroduction reference. https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf # including kor/eng/num/- \u0026quot;[가-힣|a-z|0-9|\\-]+\u0026quot; 1. Special Characters ^ | Matches the expression to its right at the start of a string. It matches every such instance before each \\n in the string. $ | Matches the expression to its left at the end of a string. It matches every such instance before each \\n in the string. . | Matches any character except line terminators like `\\n`` ``` | Escapes special characters or denotes character classes A|B | Matches expression A or B. If A is matched first, B is left untried + | Greedily matches the expression to its left 1 or more times * | Greedily matches the expression to its left 0 or more times ? | Greedily matches the expression to its left 0 or 1 times. But if ? is added to qualifiers (+, *, and ? itself) it will perform matches in a non-greedy manner {m} | Matches the expression to its left m times, and not less {m,n} | Matches the expression to its left m to n times, and not less. {m,n}? | Matches the expression to its left m times, and ignores n. See ? above. 2. Character Classes (a.k.a. Special Sequences) \\w | Matches alphanumeric characters, which means a-z, A-Z, and 0-9. It also matches the underscore, _. \\d | Matches digits, which means 0-9. \\D | Matches any non-digits. \\s | Matches whitespace characters, which include the \\t, \\n, \\r, and space characters. \\S | Matches non-whitespace characters. \\b | Matches the boundary (or empty string) at the start and end of a word, that is, between \\w and \\W. \\B | Matches where \\b does not, that is, the boundary of \\w characters. \\A | Matches the expression to its right at the absolute start of a string whether in single or multi-line mode. \\Z | Matches the expression to its left at the absolute end of a string whether in single or multi-line mode. 3. Sets [ ] | Contains a set of characters to match. [amk] | Matches either a, m, or k. It does not match amk. [a-z] | Matches any alphabet from a to z. [a\\-z] | Matches a, -, or z. It matches - because \\ escapes it. [a-] | Matches a or -, because - is not being used to indicate a series of characters. [-a] | As above, matches a or -. [a-z0-9] | Matches characters from a to z and also from 0 to 9. [(+*)] | Special characters become literal inside a set, so this matches (, +, *, and ). [^ab5] | Adding ^ excludes any character in the set. Here, it matches characters that are not a, b, or 5. 4. Groups ( ) | Matches the expression inside the parentheses and groups it.\n(? ) | Inside parentheses like this, ? acts as an extension notation. Its meaning depends on the character immediately to its right.\n(?PAB) | Matches the expression AB, and it can be accessed with the group name.\n(?aiLmsux) | Here, a, i, L, m, s, u, and x are flags:\na — Matches ASCII only i — Ignore case L — Locale dependent m — Multi-line s — Matches all u — Matches unicode x — Verbose (?:A) | Matches the expression as represented by A, but unlike (?PAB), it cannot be retrieved afterwards.\n(?#...) | A comment. Contents are for us to read, not for matching.\nA(?=B) | Lookahead assertion. This matches the expression A only if it is followed by B.\nA(?!B) | Negative lookahead assertion. This matches the expression A only if it is not followed by B.\n(?\u0026lt;=B)A | Positive lookbehind assertion. This matches the expression A only if B is immediately to its left. This can only matched fixed length expressions.\n(?\u0026lt;!B)A | Negative lookbehind assertion. This matches the expression A only if B is not immediately to its left. This can only matched fixed length expressions.\n(?P=name) | Matches the expression matched by an earlier group named “name”.\n(...)\\1 | The number 1 corresponds to the first group to be matched. If we want to match more instances of the same expresion, simply use its number instead of writing out the whole expression again. We can use from 1 up to 99 such groups and their corresponding numbers.\n5. Popular Python re Module Functions re.findall(A, B) | Matches all instances of an expression A in a string B and returns them in a list. re.search(A, B) | Matches the first instance of an expression A in a string B, and returns it as a re match object. re.split(A, B) | Split a string B into a list using the delimiter A. re.sub(A, B, C) | Replace A with B in the string C. ","id":15,"section":"Engineering","summary":"Introduction reference. https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf # including kor/eng/num/- \u0026quot;[가-힣|a-z|0-9|\\-]+\u0026quot; 1. Special Characters ^ | Matches the expression to its right at the start of a string. It matches every such instance before each \\n in the string. $ | Matches the expression to its","tags":null,"title":"CheatSheet | Regex","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-regex/","year":"2021"},{"content":"\nIntroduction Nginx : light-weight Web server\ngenerally used as HTTP Web Server, Reverse Proxy Server, Load Balancer Reverse Proxy\nProxy server : is a server that acts as an intermediary between a client and the server\nLoad balancing : Proxies distribute requests to a group of servers,\nEncryption : Proxies encrypt the data to keep it from being read during transfer.\nCaching : Proxies speed up access to information by storing the results of user requests in the server’s cache for a specified time\nLoad Balancer\nGeneral Usage\nNginx as a independent docker container\nNginx as a process in web framework container\n1. Start Nginx as a independent docker container Run Nginx in a independent docker container\ndocker pull nginx\ndocker run --name nginx-server -d -p 80:80 nginx\nCustomize nginx container : docker build -t nginx:test .\n# /nginx/nginx.conf # default.conf file includes *.conf by \u0026quot;include /etc/nginx/conf.d/*.conf;\u0026quot; upstream app { server flask:5000; # UPSTREAM_CONTAINER_NAME:PORT } server { listen 80; # NGINX_PORT location / { proxy_pass http://app; # we defined upstream app above } } # Dockerfile.nginx FROM nginx:latest COPY default.conf /etc/nginx/conf.d/default.conf CMD [\u0026quot;nginx\u0026quot;, \u0026quot;-g\u0026quot;, \u0026quot;daemon off;\u0026quot;] Run docker-compose : docker-compose up\nversion: '3' services: flask: container_name: flask image: \u0026quot;flask:test\u0026quot; ports: - \u0026quot;5000:5000\u0026quot; networks: - backend nginx: container_name: nginx image: \u0026quot;nginx:test\u0026quot; ports: - \u0026quot;80:80\u0026quot; networks: - backend networks: # basic bridge network backend: driver: bridge 2. Start Nginx as a process in web app (WSGI) container Prepare nginx.conf\n```nginx # /nginx/nginx.conf # default.conf file includes *.conf by \u0026quot;include /etc/nginx/conf.d/*.conf;\u0026quot; upstream app { server localhost:{MAIN_PORT}; # upstream is running in same container } server { listen ${NGINX_PORT}; # NGINX_PORT will be filled later location / { proxy_pass http://app; # we defined upstream \u0026quot;app\u0026quot; above } } Install Nginx in Dockerfile\n# Dockerfile.web ARG PYTHON_VERSION FROM python:${PYTHON_VERSION}-slim USER root RUN apt-get -y update \\ \u0026amp;\u0026amp; apt-get -y install nginx \\ \u0026amp;\u0026amp; apt-get install gettext-base RUN pip install redis\\ gunicorn\\ uvicorn\\ fastapi RUN mkdir -p /opt COPY bin /opt/bin COPY src /opt/src COPY nginx.conf /opt/nginx.conf Run web app server in background and run Nginx\n# Run WAS in background gunicorn -k uvicorn.workers.UvicornWorker main:app --bind 0.0.0.0:${MAIN_PORT} -w ${MAIN_WORKER} --timeout ${MAIN_TIMEOUT} --daemon # Insert env-variables in .conf file and run nginx envsubst \u0026lt; /opt/nginx.conf \u0026gt; /etc/nginx/conf.d/default.conf \u0026amp;\u0026amp; nginx -g 'daemon off;' ","id":16,"section":"Engineering","summary":"Introduction Nginx : light-weight Web server\ngenerally used as HTTP Web Server, Reverse Proxy Server, Load Balancer Reverse Proxy\nProxy server : is a server that acts as an intermediary between a client and the server\nLoad balancing : Proxies distribute requests to a group of servers,\nEncryption : Proxies encrypt the data to keep it from being read during transfer.\nCaching : Proxies speed up access to information by storing the results of user requests in the server’s cache for a specified time","tags":null,"title":"System Design #1.1 | Nginx","uri":"https://koreanbear89.github.io/engineering/3.-system-design/system-design-#1.1-nginx/","year":"2021"},{"content":"\nHTTP request using ajax \u0026lt;script\u0026gt; $.ajax({url: \u0026quot;\u0026lt;HOST_URL\u0026gt;\u0026quot;, method : \u0026quot;GET\u0026quot;, datatype : \u0026quot;JSON\u0026quot;}) .done(function(data) { alert('Done');}) .fail(function(xhr, status, error){alert('Failed');}); }); \u0026lt;/script\u0026gt; ","id":17,"section":"Engineering","summary":"\nHTTP request using ajax \u0026lt;script\u0026gt; $.ajax({url: \u0026quot;\u0026lt;HOST_URL\u0026gt;\u0026quot;, method : \u0026quot;GET\u0026quot;, datatype : \u0026quot;JSON\u0026quot;}) .done(function(data) { alert('Done');}) .fail(function(xhr, status, error){alert('Failed');}); }); \u0026lt;/script\u0026gt; ","tags":null,"title":"HTML | Snippet","uri":"https://koreanbear89.github.io/engineering/2.-languages/html-snippet/","year":"2021"},{"content":"\nIntroduction Interface\npyplot interface : Rely on pyplot to automatically create and manage the figures and axes, and use pyplot functions for plotting. (MATLAB-like interface)\nobject-oriented interface : Explicitly create figures and axes, and call methods on them\nimport matplotlib.pyplot as plt import numpy as np # pyplot interface (MATLAB-like, rely on pyplot) x = np.linspace(0,2,100) plt.plot(x,x**2) # Recommended) object-oriented interface (create figures and axes manually) fig, ax = plt.subplots(figsize=(15,4)) ax.plot(x,x**2) 1. Plot fig, ax = plt.subplots() ax.plot(x,x**2) 2. Axes fig, ax = plt.subplots() # set axes ax.set_title(\u0026quot;Title\u0026quot;) ax.set_xlabel('xlabel') # set limits ax.set_ylim([0, 1e5]) # rotate tick ax.tick_params(axis = 'x', labelrotation =90) ","id":18,"section":"Engineering","summary":"Introduction Interface\npyplot interface : Rely on pyplot to automatically create and manage the figures and axes, and use pyplot functions for plotting. (MATLAB-like interface)\nobject-oriented interface : Explicitly create figures and axes, and call methods on them\nimport matplotlib.pyplot as plt import numpy as np # pyplot interface (MATLAB-like, rely on pyplot) x = np.linspace(0,2,100) plt.plot(x,x**2) # Recommended) object-oriented interface (create figures and axes manually) fig, ax = plt.","tags":null,"title":"Python | Matplotlib","uri":"https://koreanbear89.github.io/engineering/2.-languages/python-matplotlib/","year":"2021"},{"content":"\n1. Introduction 1.1 Terminologies\nJob : A piece of code that reads some input (HDFS or local), performs some computation and writes output.\nStages : Jobs are divided into stages. Stages are classified as a Map or reduce stages. Stages are divided based on computational boundaries, all computations cannot be Updated in a single Stage. It happens over many stages.\nTasks : Each stage has some tasks. One task is executed on one partition of data on one executor (machine).\nDAG : DAG stands for Directed Acyclic Graph, in the present context its a DAG of operators.\nExecutor : The process responsible for executing a task.\nMaster : The machine on which the Driver program runs\nSlave : The machine on which the Executor program runs\n1.2 Spark Components\nSpark Driver\nseparate process to execute user applications\ncreates SparkContext to schedule jobs execution and negotiate with cluster manager\nExecutors\nrun tasks scheduled by driver\nstore computation results in memory, on disk or off-heap\ninteract with storage systems\nCluster Manager\nApache Mesos : a general cluster manager that can also run Hadoop MapReduce and service applications.\nHadoop YARN : the resource manager in Hadoop.\nSpark Standalone : a simple cluster manager included with Spark that makes it easy to set up\n1.3 How Spark Works?\nSpark has a small code base and the system is divided in various layer.\nEach layers has some responsibilities. The layers are independent of each other\nInterpreter : The first layer is the interpreter, Spark uses a Scala interpreter, with some modifications. As you enter your code in spark console (creating RDD’s and applying operators), Spark creates a operator graph.\nDAG Scheduler : When the user runs an action (like collect), the Graph is submitted to a DAG Scheduler. The DAG scheduler divides operator graph into (map and reduce) stages. A stage is comprised of tasks based on partitions of the input data. The DAG scheduler pipelines operators together to optimize the graph. For e.g. Many map operators can be scheduled in a single stage. This optimization is key to Sparks performance. The final result of a DAG scheduler is a set of stages.\nTask Schedular : The stages are passed on to the Task Scheduler. The task scheduler launches tasks via cluster manager. (Spark Standalone/Yarn/Mesos). The task scheduler doesn’t know about dependencies among stages.\n2. Install Spark Locally 2.1 Prerequisite\nCheck java version first\nThen, just install miniconda3 and create virtual environment based on python 3.6\njava -version # The command above might show something like below \u0026gt;\u0026gt; openjdk version \u0026quot;1.8.0_262\u0026quot; \u0026gt;\u0026gt; OpenJDK Runtime Environment (build 1.8.0_262-b10) \u0026gt;\u0026gt; OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode) 2.2 Download Spark\n# Download spark wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz # unzip tar -xvzf spark-3.0.1-bin-hadoop2.7.tgz # move to home and rename mv spark-3.0.1-bin-hadoop2.7 ~/spark 2.3 Install pyspark\npip install pyspark 2.4 Change the execution path for pyspark\nexport SPARK_HOME=\u0026quot;/your_home_directory/spark/\u0026quot; export PATH=\u0026quot;$SPARK_HOME/bin:$PATH\u0026quot; 2.5 Test\n$ pyspark 2.6 PySpark in Jupyter\n# add below lines in .bashrc export PYSPARK_DRIVER_PYTHON=jupyter export PYSPARK_DRIVER_PYTHON_OPTS='notebook' # And run $ pyspark \u0026gt;\u0026gt; will give you a url for jupyter notebook 3. Cheat Sheet 3.1 Initialization \u0026amp; Configuration SparkContext : provides connection to Spark with the ability to create RDDs\nSQLContext : procides connection to Spark with the ability to run SQL queries on data\nSparkSession : all encompassing context which includes coverage for SparkContext, SQLContext and HiveContext\nimport pyspark from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql import SQLContext # create a SparkSession instance with the name 'appname' spark = SparkSession.builder.appName(\u0026quot;appname\u0026quot;).getOrCreate() # create a SparkContext instance which allows the Spark Application to access sc = SparkContext.getOrCreate() # create a SQLContext instance to access the SQL query engine built on top of Spark sqlContext = SQLContext(spark) 3.2 Dataframe Explore Dataframe # Prints out the schema in the tree format. df.printSchema() # To get the number of rows df.count() Modifying Dataframe # Create a column df = df.withColumn('column_new', F.lit('abc')) # with the defalut value 'abc' df = df.withColumn('column_new', 2*F.col('exisisting column')) # using an existing column # Change Column Name df.withColumnRenamed(\u0026quot;column_ori\u0026quot;, \u0026quot;column_new\u0026quot;) # add index column from pyspark.sql.functions import monotonically_increasing_id df_index = df.select(\u0026quot;*\u0026quot;).withColumn(\u0026quot;id\u0026quot;, monotonically_increasing_id()) # filter (=where) df.filter(df.column_float.between(7.5,8.2)) # remove column df.drop(\u0026quot;column_drop\u0026quot;) # grouped data : GroupBy allows you to group rows together based off some column value df.groupBy(\u0026quot;column_name\u0026quot;). .count() # Returns the count of rows for each group. .mean() # Returns the mean of values for each group. .max() # Returns the maximum of values for each group. .min() # Returns the minimum of values for each group. .sum() # Returns the total for values for each group. .avg() # Returns the average for values for each group. .agg() # Using agg() function, we can calculate more than one aggregate at a time. .pivot() # This function is used to Pivot the DataFrame Read and Write # create a SparkSession instance with the name 'appname' spark = SparkSession.builder.appName(\u0026quot;appname\u0026quot;).getOrCreate() # read (csv, json, text, parquet) df = spark.read.csv('PATH_CSV') # write df.coalesce(1).write.csv(\u0026quot;sample.csv\u0026quot;) 3.3 Join Inner join : essentially removes anything that is not common in both tables. It returns all data that has a match under the join condition(on expression is true) from both sides of the table Outer join : allows us to include in the result rows of one table for which there are no matching rows round in another table Left join : all rows of the left table remain unchanged, regardless of wheter there is a match in the right table or not. When a id match is found in the right table, it will be returned or null otherwise Right join : performs the same task as the left join, but for the right table. df1.join(df2, on='column', how='inner') 3.4 SQL functions from pyspark.sql import functions as F # Aggregate function: returns a set of objects with duplicate elements eliminated. F.collect_set() # Bucketize rows into one or more time windows given a timestamp specifying column. F.window(timeColumn='timestamp', windowDuration='20 minute') # Creates a Column of literal value. F.lit() # Replace all substrings of the specified string value that match regexp with rep. F.regexp_replace() 3.5 User Defined Function input arguments for udf should be column name as you can see in line no.3 or you might get an TypeError: Invalid argument, not a string or column: 2 of type \u0026lt;class ‘int’\u0026gt;. For column literals, use ‘lit’, ‘array’, ‘struct’ or ‘create_map’ function\n# User Defined Functions def func(x): return x test_udf = F.udf(lambda x : func(x), ArrayType(StringType())) df_udf = df.withColumn('udf', test_udf('input_column_name')) 3.6 Others $\u0026quot;colname\u0026quot; is converted to Column by SQLContext.implicits$.StringToColumn # zeppelin plot def show(p): import io img = io.StringIO() p.savefig(img, format='svg') img.seek(0) print(\u0026quot;%html \u0026lt;div style='width:400px'\u0026gt;\u0026quot; + img.getvalue() + \u0026quot;\u0026lt;/div\u0026gt;\u0026quot;) References PySpark Cheat Sheet, TowardsDataScience\nIntroduction to PySpark join types\n","id":19,"section":"Engineering","summary":"1. Introduction 1.1 Terminologies\nJob : A piece of code that reads some input (HDFS or local), performs some computation and writes output.\nStages : Jobs are divided into stages. Stages are classified as a Map or reduce stages. Stages are divided based on computational boundaries, all computations cannot be Updated in a single Stage. It happens over many stages.\nTasks : Each stage has some tasks. One task is executed on one partition of data on one executor (machine).","tags":null,"title":"CheatSheet | Spark","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-spark/","year":"2020"},{"content":"\nIntroduction Contents\nInstall docker in centos\nFrequently used docker commands\nFIle sharing in docker (bind, volume)\nDockerfile instruction\nDocker compose\nDocker swarm\nGlossary\nswarm : almost same with word \u0026ldquo;cluster\u0026rdquo; node (manager/worker) : A unit of server in a cluster. You can run swarm commands only on the manager node. service : A unit of modules in project, a basic distribution unit, stack : You can think of it as a unit of a project, and containers grouped into one stack basically belong to the same overlay network. 1. Install docker in centOS Install Docker Engine on CentOS | Docker Documentation # (1) Set up the repository $ sudo yum install -y yum-utils $ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # (2) Install Docker Engine $ sudo yum install docker-ce docker-ce-cli containerd.io Post-installation steps for Linux | Docker Documentation # (3) Create the docker group. # $ sudo groupadd docker # (4)Add your user to the docker group. # sudo usermod -aG docker irteam $ sudo /usr/sbin/usermod -aG docker irteam $ sudo /usr/sbin/usermod -aG docker irteamsu # (5) change root directory (storage for default docker directory is not enough) # add {\u0026quot;data-root\u0026quot;: \u0026quot;/home1/irteam/docker-data\u0026quot;} in /etc/docker/daemon.json $ sudo vim /etc/docker/daemon.json # (6) Run docker $ sudo systemctl start docker # if got permission error for /var/run/docker.sock $ sudo chmod 666 /var/run/docker.sock # check root directory $ docker info | grep Root $ docker run hello-world 2. Frequently used Docker Commands Dockerize an SSH service $ docker --version # check docker version $ docker build -t [TAG] . # build using Dockerfile in cwd $ docker images # show docker images $ docker ps -a # show docker containers $ docker rm -f [CONTAINER_NAME] # remove docker container $ docker rmi [IMAGE_NAME] # remove docker image $ docker run --dit --rm -p 22:22 --name [CONTAINER_NAME] -v [SRC]:[DST] [IMAGE_NAME] # run docker container $ ctrl p q # exit without removing container $ docker attach [CONTAINER_NAME]] # attach to docker container $ docker exec -u 0 -it [CONTAINER_NAME] bash # exec on root $ docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] $ docker push [REPOSITORY[:TAG]] $ docker container prune # prune docker container $ docker image prune # prune docker images $ docker system prune -a # remove all building cache $ docker logs -f \u0026lt;CONTAINER_NAME\u0026gt; --tail=1000 2\u0026gt;\u0026amp;1 | grep complete # show logs in container 3. File sharing in docker container Bind-mount : are files mounted from your host machine (the one that runs your docker daemon) onto your container.\nVolume : are like storage spaces totally managed by docker. In fact, volumes are managed in the hidden(?) path of host machine such as \u0026lsquo;/var/lib/docker/volumes/VOLUME_NAME\u0026rsquo;\nnamed volumes : you provide the name of it\nanonymous volumes : usual UUID names from docker, like you can find them on container or untagged images\n$ docker volume ls $ docker volume rm $ docker volume inspect VOLUME_NAME 4. Dockerfile Instructions # Set base image FROM ubuntu:16.04 # argument used only in build time ARG PYVERSION # run shell cmd using \u0026quot;bin/sh -c\u0026quot; in docker image RUN [\u0026quot;apt-get\u0026quot;, \u0026quot;install\u0026quot;, \u0026quot;-y\u0026quot;, \u0026quot;nginx\u0026quot;] # set expose port EXPOSE 8080 # set env-var, env-var can be used as $variable_name ENV FOO /bar # set user of docker image USER nginx # volume mount from host to docker container VOLUME [\u0026quot;opt/project\u0026quot;] # copy files from host to docker image ADD file /some/dir/file # almost same with ADD, do not unzip zipped files automatically, cannot use URL as source of file COPY file /some/dir/file # cmd to run when docker container starts CMD [\u0026quot;python\u0026quot;, \u0026quot;main.py\u0026quot;] 5. Docker Compose $ docker-compose up -d $ docker-compose down $ docker-compose stop [CONTAINER_NAME] $ docker-compose ps version: '3.7' services: my_service1: build: # if wanna build a image context: ./ dockerfile: ./Dockerfile image: # image name hostname: # host name tty: true # docker run -t container_name: \u0026lt;my_cont1\u0026gt; # container_name volumes: # mout volumes - ./src:/myproject/src networks: - myproject ports: - 2003:3003 user: celery command: python -m black /myproject/src -t py39 depends_on: - black volumes: rabbitmq: driver: local redis: driver: local networks: myproject: 6. Docker swarm Server Orchestration\nScheduling : Distribute multiple containers to each server, and when the server dies, it is deployed to another server so that there is no disruption to the service. Clustering : Multiple servers can be used like one server. By adding/removing new servers to the cluster, scattered containers can communicate easily as if they were on the same server using a virtual network. Service Discovery Logging, Monitoring Why Docker Swarm?\nWhen you build an API server and traffic increases =\u0026gt; one server cannot handle it,\nWhat if the images constituting the container are updated. Should I delete all currently running containers and create a new container again with docker-compose =\u0026gt; Rolling update of Docker Swarm\nSwarm was developed separately from Docker, and since v1.12, it was merged under the name of Swarm Mode.\n# (1) init docker swarm $ docker swarm init # run on manager node # This will return the following command. To add a worker to this swarm, just run that command on the worker node \u0026gt; docker swarm join --token ............. # (2) deploy $ docker stack deploy --compose-file \u0026lt;docker-compose.yml\u0026gt; \u0026lt;STACK_NAME\u0026gt; # deploy using docker-compse.yml # (3) manage the swarm node $ docker node ls # show all nodes joined to current node $ docker stack ls # show all stacks in current node (manager) $ docker service ls # show all services (including worker nodes) managed by current node $ docker service ps \u0026lt;SERVICE_NAME\u0026gt; ","id":20,"section":"Engineering","summary":"Introduction Contents\nInstall docker in centos\nFrequently used docker commands\nFIle sharing in docker (bind, volume)\nDockerfile instruction\nDocker compose\nDocker swarm\nGlossary\nswarm : almost same with word \u0026ldquo;cluster\u0026rdquo; node (manager/worker) : A unit of server in a cluster. You can run swarm commands only on the manager node. service : A unit of modules in project, a basic distribution unit, stack : You can think of it as a unit of a project, and containers grouped into one stack basically belong to the same overlay network.","tags":null,"title":"CheatSheet | Docker","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-docker/","year":"2020"},{"content":"\nIntroduction Pull Request Summary\nFork repo to your own repo\nclone, set remote\ngit remote add \u0026lt;remote_name\u0026gt; \u0026lt;URL\u0026gt; Generate branch\ngit checkout -b \u0026lt;feature/issue_number\u0026gt; Add, Commit, Push\ngit push \u0026lt;remote_name\u0026gt; \u0026lt;branch_name\u0026gt; Pull Request\nCode Review\nMerge\nSquash and Merge : squash multiple commits into a new commit\nRebase and Merge : multiple commits will be merged\n1. Setup Git Environment # (1) create new git $ git init # in the working dir, generates .git directory $ git clone \u0026lt;URL\u0026gt; # cloning repo registers remote named origin automatically $ git clone --recursive \u0026lt;URL\u0026gt; # recursive clone including submodule $ git clone -b \u0026lt;BRANCH\u0026gt; \u0026lt;URL\u0026gt; # without \u0026lt;BRANCH\u0026gt;, clone master branch # (2) Add remote git $ git remote # show remotes for current project $ git remote add \u0026lt;remote_name\u0026gt; \u0026lt;URL\u0026gt; # (3) git user check $ git config --list $ git config --global --list # (4) Add user $ git config user.name \u0026quot;Your Name\u0026quot; # only for this project $ git config user.email you@example.com $ git config --global user.name \u0026quot;Your Name\u0026quot; # for global usage $ git config --global user.email you@example.com 2. Changes in source code : Add, Commit, Push # (1) add $ git add * # add changes to index $ git status # Check git status (staged/unstaged) $ git reset HEAD \u0026lt;FILE\u0026gt; # cancle add, unclearstage file. unstage all w/o \u0026lt;FILE # (2) commit $ git commit -m \u0026quot;explain this commit\u0026quot; # commit $ git log # show commit log $ git commit --amend -m \u0026quot;modify commit message\u0026quot; # (3) push $ git push \u0026lt;remote_name\u0026gt; \u0026lt;branch_name\u0026gt; # changes from local to server # (4) pull $ git pull # download and merge $ git fetch # only download, not merge 3. Branch # (0) Show all branches $ git branch # (1) create new local branch $ git branch \u0026lt;BRANCH\u0026gt; # (2) change to the new branch $ git checkout \u0026lt;BRANCH\u0026gt; # (3) Delete branch # (3.1) Delete local branch $ git branch -d \u0026lt;BRANCH\u0026gt; # (3.2) Delete remote branch $ git push origin --delete \u0026lt;BRANCH\u0026gt; 4. gitignore File # ignore every 'file.ext' in project file.ext # You cannot write annotation in define ignore-rule line (like below) file.ext # not a comment # ends with / will ignore directory only bin/ # the line below will ignore both file named bin and directory named bin bin # You can use Glob pattern. e.g the line below will ignore build/ and Build [bB]uild/ # You can ignore files with certain extension *.apk # exception from ignore list with ! !.gitignore [Reference] https://rogerdudler.github.io/git-guide/index.ko.html)\n","id":21,"section":"Engineering","summary":"Introduction Pull Request Summary\nFork repo to your own repo\nclone, set remote\ngit remote add \u0026lt;remote_name\u0026gt; \u0026lt;URL\u0026gt; Generate branch\ngit checkout -b \u0026lt;feature/issue_number\u0026gt; Add, Commit, Push\ngit push \u0026lt;remote_name\u0026gt; \u0026lt;branch_name\u0026gt; Pull Request\nCode Review\nMerge\nSquash and Merge : squash multiple commits into a new commit\nRebase and Merge : multiple commits will be merged\n1. Setup Git Environment # (1) create new git $ git init # in the working dir, generates .","tags":null,"title":"CheatSheet | Git","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-git/","year":"2020"},{"content":"\n1. HDFS $ hdfs dfs -ls $ hdfs dfs -put [LOCAL_FILE_NAME] [DEST] $ hdfs dfs -get [FILE_NAME or FOLDER_PATH] $ hdfs dfs -rm [-f] [-r|-R] [-skipTrash] URI [URI ...] ","id":22,"section":"Engineering","summary":"\n1. HDFS $ hdfs dfs -ls $ hdfs dfs -put [LOCAL_FILE_NAME] [DEST] $ hdfs dfs -get [FILE_NAME or FOLDER_PATH] $ hdfs dfs -rm [-f] [-r|-R] [-skipTrash] URI [URI ...] ","tags":null,"title":"CheatSheet | HDFS","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-hdfs/","year":"2020"},{"content":"\n1. Setup vim Install Vundle\ngit clone [GitHub - VundleVim/Vundle.vim: Vundle, the plug-in manager for Vim](https://github.com/VundleVim/Vundle.vim.git) ~/.vim/bundle/Vundle.vim color scheme setup\nJellybean color scheme official install\nAnd simply add line color jellybean in .vimrc\nmkdir -p ~/.vim/colors cd ~/.vim/colors curl -O https://raw.githubusercontent.com/nanotech/jellybeans.vim/master/colors/jellybeans.vim write .vimrc\nwget https://raw.githubusercontent.com/jjeaby/jscript/master/.vimrc\nAdded Plugin 'preservim/nerdcommenter'\ninstall plugins in vimrc\n:PluginInstall Shell\n# install Vundle git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim # setup jellybean color scheme mkdir -p ~/.vim/colors cd ~/.vim/colors curl -O https://raw.githubusercontent.com/nanotech/jellybeans.vim/master/colors/jellybeans.vim # copy .vimrc cp ~/configuration/vim/.vimrc ~/.vimrc vim -c 'PluginInstall' -c 'qa!' Reference\nVim as IDE 2. shortcuts Basic commands - a # write text to the cursor - A # write text at the end of the current line - i # write text to the cursor - I # write text to the front of the current line - ^ # move cursor to the front of the line - $ # move cursor to the back of the line - ESC # shift to command mode - x # in command mode, remove the text - u # in command mode, undo - :w # in command mode, save (write on the disk) - :q # in command mode, quit - y # copy in visual mode - p # paste in visual mode Commands for multiple windows - :split # split in horizontal - :vsplit # split in vertical - ctrl + w + w # move to next window - ctrl + w + p # move to previous window Fold and Expand - :set foldmethod=indent - zi # toggle folding Indent \u0026amp; Un-indent # insert mode - ctrl + d - ctrl + t # visual mode : if 1 or more lines are selected - \u0026lt; - \u0026gt; 3. Plugins NerdTree :nerd + tab # turn on nerdtree ctrl+w+w # move focus between editor and file tree m + a # add a child file ","id":23,"section":"Engineering","summary":"1. Setup vim Install Vundle\ngit clone [GitHub - VundleVim/Vundle.vim: Vundle, the plug-in manager for Vim](https://github.com/VundleVim/Vundle.vim.git) ~/.vim/bundle/Vundle.vim color scheme setup\nJellybean color scheme official install\nAnd simply add line color jellybean in .vimrc\nmkdir -p ~/.vim/colors cd ~/.vim/colors curl -O https://raw.githubusercontent.com/nanotech/jellybeans.vim/master/colors/jellybeans.vim write .vimrc\nwget https://raw.githubusercontent.com/jjeaby/jscript/master/.vimrc\nAdded Plugin 'preservim/nerdcommenter'\ninstall plugins in vimrc\n:PluginInstall Shell\n# install Vundle git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim # setup jellybean color scheme mkdir -p ~/.vim/colors cd ~/.","tags":null,"title":"Cheat Sheet | VIM","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-vim/","year":"2020"},{"content":"\n5.1 Singular Value decomposition (SVD) singular value decomposition (SVD) : is a factorization of a real or complex matrix that generalizes the eigendecomposition (EVD) of a square normal matrix to any $m \\times n$ matrix via an extension of the polar decomposition.\n$$ A = U \\Sigma V^T$$ $A \\in \\mathbb{R}^{m \\times n}$ : A given rectangular matrix $U \\in \\mathbb{R}^{m\\times m} $ : matrices with orthonormal columns, providing an orthonormal basis of $Col(A)$ $V \\in \\mathbb{R}^{n \\times n}$ : matrices with orthonormal columns, providing an orthonormal basis of $Row(A)$ $\\Sigma \\in \\mathbb{R}^{m \\times n}$ : a diagonal matrix whose entries are in a decreasing order, i.e., $\\sigma_1 \\geq \\sigma_2 \\geq \u0026hellip; \\geq \\sigma_{min(m,n)}$ 5.2 SVD as Sum of Outer Products SVD as Sum of Outer Products : $A$ can also be represented as the sum of outer products $$ A = U \\Sigma V^T = \\Sigma_{i=1}^n \\sigma_i \\mathbf{u_j v_i^T}$$\nFrom now on, we just want to show the equation below is true in this chapter (5.2) for the next chapter (5.3)\n$$AV = U\\Sigma \\Longleftrightarrow A=U \\Sigma V^T$$\nAnother Perspective of SVD : We can easily find two orthonormal basis sets, {$u_1, \u0026hellip; , u_n$} for $Col(A)$ and {$v_1, \u0026hellip; v_n$} for $Row (A)$, by using, Gram-Schmidt orthogonalization.\nAre these unique orthonormal basis sets? No, then can we jointly find them such that\n$$ A \\mathbf{v_i} = \\sigma_i \\mathbf{u_i}$$\nLet us denote $ U = \\begin{bmatrix} \\mathbf{u_1 \\ u_2 \\ \u0026hellip; \\ u_m} \\end{bmatrix} \\in \\mathbb{R}^{m\\times n}$, $ V = \\begin{bmatrix} \\mathbf{v_1 \\ v_2 \\ \u0026hellip; \\ v_n} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}$ and $ \\Sigma = diag(\\sigma_n) \\in \\mathbb{R}^{n \\times n}$\n$AV = U\\Sigma \\Longleftrightarrow \\begin{bmatrix} A\\mathbf{v_1} \\ A\\mathbf{v_2} \\ \u0026hellip; \\ A\\mathbf{v_n} \\end{bmatrix} = \\begin{bmatrix} \\ \\sigma_1 \\mathbf{u_1} \\ \\sigma_2 \\mathbf{u_2} \\ \u0026hellip; \\ \\sigma_n \\mathbf{u_n} \\end{bmatrix}$\n$V^{-1} = V^T $ since $\\mathbb{R}^{n \\times n}$ has orthonormal columns.\nThus $AV = U\\Sigma \\Longleftrightarrow A=U \\Sigma V^T$\n5.3 Geometrical Interpretation of SVD Geometrical Interpretation : For every linear transformation (rectangular matrix) $A$ , one can find $n$ dimensional orthonormal basis of $V$ and transformed orthonormal basis $U$ $$A = U \\Sigma V^T \\Longleftrightarrow AV = U\\Sigma$$\nFurthermore : The figure below shows the orthogonal vector set $x, y$ and the linearly transformed one $Ax, Ay$.\nAnd you can see two geometrical features :\nThere could be one or more sets of orthogonal {$Ax, Ay$}. After transformed by matrix $A$, the lengths of $x,y$ are scaled by scaling factor. It is called singular value (like eigenvalue at EVD) and represented as $\\sigma_1, \\sigma_2, \\sigma_3, \u0026hellip;$ starting with the larger value. Figure. the orthogonal vector set $x, y$ and the linearly transformed one $Ax, Ay$. [reference] 5.4 Computing SVD First, we form $AA^T$ , $A^T A$ and then get SVD of each as followings : $$ AA^T = U \\Sigma V^T V \\Sigma^T U^T = U \\Sigma \\Sigma^T U^T = U \\Sigma^2 U^T$$\n$$ A^T A = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^T \\Sigma U^T = V \\Sigma^2 V^T$$\nFrom the above, you can see the form of result equation is very similar to EVD: $U$ Is a orthogonal matrix which is from the eigen-decomposition of $AA^T$, and it is called left singular vector. $\\Sigma$ is a diagonal matrix whose diagonal entries are equal to the square root of the eigenvalues from $AA^T$. $V$ is a orthogonal matrix which is from the eigen-decomposition of $A^TA$, and it is called right singular vector. 5.5 Application of SVD Recall SVD as Sum of Outer Products : matrix $A$ can also be represented as the sum of outer products $$ A = U \\Sigma V^T = \\Sigma_{i=1}^n \\sigma_i \\mathbf{u_j v_i^T} = \\sigma_1 \\mathbf{u_1 v_1^T} + \\sigma_2 \\mathbf{u_2 v_2^T} + \\sigma_3 \\mathbf{u_3 v_3^T} \u0026hellip;$$\nHere, $\\mathbf{u_j v_i^T} $ is $m \\times n$ matrix and we can see matrix $A$ as a sum of multiple layers via SVD.\nThat is, we can reconstruct $A$ with only a few singular values rather than using all of them ($\\Sigma$). And this can be used for dimensionality reduction like task such as image compression.\nFigure. Partial reconstruction of matrix using SVD [reference] Figure. image compression using SVD (original, p=20, p=50, p=100) [reference] ","id":24,"section":"Mathematics","summary":"5.1 Singular Value decomposition (SVD) singular value decomposition (SVD) : is a factorization of a real or complex matrix that generalizes the eigendecomposition (EVD) of a square normal matrix to any $m \\times n$ matrix via an extension of the polar decomposition. $$ A = U \\Sigma V^T$$ $A \\in \\mathbb{R}^{m \\times n}$ : A given rectangular matrix $U \\in \\mathbb{R}^{m\\times m} $ : matrices with orthonormal columns, providing an","tags":null,"title":"Linear Algebra for ML #5 | Singular Value Decomposition","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec5/","year":"2019"},{"content":"\n4.0 Introduction Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource.\nThe above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$\n$D$ is a diagonal matrix with eigenvalues in diagonal entries\n$V$ is a matrix whose column vectors are eigenvectors\n$A$ is a given square matrix $A \\in \\mathbb{R}^{n \\times n}$\nConsider the linear transformation $T(\\mathbf{x}) = A \\mathbf{x} = VDV^{-1} \\mathbf{x} $, it can be seen to be a series of following transformation. (4.5.1+ 4.5.2 + 4.5.3)\nChange of Basis\nElement-wise Scaling\nBack to Original Basis\n4.1 Eigenvectors and Eigenvalues An eigenvector of a square matrix $A \\in \\mathbb{R}^{n \\times n}$ is a nonzero vector $\\mathbf{x} \\in \\mathbb{R}^n$ such that $A \\mathbf{x} = \\lambda \\mathbf{x}$ for some scalar $\\lambda$. In this case $\\mathbf{\\lambda}$ is called an eigenvalue of $A$, and such an $\\mathbf{x}$ is called an eigenvector.\n$A \\mathbf{x}$ can be considered as a linear transformation $T( \\mathbf{x})$. If $\\mathbf{x} $ is an eigenvector, then $T(x) = A \\mathbf{x} = \\lambda \\mathbf{x} $, which means the output vector has the same direction as $\\mathbf{x}$, but the length is scaled by a factor of $\\lambda$ Fig1. Example of eigenvector and eigenvalue And here, $8 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $ is faster than $ \\begin{bmatrix} 2\\ 6 \\\\ 5 \\ 3 \\end{bmatrix}\\begin{bmatrix} 1\\ 1 \\end{bmatrix} $\nThe equation $A \\mathbf{x} = \\lambda \\mathbf{x}$ can be re-written as $$(A-\\lambda I)\\mathbf{x}=\\mathbf{0} $$\n$\\lambda$ is an eigenvalue of an $n \\times n$ matrix $A$ if and only if this equation has a nontrivial solution. (since $\\mathbf{x}$ should be a nonzero vector).\nThe set of all solutions of the above equation is the null space of the matrix $A-\\lambda I$, which we call the eigenspace of A corresponding to $\\lambda$\nThe eigenspace consists of the zero vector and all the eigenvectors corresponding to $\\lambda$, satisfying the above equation.\n4.2 Null Space Null Space : The null space of a matrix $A \\in \\mathbb{R}^{m\\times n}$ is the set of all solutions of a homogeneous linear system, $A \\mathbf{x = 0}$ For $A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \u0026hellip; \\\\ \\mathbf{a_m^T} \\end{bmatrix}$, $\\mathbf{x}$ should satify the following $ \\mathbf{ a_1^Tx =a_2^Tx= \u0026hellip; =a_m^Tx }=0$, That is, $\\mathbf{x} $ should be orthogonal to every row vector in $A$ Orthogonal Complement : The set of all vectors $\\mathbf{z}$ that are orthogonal to $W$ is called the orthogonal complement of $W$ and is denoted by $W ^ \\perp$. 4.3 Characteristic equation $$(A - \\lambda I) \\mathbf{x = 0}$$\nThe set of all solutions of the above equation = null space of the matrix $(A - \\lambda I)$ = eigenspace of $A$ corresponding to $\\lambda$\nThe eigensapce consists of the zero vector and all the engienvectors corresponding to $\\lambda$\nHow can we find the eigenvalues?\nIf $(A-\\lambda I)\\mathbf{x = 0}$ has a nontrivial solution, then the columns of $(A - \\lambda I)$ should be noninvertible.\nIf it is invertible, $\\mathbf{x}$ cannot be a nonzero vector since\n$$ (A - \\lambda I)^{-1} (A - \\lambda I) \\mathbf{x} = (A - \\lambda I)^{-1} \\mathbf{0} \\longrightarrow \\mathbf{x = 0} $$\nThus, we can obtain eignevalues by solving characteristic equation : $$ det(A - \\lambda I) = 0$$\nAlso, the solution is not unique, and thus $A-\\lambda I$ has linearly dependent columns.\nOnce obtaining eigenvalues, we compute the eigenvectors for each $\\lambda$ by solving : $$ (A-\\lambda I) \\mathbf{x = 0}$$\nEigenspace : Note that the dimension of the eigenspace (corresponding to a particular $\\lambda$) can be more than one. In this case, any vector in the eigenspace satisfies\n$$ T\\mathbf{(x)} = A \\mathbf{(x)} = \\lambda \\mathbf{(x)}$$\nIn summary, we can find all the possible eigenvalues and eignevectors, as follows.\nFirst, find all the eigenvalue by solving the characteristic equation : $ det(A-\\lambda I) = 0$\nSecond, for each eigenvalue $\\lambda$, solve for $(A-\\lambda I) \\mathbf{x=0}$ and obtain the set of basis vectors of the corresponding eigenspace.\n4.4 Diagonalization Diagonal matrix : matrix in which the entries outside the main diagonal are all zero\nDiagonalization : We want to change a given square matrix $A \\in \\mathbb{R}^{n\\times n}$ into a diagonal matrix via the following form : $D = V^{-1} A V $\nFor $A$ to be diagonalizable, an invertible $V$ should exist such that $V^{-1}AV$\nHow can we find an invertible $V$ and the resulting diagonal matrix $D$\n$V = [\\mathbf{v_1, v_2 , \u0026hellip; , v_n}]$ where $\\mathbb{v_i}$ are column vectors of $V$\n$D = diag[\\lambda_1, \\lambda_2 , \u0026hellip; , \\lambda_n ]$\n$$ D = V^{-1}AV $$\n$$ VD = AV$$\n$$AV = A[ \\mathbf{v_1, v_2 , \u0026hellip; v_n }] = [A\\mathbf{v_1}, A\\mathbf{v_2},\u0026hellip;, A\\mathbf{v_n}], (\\text{column vector matmul})$$\n$$VD = [\\lambda \\mathbf{v_1},\\lambda \\mathbf{v_2}, \u0026hellip; ,\\lambda \\mathbf{v_n}]$$\n$$AV=VD \\longleftrightarrow [A\\mathbf{v_1}, A\\mathbf{v_2},\u0026hellip;, A\\mathbf{v_n}] = [\\lambda \\mathbf{v_1},\\lambda \\mathbf{v_2}, \u0026hellip; ,\\lambda \\mathbf{v_n}]$$\nFrom above, we obtain\n$$A \\mathbf{v_1} = \\lambda \\mathbf{v_1}, A \\mathbf{v_2} = \\lambda \\mathbf{v_2}, \u0026hellip; , A \\mathbf{v_n} = \\lambda \\mathbf{v_n}$$\nThus $\\mathbf{v_1, v_2, \u0026hellip; v_n} $ should be eigenvectors and $\\lambda_1, \\lambda_2, \u0026hellip;. \\lambda_n$ should be eigenvalues. Then, For $VD = AV \\longrightarrow D = V^{-1}AV$ to be true, $V$ should invertible.\nIn this case, the resulting diagonal matrix D has eigenvalues as diagonal entries\nDiagonalizable matrix : For $V$ to be invertible, $V$ should be a square matrix in $\\mathbb{R}^{n \\times n}$, and $V$ should have $n$ linearly independent columns : Hence, $A$ should have $n$ linearly independent eigenvectors. 4.5 Eigen-Decomposition and Linear Transfomation Eigendecomposition : If $A$ is diagonalizable, we can write $A = V^{-1}DV$, and we can also write $A = VDV^{-1}$, which we call eigendecomposition of $A$. So, $A$ being diagonalizable is equilvalent to $A$ having Eigendecomposition.\nSuppose $A$ is diagonalizable, thus having eigendecomposition $ A = VDV^{-1}$, Consider the linear transformation $T(\\mathbf{x}) = A \\mathbf{x}$, it can be seen to be a series of following transformation. (4.5.1+ 4.5.2 + 4.5.3)\n$$T(\\mathbf{x}) = A\\mathbf{x} = VDV^{-1}\\mathbf{x} = V(D(V^{-1}\\mathbf{x}))$$\n4.5.1 Change of Basis\nLet $\\mathbf{y} = V^{-1}\\mathbf{x}$ then $V \\mathbf{y=x}$ $\\mathbf{y}$ is a new coordinate of $\\mathbf{x}$ with respect to a new basis of eigenvectors {$\\mathbf{v_1, v_2}$} https://angeloyeo.github.io/2020/12/07/change_of_basis.html 4.5.2 Element-wise and Dimension-Wise Scaling\n$T(\\mathbf{x}) = V(D(V^{-1}\\mathbf{x})) = V(D\\mathbf{y})$\nLet $ z=D\\mathbf{y}$. This computation is a simple Element-wise scaling of $\\mathbf{y}$\n4.5.3 Back to Original Basis\n$T(\\mathbf{x}) = V(D\\mathbf{y}) = Vz$\n$z$ is still a coordinate based on the new basis $\\mathbf{v_1, v_2}$\n$Vz$ converts $z$ to another coordinates based on the original standard basis.\nThat is, $Vz$ is a linear combination of $\\mathbf{v_1}$ and $\\mathbf{v2}$ using the coefficient vector $z$.\nThat is,\n$$Vz = \\begin{bmatrix} \\mathbf{v_1} \\ \\mathbf{v_2} \\end{bmatrix} \\begin{bmatrix} z_1 \\ z_2 \\end{bmatrix} = \\mathbf{v_1}z_1 + \\mathbf{v_2}z_2$$\nFig3. Eigendecomposition as a series of transformation Example : $A = VDV^{-1} = \\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} -1\u0026amp;0\\\\ 0\u0026amp;2 \\end{bmatrix}\\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix}^{-1}, \\text{ and } \\bf{x}=\\begin{bmatrix} 4\\\\ 3 \\end{bmatrix} $\n$$ y = \\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix}^{-1}\\begin{bmatrix} 4\\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} $$\n$$ z = \\begin{bmatrix} -1 \u0026amp; 0\\\\ 0 \u0026amp;2 \\end{bmatrix} \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -2\\\\ 2 \\end{bmatrix}$$\n$$ Vz = \\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix}\\begin{bmatrix} -2\\\\ 2 \\end{bmatrix} = \\begin{bmatrix} -10\\\\ 0 \\end{bmatrix} $$\n4.5.4 Linaer Transformation via $A^k$\nNow, consider recursive transformation $A \\times A \\times A \u0026hellip; \\times A \\mathbf{x} = A^k \\mathbf{x} $\nIf $A$ is diagonalizable, $A$ has Eigendecomposition\n$$ A = VDV^{-1}$$\n$$ A^k = (VDV^{-1})(VDV^{-1})(VDV^{-1})\u0026hellip;(VDV^{-1}) = (VD{^k}V^{-1})$$\nHere, $D^k$ is simply computed as $diag[\\lambda_1^k, \\lambda_2^k\u0026hellip; , \\lambda_n^k]$\nIt is much faster to compute $V(D^k(V^{-1}\\mathbf{x}))$ than to compute $A^k \\mathbf{x}$\nSummary Eigenvalue and Eigenvectors\nNull space, Column space, and orthogonal complement in $\\mathbb{R}^n$\nDiagonalization and Eigendecomposition\nLinear transforamtion via eigendecomposition\n","id":25,"section":"Mathematics","summary":"4.0 Introduction Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource. The above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$ $D$ is a diagonal matrix with eigenvalues in diagonal entries $V$ is a matrix whose column vectors","tags":null,"title":"Linear Algebra for ML #4 | Eigen Decomposition ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec4/","year":"2019"},{"content":"\n3.0 Least Square Inner Product : Given $ \\mathbf{u,v} \\in \\mathbb{R}^n$, we can consider $ \\mathbf{u,v} $ as $n \\times 1$ matrices. The number $\\mathbf{u^Tv}$ is called inner product or dot product, and it is written as $ \\mathbf{u \\cdot v} $.\nVector Norm : The length or magnitude of $\\mathbf{v}$, can be calculated as $ \\sqrt{ \\mathbf{v} \\cdot \\mathbf{v} }$\n$L_p$ Norm : $ \\Vert \\mathbf{x} \\Vert_p = ( \\Sigma_{i=1}^n \\vert x_i \\vert^p)^{1/p}$\nEuclidean Norm : $L_2$ norm\nFrobenius Norm : can be seen as an expansion of $L_2$ norm to apply to the matrix : $ \\Vert A \\Vert_F = \\sqrt{\\Sigma_{i=1}^m \\Sigma_{j=1}^n} \\vert a_{ij}\\vert^2$\nUnit Vector : A vector whose length is 1\nNormalizing : Given a nonzero vector $\\mathbf{v}$, if we divide it by its length, we obtain a unit vector.\nDistance between vectors : dist($\\mathbf{u,v}$) = norm($\\mathbf{u-v}$) = $\\Vert \\mathbf{u-v} \\Vert$\nInner Product and Angle Between Vectors : $\\mathbf{u \\cdot v} = \\mathbf{\\Vert u \\Vert \\Vert v \\Vert} cos \\theta $\nOrthogonal Vectors : $\\mathbf{u \\cdot v} = \\mathbf{\\Vert u \\Vert \\Vert v \\Vert} cos \\theta = 0$. That is $ (\\mathbf{u \\perp v})$\n3.1 Introduction to Least Squares Problem Over-Determined : number of equations \u0026gt; number of variables (we have much more data examples), usually no solution exists\nMotivation for Least Squares : Even if no solution exists, we want to approximately obtain the solution for an over-determined system.\nThe example below shows how to determine which solution is better between $\\begin{bmatrix} -0.12 \\\\ 16 \\\\ -9.5 \\end{bmatrix}$ and $\\begin{bmatrix} -0.4 \\\\ 20 \\\\ -20 \\end{bmatrix}$ for given Over-Determined System. Least Squares Problem : Given an over-determined system, $A \\mathbf{x \\simeq b}$, a least squares solution $\\hat{x}$ is defined as $$\\mathbf{\\hat{x}} = argmin_x | \\mathbf{b} - A \\mathbf{x} | $$\nThe most important aspect of the least-squares problem is that no matter what $\\mathbf{x}$ we select, the vector $A \\mathbf{x}$ will necessarily be in the column space Col $A$\nThus, we seek for $\\mathbf{x}$ that makes $A \\mathbf{x}$ as the closest point in Col $A$ to $\\mathbf{b}$\n3.2 Geometric Interpretation of Least squares Consider $ \\mathbf{\\hat{x}} $ such that $ \\hat{b} = A\\mathbf{\\hat{x}} $ is the closest point to $\\mathbf{b}$ among all points in Column Space of $A$.\nThat is, $\\mathbf{b}$ is closer to $\\mathbf{\\hat{b}}$ than to $A \\mathbf{x}$ for any other $\\mathbf{x}$.\nTo satisfy this, the vector $\\mathbf{b}- A\\mathbf{\\hat{x}}$ should be orthogonal to Col $A$\nThis means $\\mathbf{b} - A \\mathbf{\\hat{x}}$ should be orthogonal to any vector in Col A:\n$$ \\mathbf{b} - A\\mathbf{\\hat{x}} \\perp (x_1\\mathbf{a}_1 + \u0026hellip;. x_p\\mathbf{a}_n)$$\nOr equivalently, $$ A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = 0$$\n$\\mathbf{b} - A \\mathbf{\\hat{x}} \\perp \\mathbf{a_1} \\rightarrow \u0026gt;\\mathbf{a}_1^T(\\mathbf{b} - A \\mathbf{\\hat{x}})=0 $\n$\\mathbf{b} - A \\mathbf{\\hat{x}} \\perp \\mathbf{a_2} \\rightarrow \u0026gt;\\mathbf{a}_2^T(\\mathbf{b} - A \\mathbf{\\hat{x}})=0 $ \u0026hellip;\n$\\mathbf{b} - A \\mathbf{\\hat{x}} \\perp \\mathbf{a_m} \\rightarrow \u0026gt;\\mathbf{a}_m^T(\\mathbf{b} - A \\mathbf{\\hat{x}})=0 $\nsame with the equation below which is called a normal equation $$A^T A\\hat{\\mathbf{x}}= A^T\\mathbf{b}$$\n3.3 Normal Equation given a least squares problem, $Ax \\simeq \\mathbf{b}$ , we obtain normal equation $$A^T A\\hat{\\mathbf{x}}= A^T\\mathbf{b}$$\nif $A^T A$ is invertible, then the solution is computed as $$\\hat{\\mathbf{x}}= (A^T A)^{-1} A^T\\mathbf{b} $$\nIf $A^T A$ is not invertible, the system has either no solution or infinitely many solutions. However, the solution always exist for this \u0026ldquo;normal\u0026rdquo; equation, and thus infinitely many solutions exist.\nIf and only if the columns of $A$ are linearly dependent, $A^TA $ is not invertible. However, $A^T A$ is usually invertible.\n3.4 Orthogonal Projection consider the orthogonal projection of $\\mathbf{b}$ onto Col $A$ as\n$$ \\mathbf{\\hat{b}} = f(\\mathbf{b})= A{\\mathbf{\\hat{x}}} = A(A^TA)^{-1}A^T\\mathbf{b} $$\nOrthogonal set : A set of vectors {$ \\mathbf{u_1, \u0026hellip; u_p}$} in $\\mathbb{R^n}$ if each pair of distinct vectors from the set is orthogonal. That is, if $\\mathbf{u_i \\cdot u_j}=0$ whenever $i \\neq j$. So, All vectors in the orthogonal set are orthogonal to each other.\nOrthonormal set : A set of vectors {$ \\mathbf{u_1, \u0026hellip; u_p}$} in $\\mathbb{R^n}$ if it is an orthogonal set of unit vectors (norm=1) .\nOrthogonal and Orthonormal Basis : Consider basis {$\\mathbf{v_1, \u0026hellip; , v_p}$} of a p-dimensional subspace $W $ in $\\mathbb{R}^n$. We can make it as an orthogonal(or orthonormal) basis using Gram-Schmidt process.\nOrthogonal Projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto Line : Consider the orthogonal projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto Line (1D subspace $L$). From the above picture, $\\hat{\\mathbf{y}}$ can be represented by multiplication of the norm (length) for $\\hat{\\mathbf{y}}$ (=$ | | \\hat{\\mathbf{y}} | |$) and the unit vector of $\\mathbf{u}$ ( = $\\mathbf{\\frac{u}{||u||}} $ ). And we can calculate $\\hat{\\mathbf{y}}$ from the inner product of $\\mathbf{y} \\text{ and } \\mathbf{u}$ $$ \\hat{\\mathbf{y}} = proj_L(\\mathbf{y})$$\n$$ = | | \\hat{\\mathbf{y}} | | \\cdot \\mathbf{\\frac{u}{||u||}} = \\mathbf{\\frac{y \\cdot u}{||u||}} \\cdot \\mathbf{\\frac{u}{||u||}} $$\n$$= \\mathbf{\\frac{y \\cdot u}{u \\cdot u}} \\cdot \\mathbf{u}, ( \\because \\mathbf{||u||}^2 = \\mathbf{u \\cdot u})$$\nOrthogonal Projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto Plane : Consider the orthogonal projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto two-dimensional subspace $W$ $$ W = span ( \\mathbf{u_1, u_2} ) $$\n$$ \\hat{\\mathbf{y}} = proj_L(\\mathbf{y}) = \\mathbf{\\frac{y \\cdot u_1}{u_1 \\cdot u_1}} \\cdot \\mathbf{u_1} +\\mathbf{\\frac{y \\cdot u_2}{u_2 \\cdot u_2}} \\cdot \\mathbf{u_2} $$\nOrthogonal Projection as a Linear Transformation : Consider a transformation of orthogonal projection $\\mathbf{\\hat{b}}$ of $\\mathbf{b}$. If orthonormal basis {$\\mathbf{u_1, u_2}$} of a subspace $W$ is given, both $\\mathbf{u_1, u_2}$ are unit vector and we can get following equation : $$ \\mathbf{\\hat{b}} = f(\\mathbf{b}) = \\mathbf{ (b \\cdot u_1)u_1 + (b \\cdot u_2 )u_2 } $$\n$$ \\mathbf{ = (u_1^Tb)u_1 + (u_2^T b)u_2 = (u_1u_1^T + u_2 u_2^T)b = \\begin{bmatrix} u_1^T \\\\ u_2^T \\end{bmatrix} \\begin{bmatrix} u_1 \\ u_2 \\end{bmatrix} = UU^T b } $$\n3.5 Gram-Schmidt Orthonomalization If two (or more) vectors are linearly independent, these vectors create an vector space. And we want to represent this vector space using a orthogonal (or orthonormal) set in many cases. In the previous chapter, we\u0026rsquo;ve learned orthogonal projection of one vector to the other. From this we can represent a given vector space (linearly independent vector set) with a orthogonal vector set : Gram-Schmidt Example: Let $W=span( \\mathbf{x_1, x_2})$, where $\\mathbf{x_1} = \\begin{bmatrix} 3\\\\ 6\\\\ 0\\end{bmatrix}$, $\\mathbf{x_2} = \\begin{bmatrix} 1\\\\ 2\\\\ 2\\end{bmatrix}$. Construct an orthogonal basis {$\\mathbf{v_1, v_2}$} for $W$\nSolution:\nLet $\\mathbf{v_1} = \\mathbf{x_1}$. And, let $\\mathbf{v_2}$ the component of $\\mathbf{x_2}$ is orthogonal to $\\mathbf{x_1}$, i.e. $$ \\mathbf{v_2 = x_2 - proj_{v1}(x2) = x_2 - \\frac{x_2 \\cdot x_1}{x_1 \\cdot x_1} x_1 }= \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} - \\frac{15}{45}\\begin{bmatrix} 3 \\\\ 6 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 2 \\end{bmatrix} $$ Now, we get orthogonal basis for $W$. That is, $$\\mathbf{v_1} = \\begin{bmatrix} 3\\\\ 6\\\\ 0\\end{bmatrix}, \\mathbf{v_2} = \\begin{bmatrix} 0\\\\ 0\\\\ 2\\end{bmatrix}$$ We can get the orthonormal set $\\mathbf{(u_1,u_2)}$ of these vectors by dividing the norm of each vectors $$\\mathbf{u_1} = \\frac{1}{\\sqrt{45}} \\begin{bmatrix} 3\\\\ 6\\\\ 0\\end{bmatrix}, \\mathbf{u_2} = \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix}$$ 3.6 QR Factorization (QR Decomposition) QR Decomposition : is a decomposition of a matrix $A$ into a orthogonal matrix $Q$ (with Gram-Schmidt) and upper triangular matrix $R$ . For a given lineary independent matrix, we can find orthogonal basis of the vector space using Gram-Schumidt. There should be another matrix that undo the above orthogonalization tranfrom Here, the orthogonalized matrix is represented as $Q$ and the \u0026lsquo;un-doing\u0026rsquo; matrix is represented as $R$. $$A \\rightarrow \\text{Gram-Schumidt} \\rightarrow Q \\rightarrow \\times R \\rightarrow A $$\nExample : Consider the decomposition of linearly independeny matrix $A = \\begin{bmatrix} 12\u0026amp;-51\u0026amp;4\\\\ 6\u0026amp;167 \u0026amp;-68\\\\ -4\u0026amp;24\u0026amp;-41\\end{bmatrix}$\nSolution :\nThen, we can calculate $Q$ by means of Gram-Schmidt as follows:\n$$U = [\\mathbf{u1, u2, u3}] = \\begin{bmatrix} 12\u0026amp;-69\u0026amp;-58/5\\\\ 6\u0026amp;158 \u0026amp;-6/5\\\\ -4\u0026amp;30\u0026amp;-33\\end{bmatrix}$$\n$$ Q = [\\mathbf{\\frac{u_1}{|u_1|}, \\frac{u_2}{|u_2|}, \\frac{u_3}{|u_3|}}] = \\begin{bmatrix} 6/7\u0026amp;-69/175\u0026amp;-58/175\\\\ 3/7\u0026amp;158/175 \u0026amp;-6/175\\\\ -2/7\u0026amp;30/175\u0026amp;-33/35\\end{bmatrix} $$\nThus, we have\n$$R = Q^TA = \\begin{bmatrix} 14\u0026amp;21\u0026amp;-14\\\\ 0\u0026amp;175 \u0026amp;-70\\\\ 0\u0026amp;0\u0026amp;35\\end{bmatrix}$$\nSummary Least Square Problem : No solution exists for the linear equation $Ax=b$\n=\u0026gt; Approximate the solution $\\hat{x}$ minimizes $ b - A \\hat{x}$.\n=\u0026gt; And $ b - A \\hat{x}$ should be orthogonal to Column Space =\u0026gt; $ A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = 0$\n=\u0026gt; From this equation, We get the normal equation $A^T A\\hat{\\mathbf{x}}= A^T\\mathbf{b}$\n=\u0026gt; And if $A^T A$ is invertible, the solution $\\hat{\\mathbf{x}}= (A^T A)^{-1}A^T\\mathbf{b}$\n=\u0026gt; On the other view, $\\hat{b}$ is the orthogonal projection of $b$\n=\u0026gt; Now, we can find orthogonal projection of a vector\n=\u0026gt; we can find orthogonal vector set using given linearly independ vectors (in that vector space)\n","id":26,"section":"Mathematics","summary":"3.0 Least Square Inner Product : Given $ \\mathbf{u,v} \\in \\mathbb{R}^n$, we can consider $ \\mathbf{u,v} $ as $n \\times 1$ matrices. The number $\\mathbf{u^Tv}$ is called inner product or dot product, and it is written as $ \\mathbf{u \\cdot v} $. Vector Norm : The length or magnitude of $\\mathbf{v}$, can be calculated as $ \\sqrt{ \\mathbf{v} \\cdot \\mathbf{v} }$ $L_p$ Norm : $ \\Vert \\mathbf{x} \\Vert_p = (","tags":null,"title":"Linear Algebra for ML #3 | Least Square ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec3/","year":"2019"},{"content":"\n2.1 Linear Equation and Linear System Linear Equation is an equation that can be written in the form $$ a_1x_1 + \u0026hellip;. a_nx_n = b $$ The above equation can be written as $ \\textbf{a}^T \\textbf{x} = b $. Linear System is a collection of one or more linear equations Identity Matrix : $I$ is a square matrix whose diagonal entries are all 1\u0026rsquo;s and all the other entries are zeros.\nInverse Matrix : $A^{-1}$ is defined such that $A^{-1}A = AA^{-1} = I$\n$$A^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d\u0026amp;{-b}\\\\ -c\u0026amp;a \\end{bmatrix}$$\nDeterminant : $det(A) = ad-bc$ determines whether $A$ is invertible\nIf $A$ is non invertible, $A \\textbf{x} = \\textbf{b}$ will have either no solution or infinitely many solutions.\n2.2 Linear Combination For given vectors $\\textbf{v}_1,\\textbf{v}_2, \u0026hellip; \\textbf{v}_p $ and given scalars $ c_1, c_2, \u0026hellip; c_p $,\n$ c_1\\textbf{v}_1 + c_2 \\textbf{v}_2, \u0026hellip; + c_p \\textbf{v}_p $ is called Linear Combination of vectors with weights $c$.\nSpan : Span{$\\textbf{v}_1, \u0026hellip; \\textbf{v}_p$} is defined as the set of all linear combinations of $\\textbf{v}_1, \u0026hellip; \\textbf{v}_p$. That is, Span is the collection of all vectors that can be written in the form $ c_1\\textbf{v}_1 + c_2 \\textbf{v}_2, \u0026hellip; + c_p \\textbf{v}_p $. Geometric Description of Span : $ \\textbf{v}_1 $ and $\\textbf{v}_2$ are in $\\mathbb{R}^3$ then Span is the plane in $\\mathbb{R}^3$ that contains $ \\textbf{v}_1 $ and $\\textbf{v}_2$ Geometric Interpretation of Vector Equation : we can find whether the solution of vector equation exists using the knowledge of span. The solution of below vector equation exists only when $ \\textbf{b} \\in Span(\\textbf{a}_1, \\textbf{a}_2, \\textbf{a}_3) $. $$\\textbf{a}_1 x_1 + \\textbf{a}_2 x_2 + \\textbf{a}_3 x_3 = b $$\nMatrix Multiplications as Linear Combinations of Vectors : $$ \\begin{bmatrix} 60\u0026amp;1.7\u0026amp;1 \\\\ 65\u0026amp;1.6\u0026amp;0 \\\\ 55\u0026amp;1.8\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 60\\\\ 65\\\\ 55\\end{bmatrix} x_1 + \\begin{bmatrix} 1.2 \\\\ 1.6 \\\\ 1.8 \\end{bmatrix} x_2 + \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} x_3 $$\nMatrix Multiplication as Column Combinations $$ \\begin{bmatrix} 1\u0026amp;1\u0026amp;0 \\\\ 1\u0026amp;0\u0026amp;1 \\\\ 1\u0026amp;-1\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 1\\\\ 1\\end{bmatrix}1 + \\begin{bmatrix} 1\\\\ 0\\\\ -1\\end{bmatrix}2 + \\begin{bmatrix} 0\\\\ 1\\\\ 1\\end{bmatrix}3 $$\nMatrix Multiplication as Row Combinations $$ \\begin{bmatrix} 1\u0026amp;2\u0026amp;3 \\end{bmatrix} \\begin{bmatrix} 1\u0026amp;1\u0026amp;0 \\\\ 1\u0026amp;0\u0026amp;1 \\\\ 1\u0026amp;-1\u0026amp;1 \\end{bmatrix} = 1 \\times \\begin{bmatrix} 1\u0026amp;1\u0026amp;0 \\end{bmatrix} + 2\\times \\begin{bmatrix} 1\u0026amp; 0\u0026amp;1 \\end{bmatrix} + 3 \\times \\begin{bmatrix} 1\u0026amp; -1\u0026amp; 1\\end{bmatrix}$$\nMatrix Multiplication as Sum of Outer Products $$\\begin{bmatrix} 1\u0026amp;1 \\\\ 1\u0026amp;-1 \\\\ 1\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} 1\u0026amp; 2\u0026amp; 3 \\\\ 4\u0026amp;5\u0026amp;6 \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 1\\\\ 1\\end{bmatrix} \\begin{bmatrix} 1\u0026amp; 2\u0026amp; 3\\end{bmatrix} + \\begin{bmatrix} 1\\\\ -1\\\\ 1\\end{bmatrix} \\begin{bmatrix} 4\u0026amp; 5\u0026amp; 6\\end{bmatrix} $$\n2.3 Linear Independence The solution for $A \\textbf{x} = \\textbf{b} $ is unique, when $\\textbf{a}_1, \\textbf{a}_2, \\textbf{a}_3 $ are linearly independent\nInfinitely many solutions exists when $\\textbf{a}_1, \\textbf{a}_2, \\textbf{a}_3 $ are linearly dependent.\nLinear Independence (practical) : Given a set of vectors $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ , check if $ \\textbf{v}_j$ can be represented as a linear combination of the previous vectors. If at least one such $ \\textbf{v}_j$ is found then $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ is linearly dependent. Linear Independence (Formal) : Consider $ x_1 \\textbf{v}_1 + \u0026hellip; + x_p \\textbf{v}_p = \\textbf{0} $. Obviously, one solution is $\\textbf{x} = [0\u0026hellip;0]$, which we call a trivial solution. If this is the only solution $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ is the only solution. if this system also has other nontrivial solutions, $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ are linearly dependent. 2.4 Basis of a Subspace Subspace : is defined as a subset of $\\mathbb{R}^n$ closed under linear combination\nA subspace is always represented as Span {$\\mathbf{v}_1 , \u0026hellip; , \\mathbf{v}_p$}\nBasis of a subspace : a set of vectors that satisfies both of the following Fully spans the given subspace $H$\nLinearly independent (i.e., no redundancy) Non-Uniqueness of Basis : In the subspace $H$ (green plane), there other set of linearly independent vectors that span the subspace $H$ Dimension of Subspace : Even though different basis exist for $H$, the number of vectors in any basis for $H$ will be unique. This number is the dimension of $H$. Column Space of Matrix : the subspace could be spanned by the column vector of $A$. Rank of Matrix : the dimension of the column space of $A$ Details $$ A = \\begin{bmatrix} 1\u0026amp;0\u0026amp;2 \\\\ 0\u0026amp;1\u0026amp;1 \\\\ 1\u0026amp;0\u0026amp;2 \\end{bmatrix}, \\ \\ \\text{ Column Vector } a_1, a_2, a_3 = \\begin{bmatrix} 1\\\\ 0\\\\ 1\\end{bmatrix} , \\begin{bmatrix} 0\\\\ 1\\\\ 0\\end{bmatrix}, \\begin{bmatrix} 2\\\\ 1\\\\ 2\\end{bmatrix} $$\nColumn Space is subspace of column vectors from matrix $A$. And we need to find basis vector to get column space.\n$a_1, a_2$ is independent each other. $a_3$ is represented as linear combination of $a_1, a_2$ . As a result, $a_3$ is dependent on $a_1, a_2$. So, $a_1, a_2$ become basis vector for colum space of $A$\nHere, Rank of matrix $A$ is two. Because we have only two vectors $a_1, a_2$ that are linearly independent.\nIf a solution for $A \\textbf{x} = \\textbf{b} $ exists, then $b$ must be on the column space of $A$. Because, $b$ is linear combination of column vectors\n$$ Ax = \\begin{bmatrix} 1\u0026amp;0\u0026amp;2 \\\\ 0\u0026amp;1\u0026amp;1 \\\\ 1\u0026amp;0\u0026amp;2 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = \\begin{bmatrix} 1\\\\ 0\\\\ 1\\end{bmatrix} x_1 + \\begin{bmatrix} 0\\\\ 1\\\\ 0\\end{bmatrix} x_2 + \\begin{bmatrix} 2\\\\ 1\\\\ 2\\end{bmatrix} z_3 = \\begin{bmatrix} b_1\\\\ b_2\\\\ b_3\\end{bmatrix} $$\n2.5 Chage of Basis Generally, vectors are defined on cartesian coordinate , and it means they are based on standard basis (1,0), (0,1). But sometimes, we need vectors on different coordinate (basis) for some reasons like computational efficiency. (e.g. eigendecomposition) When the coordinate changes, the vector itself does not changed, but the representation of vector has been changed from (2,2) to (2,0) as in figure below 2.6 Linear Transformation A transformation, function, or mapping maps an input $x$ to an output $y$\nDomain : Set of all the possible values of $x$\nCo-Domain : Set of all the possible values of $y$\nRange : Set of all the output values mapped by each $x$ in the domain\nLinear Transforamtion : A transformation (or mapping) $T$ is linear if : $$ T(c \\mathbf{u}+ d \\mathbf{v}) = cT (\\mathbf{u}) + dT(\\mathbf{v})$$\nMatrix of Linear Transformation : In general, let $T$ : $ \\mathbb{R}^n \\rightarrow \\mathbb{R}^m $ be a linear transformation. Then $T$ is always written as a matrix-vector multiplication. i.e., $T(\\mathbf{x}) = A \\mathbf{x}$\nHere, the matrix $A$ is called the standad matrix of the linear transformation.\n2.7 Linear Transformation in Neural Networks Fully Connected Layers can be thought as a linear transformation\nFully Connected Layers usually involve a bias term, That\u0026rsquo;s why we call it an affine layer, not a linear Layers\nReference : colah\u0026rsquo;s blog\n2.8 ONTO and ONE-TO-ONE ONTO : A mapping $T$ : $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is said to be onto $\\mathbb{R}^m$ if each $\\mathbf{b} \\in \\mathbb{R}^m$ is the image of at least one $x \\in \\mathbb{R}^n$. That is, the range is equal to the co-domain.\nONE-TO-ONE : A mapping $T$ : $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is said to be one-to-one if each $\\mathbf{b} \\in \\mathbb{R}^m$ is the image of at most one $x \\in \\mathbb{R}^n$. That is, each output vector in the range is mapped by only one input vector, no more than that.\n","id":27,"section":"Mathematics","summary":"2.1 Linear Equation and Linear System Linear Equation is an equation that can be written in the form $$ a_1x_1 + \u0026hellip;. a_nx_n = b $$ The above equation can be written as $ \\textbf{a}^T \\textbf{x} = b $. Linear System is a collection of one or more linear equations Identity Matrix : $I$ is a square matrix whose diagonal entries are all 1\u0026rsquo;s and all the other entries are","tags":null,"title":"Linear Algebra for ML #2 | Linear System \u0026 Linear Transform ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec2/","year":"2019"},{"content":"\n1.1 Scalars, Vectors, Matrices, and Tensors Scalars : just a single number, italics in this book, lower-case variable name, such as $x$ (loswercase)\nVectors : an array of numbers, in order, bold lower case name, such as $\\bf x$ (bold lowercase)\nMatrices : 2-D array of numbers, with two indices, bold uppercase variable name, such as $A$ (uppercase)\nTensors : an array of numbers arranged on a regular grid with a variable number of axes.\n1.2 Matrix Additions and Multiplications Transpose : operation on matrices. We can define a vector using the transpose operator e.g. ${\\bf x} = [x_{1}, x_{2}, x_{3}]^{T}$ , for scalar $a = a^{T}$\nAdd : we can add matrices to each other as long as they have the same shape.\nTo define the matrix product of matrices $A$ and $B$, $A$ must have the same number of columns as the number of rows in $B$\nThe matrix product is not commutative ( $AB ≠ BA$ )\n$$C_{i,j} = \\sum_k A_{i,k}B_{k,j}$$\nelement-wise product (product of the individual elements) is denoted as $A\\odot B$\ndot-product between two vectors $x$ and $y$ is the matrix product $x^{T}y$\n1.3 Reference Linear Algebra for AI, Edwith Lay et al. Linear Algebra and Its applications, 5th editionS Ian Good Fellow. Deep Learning Book Gilbert Strang\u0026rsquo;s MIT Lecture [Summary of Gilbert LA](https://catonmat.net/mit-linear-algebra-part-one ","id":28,"section":"Mathematics","summary":"1.1 Scalars, Vectors, Matrices, and Tensors Scalars : just a single number, italics in this book, lower-case variable name, such as $x$ (loswercase)\nVectors : an array of numbers, in order, bold lower case name, such as $\\bf x$ (bold lowercase)\nMatrices : 2-D array of numbers, with two indices, bold uppercase variable name, such as $A$ (uppercase)\nTensors : an array of numbers arranged on a regular grid with a variable number of axes.","tags":null,"title":"Linear Algebra for ML #1 | Introductions ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec1/","year":"2019"},{"content":"\n6.1 Introduction Statistical Inference, or \u0026ldquo;learning\u0026rdquo; as it is called in computer science, is the process of using data to infer distribution that generated the data. 6.2 Parametric and Nonparametric Models A statistical model $\\Im$ is a set of distributions (or densities or regression functions)\nParametric model : is a set of $\\Im$ that can be parameterized by a finite number of parameters\nIf we assume that the data come from a Normal distribution, then It would be two-prarmeter model.\n$$ \\Im = {f(x; \\mu \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}} exp {-\\frac{1}{2 \\sigma^2}(x-\\mu)^2 } $$\nIn general, a parametric model takes the form $$ \\Im = {f(x; \\theta) : \\theta \\in \\Theta }$$\nNon-Parametric model : is a set $\\Im$ that cannot be parameterized by a finited number of parameters.\nFrequentists and Bayesians : The two dominant approaches to statistical inference are called frequentists inference and Bayyesian Inference.\n6.3 Fundamental Concepts in inference Many inferential problems can be identified as being one of three types : estimation, confidence sets, or hypothesis testing.\nPoint Estimation : refers to providing a single \u0026ldquo;best guess\u0026rdquo; of some quantity of interest\nConfidence Sets : A $1-\\alpha$ confidence interval for a parameter $\\theta$ is an interval $C_n = (a,b)$ where $a = a(X_1, \u0026hellip; , X_n)$ and $b = b(X_1, \u0026hellip; , X_n)$\nHypothesis Testing : In hypothesis testing, we start with some default theory , called null hypothesis, and we ask if the data provide sufficient evidence to reject the theory.\n","id":29,"section":"Mathematics","summary":"6.1 Introduction Statistical Inference, or \u0026ldquo;learning\u0026rdquo; as it is called in computer science, is the process of using data to infer distribution that generated the data. 6.2 Parametric and Nonparametric Models A statistical model $\\Im$ is a set of distributions (or densities or regression functions)\nParametric model : is a set of $\\Im$ that can be parameterized by a finite number of parameters\nIf we assume that the data come from a Normal distribution, then It would be two-prarmeter model.","tags":null,"title":"Statistics #6 | Models, Statistical Inference and Learning ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch6/","year":"2019"},{"content":"\n5.1 Introduction The law of large numbers says that the sample average converges in proability to the expectation $ \\mu = \\mathbb{E}(X_i)$\nThe central limit theorem says that $ \\sqrt{n} (\\overline{X - \\mu})$ converges in distribution to a Normal distribution.\n5.2 Types of Convergence $X_n$ converges to $X$ in probability, written $X_n \\xrightarrow{P}{} X$ if for every $\\epsilon \u0026gt; 0$ $$ \\mathbb{P}(|X_n - X| \u0026gt; \\epsilon) \\rightarrow 0$$\n$X_n$ converges to $X$ in distribution, written $X_n \\rightsquigarrow X$ if $$ \\lim_{n\\rightarrow \\infty} F_n(t) = F(t) $$\n$X_n$ converges to $X$ in quadratic mean (also called convergence in $L_2$), written $X_n \\xrightarrow{qm}{} X$, if $$ \\mathbb{E}(X_n - X)^2 \\rightarrow 0 \\ \\text{as} \\ n \\rightarrow \\infty $$\n5.3 The Law of Large Numbers The weak Law of Large Numbers $$ If X_1 , \u0026hellip; , X_n \\ \\text{ are IID, then } \\overline{X}_n \\xrightarrow{P}{} \\mu$$\n5.4 The Central Limit Theorem Let $X_1, \u0026hellip; , X_n$ be IID with mean $\\mu$ and variance $\\sigma^2$. Let $\\overline{X_n} = n^{-1} \\sum_{i=1}^n X_i$ Then, $$Z_n \\equiv \\frac{\\overline{X_n}-\\mu}{\\sqrt{\\mathbb{V(\\overline{X_n})}}} = \\frac{\\sqrt{n}({\\overline{X_n}-\\mu)}}{\\sigma} \\rightsquigarrow Z$$\n","id":30,"section":"Mathematics","summary":"5.1 Introduction The law of large numbers says that the sample average converges in proability to the expectation $ \\mu = \\mathbb{E}(X_i)$\nThe central limit theorem says that $ \\sqrt{n} (\\overline{X - \\mu})$ converges in distribution to a Normal distribution.\n5.2 Types of Convergence $X_n$ converges to $X$ in probability, written $X_n \\xrightarrow{P}{} X$ if for every $\\epsilon \u0026gt; 0$ $$ \\mathbb{P}(|X_n - X| \u0026gt; \\epsilon) \\rightarrow 0$$\n$X_n$ converges to $X$ in distribution, written $X_n \\rightsquigarrow X$ if $$ \\lim_{n\\rightarrow \\infty} F_n(t) = F(t) $$","tags":null,"title":"Statistics #5 | Convergence of Random Variable ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch5/","year":"2019"},{"content":"\n3.1 Expectation of a Random Variable The expected value, or mean, or first moment of $X$ is defined to be $$ \\mathbb{E}(X) = \\int xdF(x) = \\begin{cases} \\sum_x xf(x) \u0026amp; (\\text{if X is discrete})\\ \\int xf(x)dx \u0026amp; ( \\text{if X is continuous}) \\end{cases}$$\n3.3 Variance and Covariance The variance measures the spread of a distribution $$ \\sigma^2 = \\mathbb{E}(X-\\mu)^2 = \\int (x-\\mu)^2 dF(x) $$\nThe covariance and correlation between $X$ and $Y$ measure how strong the linear relationship is between $X$ and $Y$ $$\\text{Cov}(X,Y) = \\mathbb{E}((X-\\mu_X)(Y- \\mu_Y ))$$\n$$ \\text{Correlation} \\ \\rho = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} $$\n3.5 Conditional Expectation The conditional expectation of $X$ given $Y=y$ is $$ \\mathbb{E}(X | Y=y) = \\begin{cases} \\sum x f_{X|Y} (x|y) dx \u0026amp; (\\text{discrete case}) \\ \\int x f_{X|Y}(x|y)dx \u0026amp; (\\text{continuous case}) \\end{cases}$$\n3.6 Moment Generating Functions The moment generating function or Laplace transform , of $X$ is defined by $$ \\psi_X(t) = \\mathbb{E}(e^{tX}) = \\int e^{tx}dF(x) $$\n","id":31,"section":"Mathematics","summary":"3.1 Expectation of a Random Variable The expected value, or mean, or first moment of $X$ is defined to be $$ \\mathbb{E}(X) = \\int xdF(x) = \\begin{cases} \\sum_x xf(x) \u0026amp; (\\text{if X is discrete})\\ \\int xf(x)dx \u0026amp; ( \\text{if X is continuous}) \\end{cases}$$\n3.3 Variance and Covariance The variance measures the spread of a distribution $$ \\sigma^2 = \\mathbb{E}(X-\\mu)^2 = \\int (x-\\mu)^2 dF(x) $$\nThe covariance and correlation between $X$ and $Y$ measure how strong the linear relationship is between $X$ and $Y$ $$\\text{Cov}(X,Y) = \\mathbb{E}((X-\\mu_X)(Y- \\mu_Y ))$$","tags":null,"title":"Statistics #3 | Expectation ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch3/","year":"2019"},{"content":"\n2.1 Introduction How do we link sample spaces and events to data? Random Variable!\nA random variable is a mapping $ X : \\Omega \\rightarrow \\mathbb{R} $ that assigns a real number $X(\\omega)$ to each outcome $\\omega$.\n2.2 Distribuition Functions and Probability Functions Cumulative Distribution Function (CDF) : is the function $F_x : \\mathbb{R} \\rightarrow [0,1] $ defined by $$F_X(x) = \\mathbb{P}(X \\leq x)$$\nProbability Mass Function : A Random Variable $X$ is discrete if it takes countably many values ${x_1, x_2, \u0026hellip;}$. We define probability mass function for $X$ by $$f_X(x) = \\mathbb{P}(X=x).$$\nProbability Density Function : A Random Variabe $X$ is continuous if there exists a function $f_X$ such that $f_X(x) \\geq$ for all $x$, $\\int f_X(x)dx = 1$ and for every $a \\leq b$, the probability Density function is $$ \\mathbb{P}(a\u0026lt;X\u0026lt;b) = \\int_{a}^b f_X(x)dx $$\n2.3 Some Important Discrete Random Variables The Point Mass Distribution $$ F(x) = \\begin{cases} 0 \u0026amp; (x\u0026lt;a)\\ 1 \u0026amp; (x\\geq a) \\end{cases} $$\nThe Discrete Uniform Distribution $$ f(x) = \\begin{cases} 1/k \u0026amp; ( \\text{for} \\ x=1, \u0026hellip; ,k ) \\ 0 \u0026amp; ( \\text{otherwise} ) \\end{cases} $$\nThe Bernoulli Distribution : Let $X$ represent a binary coin flip. Then $\\mathbb{P}(X=1) = p $ and $\\mathbb{P}(X=0) = 1- p$ for some $p \\in [0,1] $. We saye that $X$ has a Bernoulli Distribution written $X \\sim Bernoulli(p)$. $$ f(x) = p^x(1-p)^{1-x} \\ \\text{for} \\ x \\in {0,1}$$\nThe Binomial Distribution $$ f(x) = \\begin{cases} {n \\choose x} p^x (1-p)^{n-x} \u0026amp; ( \\text{for} \\ x=0, \u0026hellip; ,n ) \\ 0 \u0026amp; ( \\text{otherwise} ) \\end{cases} $$\nThe Geometric Distribution $$ \\mathbb{P}(X=k) = p(1-p)^{k-1} , \\ k \\geq 1 $$\nThe Poisson Distribution : The Poisson is often used as a model for counts of rare events like radioactive decay and traffic accidents. $$ f(x) = e^{- \\lambda} \\frac{\\lambda^x}{x!}, \\ x \\geq 0 $$\n2.4 Some Important Continuous Random Variables The Uniform Distribution:\n$$ f(x) = \\begin{cases} \\frac{1}{b-a} \u0026amp; ( \\text{for} \\ x \\in [a,b]) \\ 0 \u0026amp; ( \\text{otherwise} ) \\end{cases} $$\nGaussian Distribution : $X$ has a Normal distribution with parameters $ \\mu $ and $\\sigma$, denoted by $X \\sim N(\\mu, \\sigma^2)$\nExponential Distribution : $X$ has an Exponential distribution with parameter $\\beta$, denoted by $X \\sim \\text{Exp}(\\beta)$, if\n$$f(x) = \\frac{1}{\\beta} e^{-x/\\beta}$$\n2.5 Bivariate Distributions Joint Mass Function : Given a pair of discrete random variables $X$ and $Y$ define the joint mass function by $f(x,y) = \\mathbb{P}(X = x, Y = y)$ 2.6 Marginal Distributions If $(X, Y)$ have joint distribution with mass function $f_{X,Y}$ then the marginal mass function for $X$ and $Y$ is defined by $$ f_X(x) = \\mathbb{P}(X=x) = \\sum_y \\mathbb{P}(X=x, Y=y) = \\sum_y f(x,y) $$\n$$ f_Y(y) = \\mathbb{P}(Y=y) = \\sum_x \\mathbb{P}(X=x, Y=y) = \\sum_x f(x,y) $$\nFor continuous random variables, the marginal densities are $$ f_X(x) = \\int f(x,y) dy, \\ \\text{and} f_Y(y) = \\int f(x,y)dx $$\n2.7 Independent Random Variables Two random variables $X$ and $Y$ are independent if for every $A$ and $B$, $$ \\mathbb{P}(X \\in A, Y \\in B) = \\mathbb{P}(X \\in A) \\mathbb{P}(Y \\in B)$$\n2.8 Conditional Distributions The conditional probability mass function is $$ f_{X|Y}(x|y) = \\mathbb{P}(X=x | Y=y) = \\frac{\\mathbb{P}(X=x, Y=y)}{\\mathbb{P}(Y=y)} = \\frac{f_{X,Y}(x,y)}{f_Y(y)}$$\n2.9 Multivariate Distributions and IID Samples If $X_1, \u0026hellip; , X_n $ are independent and each has the same marginal distribution with CDF $F$ we say that $X_1, \u0026hellip; , X_n$ are IID(independent and identically distributed) and we write $$ X_1 , \u0026hellip; , X_n \\sim F.$$\n2.10 Two Important Multivariate Distributions Multinomial : The multivariate version of a Binomial is called a Multinomial.\nMulticariate Normal : The univariate normal has two parameters, $\\mu$ and $\\sigma$. In the multivariate version, $\\mu$ is a vector and $\\sigma$ is replaced by a matrix $\\Sigma$\n2.11 Transformations of Random variables Suppose that $X$ is a random variable with PDF $f_X$ and CDF $F_X$. Let $Y$ = r(X) be a function of $X$, for example, $Y = X^2$ or $Y = e^X$. We call $Y=r(X)$ a Transformation of $X$. 2.12 Transformations of Several Random variables Three Steps for Transformation\nFor each $z$, find the set $A_z = { (x,y) : r(x,y) \\leq z }$\nFind the CDF\n$$F_Z(z) = \\mathbb{P}(Z \\leq z) = \\mathbb{P}(r(X,Y) \\leq z) = \\mathbb{P}({ (x,y); r(x,y) \\leq z })$$\nThem $f_Z(z) = F\u0026rsquo;_{Z}(z)$\n","id":32,"section":"Mathematics","summary":"2.1 Introduction How do we link sample spaces and events to data? Random Variable!\nA random variable is a mapping $ X : \\Omega \\rightarrow \\mathbb{R} $ that assigns a real number $X(\\omega)$ to each outcome $\\omega$.\n2.2 Distribuition Functions and Probability Functions Cumulative Distribution Function (CDF) : is the function $F_x : \\mathbb{R} \\rightarrow [0,1] $ defined by $$F_X(x) = \\mathbb{P}(X \\leq x)$$\nProbability Mass Function : A Random Variable $X$ is discrete if it takes countably many values ${x_1, x_2, \u0026hellip;}$.","tags":null,"title":"Statistics #2 | Random Variables ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch2/","year":"2019"},{"content":"\n1.1 Introduction Probability is a mathematical language for quantifying uncertatinty 1.2 Sample Spaces and Events Sample Space $\\Omega$ : is the set of possible outcomes of an experiment\nEvents : Subsets of Ω are called Events\n1.3 Probability A function $\\mathbb{P}$ that assigns a real number $ \\mathbb{P}(A) $ to every event $A$ is a probability distribution or a probability measure. 1.4 Probability on Finite Sample Spaces If $\\Omega$ is finite and if each outcome is equally likely, then, the uniform probability distribution is $$ \\mathbb{P} = \\frac{|A|}{| \\Omega |}$$\nAnd we need to count the number of points in an event using combinational methods, to compute probabilities. $${n \\choose x} = \\frac {n \\times (n-1) \\times \u0026hellip; (n-k-1)}{k !} $$\n1.5 Independent Event Two events $A$ and $B$ are independent if $ \\mathbb{P}(AB) = \\mathbb{P}(A) \\mathbb{P}(B)$ 1.6 Conditional probability If $\\mathbb{P}(B)\u0026gt;0$ then the conditional probability of A given B is $$ \\mathbb{P(A|B)} = \\frac{\\mathbb{P}(AB)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} $$\nIn general, $ \\mathbb{P} (A | B) \\neq \\mathbb{P} (B | A) $\n$A$ and $B$ are independent if and only if $ \\mathbb{P}(A | B) = \\mathbb{P}(A) $\n1.7 Bayes\u0026rsquo; Theorem Basis of expert systems and Bayes\u0026rsquo; nets\nBayes\u0026rsquo; Theorem : Let $A_1, \u0026hellip; A_k$ be a partition of $\\Omega$ such that $\\mathbb{P}(A_i) \u0026gt; 0$ for each $i$. If $\\mathbb{P}(B)\u0026gt;0$, then, for each $i = 1 , \u0026hellip; k$,\n$$ \\mathbb{P}(A_i | B) = \\frac{\\mathbb{P}(A_i B)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B | A_i) \\mathbb{P}(A_i)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B | A_i) \\mathbb{P}(A_i)}{ \\sum_j \\mathbb{P}(B | A_j) \\mathbb{P}(A_j)} $$\n","id":33,"section":"Mathematics","summary":"1.1 Introduction Probability is a mathematical language for quantifying uncertatinty 1.2 Sample Spaces and Events Sample Space $\\Omega$ : is the set of possible outcomes of an experiment Events : Subsets of Ω are called Events 1.3 Probability A function $\\mathbb{P}$ that assigns a real number $ \\mathbb{P}(A) $ to every event $A$ is a probability distribution or a probability measure. 1.4 Probability on Finite Sample Spaces If $\\Omega$ is","tags":null,"title":"Statistics #1 | Probability ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch1/","year":"2019"},{"content":"\nchmod $ chmod -R [777] FOLDER_NAME conda # show the list of envs $ conda info --envs # Create conda ENV using specific python version $ conda create --name myenv python=3.5 # Create conda ENV using the existing ENV $ conda create --name myclone --clone myenv # Remove activated #!/usr/bin/env $ conda remove --name myenv --all # Backup conda env $ conda env export \u0026gt; [filename].yaml # create conda env from yaml file $ conda env create -f [filename].yaml curl # -s, --silent $ curl -s URL # -X, --request \u0026lt;command\u0026gt; Specify request command to use $ curl -X GET URL $ curl -X POST URL envsubst changes ${VARNAME} in string apt-get install gettext-base # put /opt/nginx.conf and redirection output to /etc/ngix... $ envsubst \u0026lt; /opt/nginx.conf \u0026gt; /etc/nginx/conf.d/default.conf kill # kill sends signal to process (-15:quit, -9:force quit) $ kill -15 PID $ kill -9 PID # pkill quit process bt name $ pkill -f PROCESS_NAME ls # the number of directories in working directory $ ls -l | grep ^d | wc -l # the number of files in working directory $ ls -l | grep ^- | wc -l # the number of files in working directory $ ls ./ | wc -l # print all the files in dir ls | gawk \u0026quot;BEGIN {\\\u0026quot;pwd\\\u0026quot; | getline cwd} {printf(\\\u0026quot;%s/%s\\n\\\u0026quot;, cwd, \\$0)}\u0026quot; ps # show all processes ps aux | grep PROCESS_NAME # show the list of process, you can find Process ID using command below $ ps -ef | grep python # show the number of processes $ ps -ef | grep -ic palantir screen # Create new session with name $ screen -S name # Create new session $ Ctrl+a+c # Move to next session $ Ctrl+a+a # Move to previous session $ Ctrl+a+n # quit the current screen session $ Ctrl+d # Detach current screen session $ Ctrl+a+d # Detach screen from remote $ screen -d \u0026lt;SCREENID\u0026gt; # connect to specific screen session using ID $ screen -r \u0026lt;SCREENID\u0026gt; # connect to specific screen session using name $ screen -r name # show the list of screen sessions $ screen -ls # kill the screen session using SCREENID $ screen -X -S SCREENID quit ssh # start ssh connection to remote server $ ssh koreanbear@192.168.0.1 # start ssh connection to remote server using private key $ ssh -i ~/Desktop/key.pem koreanbear@192.168.0.1 # copy test.txt from local PC to remote server $ scp ./test.txt koreanbear@192.168.0.1:~/Desktop $ rsync options source destination tar # tar $ tar -cvf \u0026lt;FILENAME.tar\u0026gt; \u0026lt;DIRNAME\u0026gt; # untar $ tar -xvf \u0026lt;FILENAME.tar\u0026gt; # tar.gz $ tar -zcvf \u0026lt;FILENAME.tar.gz\u0026gt; \u0026lt;DIRNAME\u0026gt; # untar.gz $ tar -zxvf \u0026lt;FILENAME.tar.gz\u0026gt; ","id":34,"section":"Engineering","summary":"chmod $ chmod -R [777] FOLDER_NAME conda # show the list of envs $ conda info --envs # Create conda ENV using specific python version $ conda create --name myenv python=3.5 # Create conda ENV using the existing ENV $ conda create --name myclone --clone myenv # Remove activated #!/usr/bin/env $ conda remove --name myenv --all # Backup conda env $ conda env export \u0026gt; [filename].yaml # create conda env from yaml file $ conda env create -f [filename].","tags":null,"title":"CheatSheet | Linux","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-linux/","year":"2016"},{"content":"\nargparse import argparse if __name__ == \u0026quot;__main__\u0026quot;: parser = argparse.ArgumentParser() parser.add_argument(\u0026quot;--name\u0026quot;, type=str, required=True, help=\u0026quot;help\u0026quot;) args = parser.parse_args() print(args.name) counter from collections import Counter Counter(['apple','red','apple','red','red','pear']) \u0026gt;\u0026gt;\u0026gt; Counter({'red': 3, 'apple': 2, 'pear': 1}) datetime from datetime import datetime datetime.today().strftime(\u0026quot;%Y%m%d%H%M%S\u0026quot;) # YYYYmmddHHMMSS 형태의 시간 출력 flask from flask import jsonify, make_response @application.route('/inference', methods=[\u0026quot;GET\u0026quot;]) def infer(): summary = {'class' : 'cat', 'score':'0.92'} # make response data res = make_response(jsonify(summary), 200) # make Response object res.headers.add(\u0026quot;Access-Control-Allow-Origin\u0026quot;, \u0026quot;*\u0026quot;) # CORS ERROR 대응 return res pandas df = pd.read_pickle('PICKLED_PATH') df.drop(i) # remove i-th row df.sort_values(by, ascending=True) # sort # Filtering df.iloc[[0,1,2,3,4,5]] # get rows by indices # not iloc() =\u0026gt; iloc[] df_new = df.loc[df['Column'].str.contains(\u0026quot;sub_str1|sub_str2\u0026quot;, case=False)] # Filtering rows that contain either sub_str1 or sub_str2 # Groupby agg_functions = {'col1':'first', 'col2' : 'sum', 'col3' : lambda col: ' \u0026amp;\u0026amp; '.join(col), } df_new = df.groupby(df['id']).aggregate(agg_functions) pickle import pickle # load or save object using pickle try: with open(path_pkl, 'rb') as f: obj_pkl = pickle.load(f) except: obj_pkl = [] with open(path_pkl, 'wb') as f: pickle.dump(obj_pkl, f) requests import requests # GET url = 'http://localhost/test' params = {'arg1':'1', 'arg2':'2'} response = requests.get(url=url, params=params).json() # POST response = requests.post(url=url, data=json.dumps(params)) ","id":35,"section":"Engineering","summary":"argparse import argparse if __name__ == \u0026quot;__main__\u0026quot;: parser = argparse.ArgumentParser() parser.add_argument(\u0026quot;--name\u0026quot;, type=str, required=True, help=\u0026quot;help\u0026quot;) args = parser.parse_args() print(args.name) counter from collections import Counter Counter(['apple','red','apple','red','red','pear']) \u0026gt;\u0026gt;\u0026gt; Counter({'red': 3, 'apple': 2, 'pear': 1}) datetime from datetime import datetime datetime.today().strftime(\u0026quot;%Y%m%d%H%M%S\u0026quot;) # YYYYmmddHHMMSS 형태의 시간 출력 flask from flask import jsonify, make_response @application.route('/inference', methods=[\u0026quot;GET\u0026quot;]) def infer(): summary = {'class' : 'cat', 'score':'0.92'} # make response data res = make_response(jsonify(summary), 200)","tags":null,"title":"Python | Snippet","uri":"https://koreanbear89.github.io/engineering/2.-languages/python-snippet/","year":"2016"},{"content":"\n1. Matrix $$ \\begin{pmatrix}1 \u0026amp; 2 \u0026amp; 3 \\4 \u0026amp; 5 \u0026amp; 6 \\7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\begin{bmatrix}a \u0026amp; b \u0026amp; c \\d \u0026amp; e \u0026amp; f \\g \u0026amp; h \u0026amp; i \\end{bmatrix} \\begin{pmatrix} a_{1,1} \u0026amp; a_{1,2} \u0026amp; \\cdots \u0026amp; a_{1,n} \\ a_{2,1} \u0026amp; a_{2,2} \u0026amp; \\cdots \u0026amp; a_{2,n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{m,1} \u0026amp; a_{m,2} \u0026amp; \\cdots \u0026amp; a_{m,n} \\end{pmatrix} $$\n\\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\begin{bmatrix} a \u0026amp; b \u0026amp; c \\\\ d \u0026amp; e \u0026amp; f \\\\ g \u0026amp; h \u0026amp; i \\end{bmatrix} \\begin{pmatrix} a_{1,1} \u0026amp; a_{1,2} \u0026amp; \\cdots \u0026amp; a_{1,n} \\\\ a_{2,1} \u0026amp; a_{2,2} \u0026amp; \\cdots \u0026amp; a_{2,n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m,1} \u0026amp; a_{m,2} \u0026amp; \\cdots \u0026amp; a_{m,n} \\end{pmatrix} ","id":36,"section":"Engineering","summary":"1. Matrix $$ \\begin{pmatrix}1 \u0026amp; 2 \u0026amp; 3 \\4 \u0026amp; 5 \u0026amp; 6 \\7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\begin{bmatrix}a \u0026amp; b \u0026amp; c \\d \u0026amp; e \u0026amp; f \\g \u0026amp; h \u0026amp; i \\end{bmatrix} \\begin{pmatrix} a_{1,1} \u0026amp; a_{1,2} \u0026amp; \\cdots \u0026amp; a_{1,n} \\ a_{2,1} \u0026amp; a_{2,2} \u0026amp; \\cdots \u0026amp; a_{2,n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{m,1} \u0026amp; a_{m,2} \u0026amp; \\cdots \u0026amp; a_{m,n} \\end{pmatrix} $$","tags":null,"title":"LaTeX | Snippet","uri":"https://koreanbear89.github.io/engineering/2.-languages/latex-snippet/","year":"2016"},{"content":"title: \u0026quot;Cheat Sheet | Mac OS Setup\u0026quot; date: 2022-05-22 09:00:13 categories: [2. Linux, Favorites] Introduction 어플리케이션 별 단축키 설정\n시스템 환경설정 \u0026gt; 키보드 \u0026gt; 단축키 \u0026gt; 앱단축키 Automount 해제 diskutil info -all 로 Volume UUID 와 type 확인\nsudo vi /etc/fstab 맨 아래에 아래와 같이 추가\nUUID=0D4EE102-A8C3-31A6-A0BD-C82A702697A2 none apfs rw,noauto UUID=BEC5D911-9473-4C3A-97C7-E68999C9ACFD none apfs rw,noauto ","id":37,"section":"Engineering","summary":"title: \u0026quot;Cheat Sheet | Mac OS Setup\u0026quot; date: 2022-05-22 09:00:13 categories: [2. Linux, Favorites] Introduction 어플리케이션 별 단축키 설정 시스템 환경설정 \u0026gt; 키보드 \u0026gt; 단축키 \u0026gt; 앱단축키 Automount 해제 diskutil info -all 로 Volume UUID 와 type 확인 sudo vi /etc/fstab 맨 아래에 아래와","tags":null,"title":"","uri":"https://koreanbear89.github.io/engineering/9.-others/set-up-mac/","year":"0001"},{"content":"title: \u0026quot;MLCV #13 | Multimodal Representation\u0026quot; date: 2021-12-07 09:00:13 categories: [2. Machine Learning] Learing Transferable Visual Models, CLIP (Contrastive Language Image Pretraining Introduction Traditional CV-DL models are trained to predict a fixed set of pre-determined object categories =\u0026gt; limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. Methods Extract feature representations of each modality (image, text) Joint multi-modal embedding Scaled pair-wise cosine similarities Symmetric loss function Conclusion We have investigated whether it is possible to transfer the success of task-agnostic web-scale pre-training in NLP to another domain. Reference OpenAI CLIP: ConnectingText and Images (Paper Explained) - YouTube , [14:40~] CLIP 은 pretrain 방법의 하나로 NLP 분야에서 BERT 등이 자연어로 pretrain 하여 성능을 대폭 향상시킨 방식으로부터 insight를 얻음 이미지+자연어 데이터셋으로 특정 이미지의 feature $I_f$ 와 대응되는 텍스트의 feature $T_f$ 의 inner product $I_f \\cdot T_f$ 가 최대가 되도록 미리 학습해둔뒤 Inference 할 떄는 추가적인 fine-tuning 없이 test-set의 label들 중에 Inner product가 최대가 되는 label을 선택 # extract feature representations of each modality I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # Joint multimodal embedding [n, d_e] # 아래의 inner-product가 최대가 되도록 W_i, W_t를 학습 I_e = np.dot(I_f, W_i) T_e = np.dot(T_f, W_t) # Scaled pairwise cosine similarities [n,n] logits = np.dot(I_e, T_e.T) # Symmetric loss function loss_i = cross_entropy(Logits, labels, axis=0) loss_t = cross_entropy(logits, labels, axis=1) A Local-to-Global Approach to Multi-modal Movie Scene Segmentation Introduction : Recognizing the movie scenes, including the detection of scene boundaries and the understanding of the scene content, facilitates a wide-range of movie understanding tasks such as scene classification, cross movie scene retrieval, human interaction graph and human-centric storyline construction\nTerminologies :\nshots : captured by a camera that operates for an uninterrupted period of time and thus is visually continuous super-shots : collection of shots, roghly segmented from local (adjacent) features scene : comprises a sequence of shots to present a semantically coherent part of the story a plot-based semantic unit, where a certain activ-ity takes place among a certain group of character often happens in a fixed place, it is also possi-ble that a scene traverses between multiple places continually Methods: solve a binary classification problem to determine whether a shot boundary is a scene boundary, by designing a three-level model to incorporate different levels of contextual information based on the shot representation\nShot Representation, $s_i$ place : ResNet50 pretrained on \u0026ldquo;Places\u0026rdquo; dataset cast : Faster-RCNN pretrained on CIM dataset to detect cast instances and ResNet50 pretrained on PIPA dataset to extract cast features action : TSN pretrained on AVA dataset to get action features audio : NaverNet pretrained on AVA-ActiveSpeaker Dataset to separate speech and background sound and stft Shot Boundary Representation : propose a Boundary Network to model the shot boundary, $b_i = \\Beta(S_{i-w_b}, \u0026hellip; S_{i+w_b})$ BNet takes a clip of the movie with $2w_b$ shots as input and outputs a boundary representation $b_i$ BNet consists of two branches, namely $B_d$ and $B_r$, that calculate differences and relations betweent adjacent shots Coarse Prediction at Segment Level : $\\Tau([b_1, \u0026hellip;, b_{n-1}]) = [p_1, \u0026hellip;, p_{n-1}] $ Bi-LSTM predicts a sequence of coarse score $[p_i,\u0026hellip;]$ from sequence of representatives $[b_i,\u0026hellip;]$ with a threshold, we get roughly classified super-shot boundaries $[\\hat{o_i},\u0026hellip;]$ Global Optimal Grouping : $ \\Gamma([\\hat{o_1},\u0026hellip;,\\hat{o_i}]) = [o_1,\u0026hellip;,o_i] $ rmfdabove local segmentation gives us an initial rough scene cut set. Our goal is to merge these super-shots into scenes Conclusion : this framework is very effective and achieves much better performance than existing methods\nMultimodal Transformer for Unaligned Multimodal Language Sequences (2019) Introduction : two major challenges in modeling multimodal human language time-series data\n(1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; (2) long-range dependencies between elements across modalities. Methods : Multimodal Transformer (MulT) for modeling unaligned multimodal language sequences.\nOverall Architecture\nTemporal Convolutions $\\hat{X}$ : pass the input seq through 1D Conv layer, project the features of different modalities to the same dimension $d$ Positional Embeddings $Z$ : enable the sequences to carry temporal information, add PE to $\\hat{X}$ Crossmodal Transformers : enables one modality for receiving information from another modality, based on the cross-modal attention blocks Crossmodal Attention : consider two modalities $\\alpha, \\beta$, calculate attention score using Query from $\\alpha$ and Key from $\\beta$\n$$ \\text{softmax}(\\frac{Q_\\alpha K^T_\\beta}{\\sqrt{d_k}}) V_\\beta $$\nResult : show that MulT exhibits the best performance when compared to prior methods (Multimodal sentiment analysis, CMU-MOSI \u0026amp; MOSEI)\nSelf-Supervised MultiModal Versatile Network (2020, Deepmind) Introduction : learn representations using self-supervision by leveraging three modalities naturally present in videos: visual, audio and language streams Methods : multimodal versatile network Input : unlabelled videos containing different modalities : RGB stream, audio track, linguistic narrations (Automatic Speech Recognition) MultiModal Versatile Networks : Shared space : all modalities are embedded into a single shared vector space $S_{vat}⊂R^{d_s}$ Disjoint spaces : have different visual-audio $S_{va}$ and visual-text $S_{vt}$ spaces Fine and coarse spaces (FAC) : visual and the audio (fine-grained) domains are different from the language domain (semantically coarse-grained) in terms of their granularities fine-grained : there are many visual or sounds of guitars that might be really different to each other coarse-grained : while the textual domain is more coarse as its goal is to abstract away details (e.g. a single “guitar” word) vision and audio are compared in the fine-grained space ($S_{va}$), while text is compared with vision and audio in the lower dimensional coarse-grained space ($S_{vat}$). since the text modality is directly obtained from the audio track using ASR, we do not construct the audio-text space nor the loss that puts them in alignment explicitly Multimodal Contrastive Loss : we construct self-supervised tasks which aim to align pairs of modalities. positive training pairs across two modalities are constructed by sampling the two streams from the same location of a video. negative training pairs are created by sampling streams from different videos. Video to image network deflation Results : exceeds the state-of-the-art for action and audio classification on five challenging benchmarks: HMDB51, UCF101, Kinetics600, ESC-50 and AudioSet VATT : Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text (2021) introduction : present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures\nMethods :\n(1) Tokenization and Positional Encoding : define a modality-specific tokenization layer\nvideo : 3D extension of the patching machanism, linear projection on the entire voxels. audio : the raw audio waveform is a 1D input with length $T\u0026rsquo;$, and we partition it to $[T\u0026rsquo;/t\u0026rsquo;]$ segments each containing $t\u0026rsquo;$ waveform amplitudes and then apply a linear projection with a learnable weight $W\\in\\mathbb{R^{t\u0026rsquo; \\times d}}$ to get a $d$ dimensional vector representation. text : construct a vocabulary of size $v$ and map each word to a $v$-dimensional one-hot vetor followed by a linear projection with a learnable weight $W \\in \\mathbb{R}^{v \\times d}$ Drop Token : a simple and yet effective strategy to reduce the computational complexity during training. The Transformer Architecture : we do not tweak the architecture\nCommon Space Projection\nVideo-audio : maps the video and audio Transformers’ outputs to the video-audio common space $S_{va}$ using Linear Projection video-text : the text Transformer’s outputs and the video embedding in the $S_{va}$ space to video-text common space $S_{vt}$ using Linear Projection Multimodal Contrastive Learning\nvideo-audio : Noise Contrastive Estimaiton to align video-audio pairs.\nvideo-text : Multiple Instance Learning NCE to align to video-text pairs.\noverall per-sample objective : for training the entire model end-to-end is as follows:\n$$ L=NCE(z_{v,va},z_{a,va}) +λ MILNCE(z_{v,vt},[z_{t,vt}]),\n$$\nResults :\nsuggests that Transformers are effective for learning semantic video/audio/text representations even if one model is shared across modalities and multi-modal self-supervised pre-training is promising for reducing their dependency on large-scale labeleddata. Video Action recognition (UCF101, HMDB, Kinetics) Audio Event Classification (ESC50, AudioSet) text to video retrieval (MSR-VTT) CLIPBERT for Video-and-Language Learning via Sparse Sampling Introduction CLIPBERT : enables end-to-end learning for video-and-language tasks, by employing sparse sampling ","id":38,"section":"Research","summary":"title: \u0026quot;MLCV #13 | Multimodal Representation\u0026quot; date: 2021-12-07 09:00:13 categories: [2. Machine Learning] Learing Transferable Visual Models, CLIP (Contrastive Language Image Pretraining Introduction Traditional CV-DL models are trained to predict a fixed set of pre-determined object categories =\u0026gt; limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much","tags":null,"title":"","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml11-multimodal-representation/","year":"0001"}],"tags":[]}