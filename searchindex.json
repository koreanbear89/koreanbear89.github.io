{"categories":[{"title":"1. CheatSheet","uri":"https://koreanbear89.github.io/categories/1.-cheatsheet/"},{"title":"1. CheatSheets","uri":"https://koreanbear89.github.io/categories/1.-cheatsheets/"},{"title":"1. Linear Algebra","uri":"https://koreanbear89.github.io/categories/1.-linear-algebra/"},{"title":"2. Languages","uri":"https://koreanbear89.github.io/categories/2.-languages/"},{"title":"2. Linux","uri":"https://koreanbear89.github.io/categories/2.-linux/"},{"title":"2. Machine Learning","uri":"https://koreanbear89.github.io/categories/2.-machine-learning/"},{"title":"2. Statistics","uri":"https://koreanbear89.github.io/categories/2.-statistics/"},{"title":"3. Computer Vision","uri":"https://koreanbear89.github.io/categories/3.-computer-vision/"},{"title":"3. Mathematics for ML","uri":"https://koreanbear89.github.io/categories/3.-mathematics-for-ml/"},{"title":"5. Natural Language","uri":"https://koreanbear89.github.io/categories/5.-natural-language/"},{"title":"9. Study","uri":"https://koreanbear89.github.io/categories/9.-study/"},{"title":"Favorites","uri":"https://koreanbear89.github.io/categories/favorites/"}],"posts":[{"content":"\n1. Introduction and Motivation Machine learning is about designing algorithms that automatically extract valuable information from data. There are three concepts that are at the core of machine learning : data, a model, and learning. Data : Since machine learning is inherently data driven, data is at the core of machine learning. Model : would describe a function that maps inputs to real-valued outputs. Learning : can be understood as a way to automatically find patterns and structure in data by optimizing the parameters of the model 1.1 Finding Words for Intuitions Data as vectors : there are (at least) three different ways to think about vectors: a vector as an array of numbers (computer science view), a vector as an arrow with a direction and magnitude (physics view), a vector as an object that obeys addition and scaling (a mathematical view) Model : A good model can be used to predict what would happen in the real world without performing real-world experiments. Learning : We learn from available data by using numerical optimization methods with the aim that the model performs well on unseen data. 1.2 Two Ways to Read This Book Bottom-up : Building up the concepts from foundational to more ad-vanced. Top-down : Drilling down from practical needs to more basic requirements Part I is about Mathematics : linear algebra : The study of vectors and matrices analytic geometry : the construction of similarity and distances matrix decomposition : Some operations on matrices are extremely useful in ML probability theory : Quantification of uncertainty vector calculus : details concept of gradients optimization : to find maxima/minima of functions Part II is about Machine Learning : linear regression ; to find functions that map inputs $x$ to corresponding observed function values $y$, model fitting via MLE and MAP. dimensionality reduction : to find a compact, lower-dimensional representation of high-dimensional data $x$. density estimation : to find a probability distribution that describes a given dataset. We will focus on Gaussian mixture models for this purpose, and we will discuss an iterative scheme to find the parameters of this model. classification : unlike regression, where the labels were real-valued, the labels in classification are integers, which requires special care. ","id":0,"section":"Mathematics","summary":"1. Introduction and Motivation Machine learning is about designing algorithms that automatically extract valuable information from data. There are three concepts that are at the core of machine learning : data, a model, and learning. Data : Since machine learning is inherently data driven, data is at the core of machine learning. Model : would describe a function that maps inputs to real-valued outputs. Learning : can be understood as a way to automatically find patterns and structure in data by optimizing the parameters of the model 1.","tags":null,"title":"Mathematics for ML #1 | Introduction Part.I ","uri":"https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml01-introduction/","year":"2022"},{"content":"\nIntroduction Tasks:\nImage Classification : The task of classifying an image according to its visual content.\nImage Representation : focus on the way to encode visual contents into vectors (embedding, encoding)\n1. AlexNet (2012) Introduction : CNNs have been prohibitively expensive to apply in large scale to high resolution images. Method : Training on Multiple GPUs def AlexNet(x): out = MP(relu(conv11x11(x))) out = MP(relu(conv5x5(out))) out = relu(conv3x3(out)) out = relu(conv3x3(out)) out = MP(relu(conv3x3(out))) out = FC(relu(FC(relu(FC(out))))) return out 2. VGG Net (2014) Introduction : come up with significantly more accurate ConvNet Method : deeper ConvNet def VGG16(x): out = MP(conv3x3(conv3x3(x))) out = MP(conv3x3(conv3x3(out))) for i in range(3): out = MP(conv3x3(conv3x3(conv3x3(out)))) out = softmax(FC(FC(FC(out)))) return out 3. GoogleNet (2015) Introduction : efficient deeper networks (with fewer params than AlexNet) Method : inception module(NIN, Bottleneck) def inception_block(x): branch_1x1 = conv1x1(x) branch_3x3 = conv3x3(conv1x1(x)) branch_5x5 = conv5x5(conv1x1(x)) branch_pool = conv1x1(MP3x3(x,same)) out = concat([branch_1x1,branch_3x3,branch_5x5,branch_pool]) return out 4. ResNet (Microsoft, 2015) Introduction : to solve the degradation problem caused by deeper layer. Method : Residual Block with shortcut(skip) connection defined as : $$ \\mathbf{x}_{l+1} = \\mathbf{x}_l + F(\\mathbf{x}_l,{W_i}) $$\ndef residual_block(x): out = relu(bn1(conv3x3(x))) out = relu(bn2(conv3x3(out)) + x) return out 5. DenseNet (2016) Introduction : information about the input or gradient can vanish and wash out as CNNs become deep Method : Dense Connectivity (not sum, just concat) Result : 77.85% of top-1 accuracy in ImageNet $$ x_l = H_l([x_0, x_1, \u0026hellip; , x_{l-1} ]) $$\n$[x_0, x_1, \u0026hellip; , x_{l-1}]$ means concatenation of the features-maps produced in previous layers.\ndef dense_block(x): out = conv1x1(relu(bn1(x))) # Bottleneck for comput. efficiency out = conv3x3(relu(bn2(out))) out = concat([x, out]) return out 6. ResNeXt (2016) Introduction : present a improved architecture that adopts ResNets strategy of repeating layers. Method : split-transform-merge strategy (cardinality) Result : 80.9% of top-1 accuracy in ImageNet with 83.6M params $$ \\mathbf{x}_{l+1} = \\mathbf{x}l + \\sum{i=1}^{cardin} F_i(\\mathbf{x}_l) $$\ndef residual_block(x): out = relu(bn1(conv3x3(x, groups=cardinality))) out = relu(bn2(conv3x3(out, groups=cardinality)) + x) return out 7. ShuffleNet(2017) Introduction : extremely computation-efficient CNN architecture named ShuffleNet, designed specially for mobile devices with very limited computing power.\nMethods : utilizes two new operations, pointwise group convolution and channel shuffle\ndivide the channels in each group into several subgroups\nfeed each group in the next layer with difference subgroup\n8. FixResNeXt (2019) Introduction : Existing augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time. Method : Simple strategy to optimize the classifier performance, that employs different train and test resolution : in face, a lower train resolution improves the classification at test time. Result : 86.4% of top-1 accuracy in ImageNet with 83.6M params (L) conventional augmentation method (R) proposed augmentation method 9. ViT : An Image is Worth 16x16 Words (2020, GoogRes) Introduction :\nTransformer architecture has become the de-facto standard for natural language processing tasks In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. Methods : applying a standard Transformer directly to images\nPatch Embedding $x_i$ : extracts N non-overlapping image patches, performs a linear projection ($E$, is equivalent to a 2D conv) and then rasterises them into 1D token.\nlearnable embedding $z_{cls}$ : an optional learned clasification token (Similar to BERT\u0026rsquo;s [cls]) is prepended to the sequene of embedded patches\nlearnable position embedding $p$ : added to the tokens to retain positional information,\n=\u0026gt; When you have no idea about how to hand-craft positional encoding for your data\n=\u0026gt; Let the transformer figure out for itself what it needs as positional embeddings\n=\u0026gt; simply train the vectors in table of figure at \u0026ldquo;NLP3 \u0026gt; Transformer \u0026gt; Binarized Indexing\u0026rdquo;\n$$ \\mathbf{z} = [z_{cls}, E_{x_1}, E_{x_1}, \u0026hellip;, E_{x_1}] + \\mathbf{p}\n$$\nResult:\nWhen trained on mid-sized datasets such as ImageNet(1.3M) without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size However, the picture changes if the models are trained on larger datasets (JFT-300M, Figure3) 10. VirTex : Learning Visual Representations from Textual Annotations introduction : revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. (1) Semantic density : Captions provide a semantically denser learning signal than unsupervised contrastive methods and supervised classification. (2) simplified data collection : natural language descriptions do not require an explicit ontology and can easily be written by non-expert workers, VirTex : a pretraining approach using semantically dense captions to learn visual representations (1) jointly train a ConvNet and Transformer from scratch to generate natural language captions for images Visual Backbone : a convolutional network which computes visual features of image Textual Head : receives features from thevisual backbone and predicts captions for images (2) transfer the learned features to downstream visual recognition tasks Result show that natural language can provide supervision for learning transferable visual representations with better data-efficiency than other approaches. VirTex matches or exceeds the performance of existing methods for supervised or unsupervised pre-training on ImageNet, despite using up to 10×fewer images ","id":1,"section":"Research","summary":"Introduction Tasks: Image Classification : The task of classifying an image according to its visual content. Image Representation : focus on the way to encode visual contents into vectors (embedding, encoding) 1. AlexNet (2012) Introduction : CNNs have been prohibitively expensive to apply in large scale to high resolution images. Method : Training on Multiple GPUs def AlexNet(x): out = MP(relu(conv11x11(x))) out = MP(relu(conv5x5(out))) out = relu(conv3x3(out)) out = relu(conv3x3(out))","tags":null,"title":"MLCV #1 | Image Classification","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv01-image-classification/","year":"2016"},{"content":"\nIntroduction Tasks\nObject Detection : a task of finding the different objects in an image and classifying them Salient Object Detection : a task based on a visual attention mechanism, in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or RGB images. Metrics:\nAP or mAP is generally used as the primary metrics metric.click here for details Others\nNon Maximum Suppression (NMS) :\n(1) get multiple bbox for each object\n(2) Leave the most confident bbox and remove other bboxes which has high IoU with it\n1. R-CNN (2013) Introduction : An early application of CNNs to Object Detection tasks\nMethod\nRegion Proposals : Generate a set of proposals ($n=2000$) for bounding boxes using selective search algorithm.\nResize Regions : resize ROI patches to 224x224 for pretrained AlexNet\nClassification : Run the images in b-boxes through a pre-trained AlexNet and SVM to see what object the image in the box is.\nb-box regressor : Run the b-box through a linear regression model to output tighter coordinates\ncf. Selective search looks at the images through windows of different sizes and for each window, tries to group together adjacent pixels by texture, color, or intensity to identify objects\n2. Fast R-CNN (2015) Introduction : RCNN was quite slow because of 2000 (number of Region patches) forward passes per image (for all proposed regions). And also it need to train three different models separately (CNN, SVM, regression). Fast-RCNN tried to solve these problems.\nMethod\npretrained CNN : get top-conv feature map using pretrained CNN\nRegion Proposal : just get ROI coordinates from input image using selective search, and does not make image patches.\nROI pooling : conv-features for each proposed ROI are obtained by selecting a corresponding region from the feature map of input image instead of running CNN for every ROI patches. Then, these conv-features are pooled adaptively.\nClassification \u0026amp; B-Box regressor\n3. Faster R-CNN (2016) Introduction : There was still one remaining bottleneck in the Fast R-CNN : the region proposer based on selective search.\nMethod : adds a Fully Convolutional Network which is called Region Proposal Network between the top-conv feature map and ROI pooling. The RPN slides a window over the top-conv features. At each window location, the network ouputs a score and a bbox per anchor.\nPretrained CNN : Run the image through a CNN to get a (top-conv) feature-map. (returns 14x14x512)\nRegion Proposal Network: slide a small conv-net over the extracted feature-map which maps the input window to lower-dimensional feature (256-dim). Then this lower-dim feature is fed into two sibling FC layers, one for box-classification and the other for box-regression.\n2.1 Classifier : returns 14x14x9x2 (9 for anchor and 2 for object/background)\n2.2 Box Regressor : returns 14x14x9x4 (9 for anchor and 4 for dx, dy, w, h)\nROI Pooling \u0026amp; Classification : same as faster RCNN\nFigure 3. Architecture of Region Proposal Network (RPN) 4. Mask R-CNN (2017) Introduction : to detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance.\nMethod : just add a third branch that outputs the object mask.\nFeature Extraction is same as Faster RCNN\nRPN is same as Faster RCNN\nThe 3rd tails outputs (class + box offset + a binary mask) for each ROI in parallel.\n3.1 : Class labels are collapsed into a short output vectors by FC layers, same as Faster RCNN 3.2 : Box offset is collapsed into a short output vectors by FV layers, same as Faster faster_RCNN 3.3 : $m \\times m $ masks are predicted for each ROI using an FCN ROI Align : If we use ROI pool at the above process, there would be small difference between the real ROI and extracted feature map. It does not matter in classification task, but does in segmentation. To address this problem, authors proposed ROI Align.\n5. SpineNet (2020) Introduction : In past few years, most networks follow the design that encodes input image into intermediate features with monotonically decreased resolutions. Most improvements of network architecture design are in adding network depth and connections within features resolution group.\nAuthors demonstrate that SpineNet can also be used as backbone model in Mask-RCNN Detector and improve both box detection and instance segmentation\n","id":2,"section":"Research","summary":"Introduction Tasks Object Detection : a task of finding the different objects in an image and classifying them Salient Object Detection : a task based on a visual attention mechanism, in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or RGB images. Metrics: AP or mAP is generally used as the primary metrics metric.click here for details Others Non Maximum Suppression","tags":null,"title":"MLCV #2 | Object Detection","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv02-object-detection/","year":"2016"},{"content":"\nSummary Language Modeling : is the task of predicting the next word or character in a document that can be used in downstream tasks like:\nMachine Translation : By comparing two sentences with a language model, and return the more natural sentence\nSpell Correction : Correct \u0008spelling by choosing a more natural vocabulary\nSpeech Recognition : Correct the recognition result with more natural words\n1. Seq2Seq Learning with Neural Networks (2014) Introduction : DNNs work well but they cannot be used to map sequences to sequences. Authors present a general end-to-end approach to sequence learning. Method : Simply using a multilayered LSTM to map the input sequence to a fixed dim vector (context vector) and then another deep LSTM to decode the target sequence from the vector 2. Attention : Neural Machine Translation by Jointly Learning to Align and Translate (2014) Introduction : A potential issue with the encoder-decoder approach is that a NN needs to be able to compress all the necessary information of a source sentence into a fixed-length vector (context vector).\nMethod : Rather than using fixed-length context vector (last hidden state value of encoder), we can use encoder\u0026rsquo;s each state with current state to generate dynamic context vector\nAttention Weight : Simply adding (FC + Softmax) to encoder output, we can get weight values of each input words for output at time step $t$. Dynamic Context Vector : We can get context vector for each timestep $c_t$ by calculating weighted sum of $h_i$ (hidden state of encoder) and $s_i$ (attention weight, softmax). Teacher Forcing : If model made wrong prediction $y_t$ for time step $t$. This causes the model to be trained in the wrong way for time step $t+1$. So we passed GT as the nex/t input for timestep $t$ to decoder rather than wrong prediction. 3. Transformer : Attention Is All You Need (2017) Introduction : propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely\nMethod : Self-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\nEncoder : Self Attention Layer (Scaled Dot-product Attention) + Feed Forward NN\nWord Embedding : word to 512-dim vector Generate key, query, value vector of $i_{th}$ word : $q_i, k_i, v_i$ by simply multiplying trainable weights, $W_K, W_Q, W_V$ Attention Score, $softmax(s_{ij})$ : by calculating $s_{ij} = q_i \\cdot k_j$ we can get score between $i_{th}$ word and other $j_{th}$ words. and scale the dot products by $\\frac{1}{\\sqrt{d}}$ We suspect that for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients Weighted Sum : $output_1 = S_{11} v_1 + S_{12}v_2 + S_{13}v_3\u0026hellip;$ Decoder : Almost same architercture with Encoder\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence . This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation. above diffrence makes attention score changed like this : $s_{ij} = q_i \\cdot k_j (i\u0026lt;j)$ The decoder stack outputs a vector of floats. And we turn that into a word by the final Linear layer which is followed by a Softmax Layer. Where does query, key, value comes from :\nThe key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values). The attention operation can be thought of as a retrieval process as well. Positional Encoding (fixed, different from positional embedding) : no recurrence and no convolution in model, in order for the model to make use of the order of the sequence =\u0026gt; we must inject some information about the relative or absolute position of the tokens in the sequence.\nSimple indexing : just created a new vector where every entry is its index number. =\u0026gt; exploding gradients, unstable training\nNormalized indexing : Just divide everything by the largest integer so all of the values are in [0,1]\nbinarized indexing : Instead of writing say 35 for the 35th element, we could instead represent it via its binary form 100011. =\u0026gt; Our binary vectors come from a discrete function, and not a discretization of a continuous function\nSinusoidal Positional Encoding : find a way to make the binary vector a discretization of something continuous. (Vanilla transformer)\nLearnable Positional Embedding : train the vectors in figure of binarized indexing (Vision Transformer)\nReferences:\nneural networks - What exactly are keys, queries, and values in attention mechanisms? - Cross Validated Transformer said, Attention is all you need. Master Positional Encoding : Part1 Self-Attention Layer Handcrafted Positional Encoding ","id":3,"section":"Research","summary":"Summary Language Modeling : is the task of predicting the next word or character in a document that can be used in downstream tasks like:\nMachine Translation : By comparing two sentences with a language model, and return the more natural sentence\nSpell Correction : Correct \u0008spelling by choosing a more natural vocabulary\nSpeech Recognition : Correct the recognition result with more natural words\n1. Seq2Seq Learning with Neural Networks (2014) Introduction : DNNs work well but they cannot be used to map sequences to sequences.","tags":null,"title":"NLP #3 | Language Modeling ","uri":"https://koreanbear89.github.io/research/5.-natural-language/nlp-3-language-modeling/","year":"2020"},{"content":"Introduction Tasks:\nImage Segmentation : The process of assigning a label to every pixel in the image.\nSemantic Segmentation : treats multiple objects of the same class as a single entity.\nInstance Segmentation : treats multiple objects of the same class as distinct individual objects.\n1. FCN (2015) Introduction : The first end-to-end pixel-wise prediction model based only on convolutional layers.\nMethod:\nFeature Extraction : using convolution layers like conventional Image Classification Tasks (layer 1,2,3,4,5) Convolutionalizing : Downsampling using 1x1 conv rather than FC layer(layer 6,7,8) Pixel Wise Classification : Last conv1x1 layer performs pixel wise classification for 21 classes. Upsampling : using deconvolution layer, also called transposed convolution Fusing Output : x32 upsample from pool5 (FCN-32S) + x16 upsample from pool4 (FCN16S) + x8 upsample from pool3 (FCN8S) Figure1. Overview of FCN Architecture Figure2. Overview of upsampling process 2. Mask R-CNN (2017) Introduction : to detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance.\nMethod : just add a third branch that outputs the object mask.\nFeature Extraction is same as Faster RCNN\nRPN is same as Faster RCNN\nThe 3rd tails outputs (class + box offset + a binary mask) for each ROI in parallel.\n3.1 : Class labels are collapsed into a short output vectors by FC layers, same as Faster RCNN 3.2 : Box offset is collapsed into a short output vectors by FV layers, same as Faster faster_RCNN 3.3 : $m \\times m $ masks are predicted for each ROI using an FCN ROI Align : If we use ROI pool at the above process, there would be small difference between the real ROI and extracted feature map. It does not matter in classification task, but does in segmentation. To address this problem, authors proposed ROI Align.\n","id":4,"section":"Research","summary":"Introduction Tasks: Image Segmentation : The process of assigning a label to every pixel in the image. Semantic Segmentation : treats multiple objects of the same class as a single entity. Instance Segmentation : treats multiple objects of the same class as distinct individual objects. 1. FCN (2015) Introduction : The first end-to-end pixel-wise prediction model based only on convolutional layers. Method: Feature Extraction : using convolution layers like conventional","tags":null,"title":"MLCV #3 | Semantic Segmentation","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv03-image-segmentation/","year":"2017"},{"content":"\n0. Introduction Tasks : Image Synthesis : The task of creating new images from some form of image description. 1. GAN (2014) Introduction : A new framework for estimating generative models via an adversarial process\nMethod: simultaneously train two models : a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$.\n$$ \\min_{G} \\max_{D} V(D,G) = \\mathbb{E} _ {x \\sim p_{data}(x)} logD(x) + \\mathbb{E} _ {z \\sim p_z(z)}log(1-D(G(z))) $$\nin terms of Discriminator : $D(x)$ should be 1 and $D(G(z))$ should be 0. So, $D$ is trained to maximize both left and right terms\nin terms of Generator : Left term can be ignored since it\u0026rsquo;s independent of $G$, and $D(G(z))$ should be 1. So, $G$ is trained to minimize right terms.\nin terms of implementation : We need to make two optimizer. Also G_loss and D_loss should be defined respectively.\n2. Conditional GAN (2014) Introduction : The conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y.\nMethod : feeding $y$ into the both the discriminator and generator as additional input layer.\n$$ \\min_{G} \\max_{D} V(D,G) = \\mathbb{E} _ {x \\sim p_{data}(x)} [logD(x,y)] + \\mathbb{E _ {z \\sim p_z(z)}[log(1-D(G(z,y),y))].} $$\ndef generator(x,y): input = concat([x,y],1) layer1 = relu(FC(input, 128)) layer2 = tanh(FC(layer1, 784)) return layer2 def discriminator(x,y): input = concat([x,y],1) layer1 = lrelu(FC(input, 128)) layer2 = tanh(FC(layer1, 1)) return layer2 3. DCGAN (2016) Introduction : Convolutional GANs that make them stable to train in most settings.\nMethod : Following techniques were used for stable Deep Conv GANS.\nReplace any pooling layers with strided convolutions (discriminator) and fractional strided convolutions (generator) Use batchnorm in both the generator and the discriminator Remove fully connected hidden layers for deeper architectures Use ReLU for all layers except for the output which uses Tanh Use LeakyReLU in discriminator for all layers 4. BEGAN (2017) Introduction : A new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based GAN.\nMethod\nuse an auto-encoder as a discriminator as was first proposed in EBGAN. aims to match auto-encoder loss distributions using a loss derived from the Wasserstein distance while typical GANs try to match data distributions directly. This is done using a typical GAN objective with the addition of an equilibrium term to balance the discriminator and the generator. $$ L_D = L(x) - k_t L(G(z_d)) $$\n$$L_G = L(G(z_G))$$\n$$ k_{t+1} = k_t + \\lambda (\\gamma L(x) - L(G(z_G)))$$\n5. GIRAFFE (2021) Introduction : Deep generative models allow for photorealistic image synthesis at high resolutions content, But this is not enough =\u0026gt; creation also needs to be controllable with 3D representation.\nOur key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. GIRAFFE : generating scenes in a controllable and photorealistic manner without additional supervision\n(3.1) model Objects as neural feature fields :\nNeRF (Neural Radiance Fields) : a function f that maps 3D point and viewing direction to a volume density and RGB color value. Input : single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, φ)), Output : the volume density and view-dependent emitted radiance at that spatial location 2D images from different view =\u0026gt; Neural Network =\u0026gt; random view image GRAF (Generative Neural Feature Fields) : unsupervised version of NeRF, trained from unposed image collections with additional latent vector. input : spatial location $\\gamma(x)$, viewing direction, $\\gamma(d)$, shape code $z_s$, appearance code $z_a$ GIRAFFE : replace 3D color output with M-dimensional feature from GRAF represent each object using a separate feature field in combination with an affine transformation (3.2) Scene Compositions\nwe describe scenes as compositions of N entities where the first N−1 are the objects in the scene and the last represents the background (3.3) Scene Rendering\n3D Volume Rendering : For given camera extrinsics, maps above evaluations to the pixel\u0026rsquo;s final feature vector\n2D Neural Rendering : maps the feature image to the final synthesized image (2D CNN)\nResult : By representing scenes as compositional generative neural feature fields, we disentangle individual objects from the background as well as their shape and appearance without explicit supervision. Combining this with a neural renderer yields fast and controllable image synthesis.\n*disentangle : commonly refer to being able to control an attribute of interest, e.g. object shape, size, or pose, without changing other attributes. ","id":5,"section":"Research","summary":"0. Introduction Tasks : Image Synthesis : The task of creating new images from some form of image description. 1. GAN (2014) Introduction : A new framework for estimating generative models via an adversarial process\nMethod: simultaneously train two models : a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$.","tags":null,"title":"MLCV #4 | Image Synthesis","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv04-image-synthesis/","year":"2017"},{"content":"\nSummary Text Segmentation : is the process of dividing written text into meaningful units, such as words, sentences, or topics.\nTo improve the results of Information Retrieval system and help users to find relevant passages faster keywords : Text Segmentation, Document Segmentation, Discourse Segmentation Methods\nLexical-Cohesion based methods : The first basic insight is that people talk about different topics in different ways: they use different words.\nDiscriminative approach : calculate similarity between neighboring sentences Hearst. TextTiling, 1997 , Alemi and Ginsparg, 2015, and Glavaˇs et al., 2016 Clustering approach : generate topic clusters Freddy. C99, 2000 Feature based Topic Boundaries (Break) Prediction : The second basic insight is that boundaries between topics have their own characteristic features\nUse neural models to identify break points within the text : Solve Binary classification problem (whether a given sentence denotes the beginning of a new segments) Sehikh et al. 2017, (p.1), Koshorek et al. 2018 and Badjatiya et al. 2018, (p.3) 1. Conventional Ideas : Application of Topic Segmentation in Audiovisual information retrieval (2012) Introduction : Overview of Conventional approaches in Topic Segmentation Methods : present several methods used for topic segmentation, based on textual, audio and visual information. Lexical Cohesion based : use only textual information. They are based on the assumption that the segment we are looking for is lexically coherent (it uses coherent vocabulary) Feature based (hand-crafted) Textual Features : Lexical Features, Contextual Features, Vocabulary, Lexical Chains Audio Features : Prosodic Features and Converational Features Video Features : Color Similarity, Motion Similarity, and Bag of Visual words Conclusion : would like to use all kinds of modality in the proposed system. Therefore, machine learning method based on the features appears to be the best solution. 2. Topic segmentation in ASR Transcript using Bidrectional RNNs for Change Detection (2017) Introduction : propose a novel approach for topic segmentation in speech recognition transcripts by measuring lexical cohesion using bidirectional Recurrent NeuralNetworks (RNN).\nMethods : Bidirectional Recurrent Neural Networks\nTwo or more news articles are randomly chosen and concatenated Then the training objective is to mark the boundary be-tween the concatenated articles as a topic change point. Conclusion : These models were trained discriminatively by concatenating news articles from the internet. Evaluation on ASR transcripts of French TV news programs showed that our RNN models can perform better than the C99-LSAand Topic Tiling baseline methods.\n3. Text Segmentation as a Supervised Learning Task (2018, Wiki-727) Introduction :\nPrevious work on text segmentation focused on unsupervised methods such as clustering or graph search, due to the paucity in labeled data.\nIn this work, we formulate text segmentation as a supervised learning problem, and present a large new dataset for text segmentation that is automatically extracted and labeled from Wikipedia.\nMethods\nWiki-727k Dataset : 727k English Wikipedia documents\nRemoved all photos, tables, Wikipedia template elements, and other non-text elements.\nRemoved single-sentence segments, documents with less than three segments, and documents where most segments were filtered.\nDivided each segment into sentences using the PUNKT-tokenizer of the NLTK library (Bird et al., 2009). This is necessary for the use of our dataset as a benchmark, as without a well-defined sentence segmentation, it is impossible to evaluate different models\nNeural Model for Text Segmentation : two sub-networks based on the LSTM architecture. One generates sentence representations, and the other predicts segment\nConclusion : Our text segmentation model outperforms prior methods on Wikipedia documents, and performscompetitively on prior benchmark\n4. Attention based Neural Text Segmentation (2018) Introduction : This paper is the first one to present a novel supervised neural approach for text segmentation.\nMethods : solve binary classification problem,\nProblem Definition : Classifies whether a given sentence denotes the beginning of a new text segments. Model : using attention based bidirectional LSTM Code : https://github.com/pinkeshbadjatiya/neuralTextSegmentation\n5. Text Segmentation by cross segment attention (2020, Google Research) Introduction : Text segmentation is a traditional NLP task that breaks up text into constituents.\nDocument Segmentation : has been shown to improve information retrieval by indexing sub-document units instead of full documents (Llopiset al., 2002; Shtekh et al., 2018)\nDiscourse Segmentation : breaks up pieces of text into sub-sentence elements (EDUs, Fig2)\nMethods : propose three transformer-based architectures\nPreprocessing : simply feed the raw input into a word-piece (sub-word) tokenize (Wu et al., 2016)\nArchitecture\nCross-segment BERT : uses only local context around candidate breaks. find appropriate location for [SEP] token\nBERT+BiLSTM : encode each sentence using a BERT model, and then feed the sentence representations into a Bi-LSTM\nHierarchical BERT : encode each sentence using BERT and then feed the output sentence representations in another transformer-based model.\nDataset : Wiki-727k contains 727 thousand articles from a snapshot of the English Wikipedia,\nre-use the original splits provided by the authors Conclusion : In particular, we found that a cross-segmentBERT model is extremely competitive. This is surprising as it suggests that local context is sufficient in many cases\nReferences\nword-piece tokenizer implementation (Devlin et al., 2018), which has a vocabulary size of 30,522 word-pieces. 6. Chapter Captor : Text Segmentation in Novels (2020) Introduction : investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts\nContributions:\n(1) Project Gutenberg Chapter Segmentation Resource : Create a ground-truth data setfor chapter segmentation\n(2) Local Methods for Chapter Segmentation :\nunsupervised weighted-cut approach minimizing cross-boundary cross-references\nsupervised neural network building on the BERT language model (Devlinet al., 2019)\n(3) Global Break Prediction using Optimization\naugmenting the BERT-based local classifier with dynamic programming References\nhttps://github.com/cpethe/chapter-captor.arXiv:2011.04163v1 7. Improving Context Modeling in Neural Topic Segmentation (2020) Introduction :\nTopic segmentation can be framed as a sequence labeling task where each sentence is either the end of a segment or not. For topic segmentation, it is critical (important) to supervise the model to focus more on the local context. with a proper way of modeling the coherence between adjacent sentences, a topic segmenter can be further enhanced. Methods: given a document represented as a sequence of sentences, model will predict the binary label for each sentence to indicate if the sentence is the end of a topical coherent segment or not.\nadd a coherence-related auxiliary task : to make model learn more informative hidden states for all the sentences in a document restricted self-attention : which enables model to pay attention to the local context and make better use of the information from the closer neighbors of each sentence Discussion\nDomain Transfer : trained on WIKI-SECTION dataset, evaluated on WIKI-50, which consists of 50 samples randomly generated from the latest English Wikipedia dump, Multilingual Evaluation : trained and tested on two other wikipedia datasets in German and Chinese : SECTION-DE, SECTION-ZH 8. Unsupervised Topic Segmentation of Meetings with BERT Embeddings (2021) Introduction : In the context of meeting recordings and their transcripts, topic segmentation can quickly provide users with a valuable high level understanding of past meetings. Topic segmentation of spoken language is significantly more challenging than written text due to the added complexity that the underlying ASR (Automated Speech Recognition) system Methods : detect topic changes based on a new similarity score using BERT embeddings. Sentence representation model : to extract semantic similarity between sentences (BERT) A segmentation scheme : that employs semantic similarity variations over time to detect topic changes. Conclusion : leveraging the strong semantic representation power of BERT, prposed model shows improved segmentation performance compared to the non neural-based approach ","id":6,"section":"Research","summary":"Summary Text Segmentation : is the process of dividing written text into meaningful units, such as words, sentences, or topics.\nTo improve the results of Information Retrieval system and help users to find relevant passages faster keywords : Text Segmentation, Document Segmentation, Discourse Segmentation Methods\nLexical-Cohesion based methods : The first basic insight is that people talk about different topics in different ways: they use different words.\nDiscriminative approach : calculate similarity between neighboring sentences Hearst.","tags":null,"title":"NLP #5 | Text Segmentation","uri":"https://koreanbear89.github.io/research/5.-natural-language/nlp-5-text-segmentation/","year":"2021"},{"content":"0. Introduction Tasks : Image Style Transfer : The task of migrating a style from one image (Style Image) to another (Content Image). 1. Image Style Transfer using CNNs (2016) Introduction : Introduce a algorithm that can separate and recombine the image content and style of natural images.\nMethod : Extract feature maps $F_l$ from each input image $I_{content} $ and $I_{style}$ using pretrained networks at $l_{th}$ layer. Then, optimize $I_{output}$ to have similar contents with $I_{content}$ and similar style with $I_{style}$.\nThe content loss between $I_{content}$ and $I_{output}$ is calculated using Frobenius norm at $l_{th}$ layer :\n$$L_{content} = \\Sigma(F_{output} - F_{content})^2$$\nThe style loss between $I_{style}$ and $I_{output}$ at $l_{th}$ layer is calculated using Frob. norm and Gram matrix. The style loss is defined by weighted sum of $L_{style}^l$ :\n$$L_{style} = \\sum w_l \\cdot L_{style}^l = \\sum w_l (\\sum(Gram(F_{output}) - Gram(F_{style})))$$\nThe final obejective function is defined as :\n$$ L_{total} = \\alpha L_{content} + \\beta L_{style}$$\n2. pix2pix (2016) Introduction : Conditional adversarial networks as a general-purpose solution to image-to-image translation problems. Method : The generator translate the input image (gray-scale) to target domain(color). And, the discrimator distinguishes between the converted image and real image. $$ \\text{GAN objective} = arg \\min_G \\max_D L_{cGAN}(G,D) + \\lambda L_{L1}(G) $$\nAdversarial Loss , the first term, is from cGAN loss :\n$$ L_{cGAN}(G,D) = \\mathbb{E}_y[log(D(x,y))] + \\mathbb{E}_x[log(1-D(G(x)))]$$\nReconstruction Loss, the second term, is from traditional CNN based loss, which means pixel-wise differences between $y$ and $G(x)$.\n$$ L_{L1}(G) = \\mathbb{E}_{x,y}[| y-G(x) |] $$\nGenerator architecture is based on U-Net and discriminator architecture is based on PatchGAN(Markovian discrimator).\nGenerator is fed on real satellite image instead of latent vector and the pair of images are fed into the discriminator.\nFigure. Overview of pix2pix Architecture 3. cycleGAN (2017) Introduction : For many tasks, paired training data will not be available. Authors present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples.\nMethod : using two discriminator (one for discriminating real $y$ and synthesized $G(x)$, the other for real $x$ and synthetic $F(y)$ ). And additional cycle consistency loss for preventing mode collapse that always return same output but very realistic.\n$$ L(G,F,D_X,D_Y) = L_{GAN}(G,D_Y,X,Y) + L_{GAN}(F,D_X,Y,X) + \\lambda L_{cyc}(G,F) $$\nAdversarial Loss : For the mapping function $G: X \\rightarrow Y $ and its discrimator $D_Y$, we express the objective as :\n$$ L_{GAN} (G,D_Y,X,Y) = \\mathbb{E}{y~p{data}(y)}[(logD_Y(y))] + \\mathbb{E}{x~p{data}(x)}[(1-logD_Y(G(x)))] $$\n$$ L_{GAN} (F,D_X,Y,X) = \\mathbb{E}{x~p{data}(x)}[(logD_X(x))] + \\mathbb{E}{y~p{data}(y)}[(1-logD_X(F(y)))] $$\nCycle Consistency Loss : Adversarial Losses alone cannot guarantee that the learned function can map an individual input $x_i$ to a desired output $y_i$. So authors argue that the learned mapping functions should be cycle-Consistent :\n$$ \\text{Forward cycle consistency} = \\mathbb{E}{x~p{data}(x)}[| F(G(x))-x |_1 ] $$\n$$ \\text{Backward cycle consistency} = \\mathbb{E}{y~p{data}(y)}[| G(F(y))-y | _ {1} ] $$\n$$ L_{cyc} (G,F) = \\text{forward} + \\text{backward} $$\nFigure. Overview of cycleGAN Architecture ","id":7,"section":"Research","summary":"0. Introduction Tasks : Image Style Transfer : The task of migrating a style from one image (Style Image) to another (Content Image). 1. Image Style Transfer using CNNs (2016) Introduction : Introduce a algorithm that can separate and recombine the image content and style of natural images. Method : Extract feature maps $F_l$ from each input image $I_{content} $ and $I_{style}$ using pretrained networks at $l_{th}$ layer. Then, optimize","tags":null,"title":"MLCV #5 | Image Style Transfer","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv05-image-style-transfer/","year":"2018"},{"content":" Summary Text Summarization : is a technique to shorten long texts such that the summary has all the important points of the actual document.\nBy Summarization Approache\nExtraction-based Summarization: The extractive approach involves picking up the most important phrases and lines from the documents. It then combines all the important lines to create the summary. So, in this case, every line and word of the summary actually belongs to the original document which is summarized.\nAbstraction-based Summarization: The abstractive approach involves summarization based on deep learning. So, it uses new phrases and terms, different from the actual document, keeping the points the same, just like how we actually summarize. So, it is much harder than the extractive approach.\nBy Number of source documents\nSingle Document Summary: Summary of a Single Document Multi-Document Summary: Summary from multiple documents Terminologies\ngold summary, reference summary : Ground Truth Reference\nhttps://github.com/uoneway/Text-Summarization-Repo#pre-trained-models 1. TextRank: Bringing order into texts (2004) introduction : Traditional Representative for extractive approach before DL based methods,\nMethod : Similar to Page-Rank algorithm\nPage-rank : is the most representative graph ranking algorithm. It\u0026rsquo;s also famous as the ranking algorithm of early Google\u0026rsquo;s search engine. Assume that pages with many backlinks are important and works like some kind of voting. References\nSummary (KOR),\ngensim.summarization\n2. BERT Sum : Text Summarization with Pretrained Encoders (2019) Introduction : propose a novel document level summerizer based on BERT\nMethod : Finetuning BERT For Summarization\nSummarization Encoder : In order to represent individual sentences, we insert external [CLS] tokens at the start of each sentence, and each [CLS] symbol collects features for the sentence preceding it. Extractive Summarization : can be defined as the binary classification task of assigning a label to each $sent_i$ indicating whether the sentence should be included in the summary. With BERT-SUM, vector $t_i$ which is the vector of the $i$-th [CLS] symbol from the top layer can be used as the representation for $sent_i$. Several inter-sentence Transformer layers are then stacked on top of BERT outputs, to capture document-level features for extracting summaries. Abstractive Summarization : use a standard encoder-decoder framework for abstractive summarization. The encoder is the pretrained BERT-SUM and the decoder is a 6-layered Transformer initialized randomly. References\nOfficial Code, Korean Model ","id":8,"section":"Research","summary":"Summary Text Summarization : is a technique to shorten long texts such that the summary has all the important points of the actual document.\nBy Summarization Approache\nExtraction-based Summarization: The extractive approach involves picking up the most important phrases and lines from the documents. It then combines all the important lines to create the summary. So, in this case, every line and word of the summary actually belongs to the original document which is summarized.","tags":null,"title":"NLP #6 | Text Summarization","uri":"https://koreanbear89.github.io/research/5.-natural-language/nlp-6-text-summarization/","year":"2021"},{"content":"Introduction Tasks:\nImage Retrieval : aims to find similar images to a query image among an image dataset. Tech Trend :\nConventional Methods : relying on local descriptor matching (scale invariant features - local image descriptors - reranking with spatial verifications)\nusing FC layers : after several conv layers as global descriptors [A Babenko et al, A Gordo et al.]\nusing global pooling methods : from the activations of conv layers.\nboost the performance : by combining different global descriptors which are trained individually.\n1.1 BoF, BoW (Bag of Features, Bag of Visual Words) Introduction : BoW is a simplifying representation used in NLP and information retrieval.\nMethods : BoF groups local descriptors.\nLocal Feature Extraction : Extract local features from image (SIFT, SURF, small img patches)\nClustering : Cluster (k-means) extracted features and find center features (codeword) of each cluster\nImage representation : Represent each image using histogram of codeword.\nLearning and Recognition :\nGenerative ways : based on Bayesian =\u0026gt; classification by histogram of each class\nDiscriminative ways: using classifier like SVM =\u0026gt; put histogram into classifier as a feature vector\nFigure 1. Overview of BoW 1.2 VLAD (Aggregating Local Descriptors) (2010) Introduction : propose a simple yet efficient way to aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation.\nFisher Vector : transform a input variable-size set of independent samples into a fixed size vector representation\nA Gaussian Mixture Model (GMM) is used to model the distribution of features(e.g. SIFT) extracted over the image.\nThe Fisher Vector encodes the gradients of the log-likelihood of the features under the GMM, with respect to the GMM parameters.\nVLAD (Vector of Locally Aggregated Descriptor) : is a feature pooling method, which can be seen as a simplification of the Fisher Kernel. VLAD encodes a set of local feature descriptors extracted from an image using a clustering method such as GMM or K-means.\naccumulate the differences $x-c_i$ for each visual word $c_i$.\nsubsequently $L_2$ normalized by $v = v / ||v||_2$\nCan be written using $a_k$ that assigns descriptor $x_i$ to specific cluster centres $c_k$.\n$$ v_{i,j} = \\sum_{x \\in C} x_j-c_{i,j} = \\sum_{i=1}^N a_k(x_i)(x_i(j)-c_k(j))$$\n2.1 NetVLAD (2016) Introduction : develop a cnn architecture that aggregates mid-level conv features into a compact single vector representation using generalized VLAD layer, NetVLAD.\nMethods : (i) extract top conv featues using pretrained CNN (ii) and pool these features using netVLAD\nnetVLAD : The source of discontinuous in VLAD is hard assignment $a_k(x_i)$ of descriptor $x_i$ to specific cluster centres $c_k$. (If $c_k$ is the closest cluster, $a_k=1$, else, $a_k=0$). Authors replace it to soft assignment (softmax of -distances to each clusters).\n$$ a_k(x_i) = softmax( -|x_i - c_k |^2) = \\frac{e^{-\\alpha | x_i| ^2 + 2\\alpha c_k x_i + |c_k |^2}}{\\sum_{k\u0026rsquo;} e^{-\\alpha | x_i| ^2 + 2\\alpha c_k x_i + |c_{k\u0026rsquo;} |^2}} = \\frac{e^{2\\alpha c_k x_i + |c_k |^2}}{\\sum_{k\u0026rsquo;} e^{ 2\\alpha c_k x_i + |c_{k\u0026rsquo;} |^2}} = \\frac{e^{w_k^T x_i + b_k}}{\\sum_{k\u0026rsquo;} e^{w_{k\u0026rsquo;}^T x_i + b_{k\u0026rsquo;}}}$$\n$$V(j,k) = \\sum_{i=1}^{N} a_k (x_i)(x_i(j) - c_k(j))$$\n3.1 Global Descriptors (~2018) SIFT, SPoC : sum pooling from the feature map which performs well mainly due to the subsequent descriptor whitening.\nMAC , regional MAC : performs max pooling (MAC) over regions then sum over the regional MAC descriptor at the end.\nGeM: generalizes max and average pooling with a pooling parameter\nweighted sum pooling, weighted GeM, multiscale RMAC, etc.\nThe performance of each global descriptor varies by dataset as each descriptor has different properties. For example, SPoC activates larger regions on the image representation while MAC activates more focused regions\n3.2 SPoC, Sum Pooling of Convolution (2015) Introduction : investigate possible ways to aggregate local deep features to produce compact global descriptors for image retrieval.\nMethods :\nSum pooling : The construction of the SPoC descriptor starts with the sum pooling of the deep features.\n$$ \\psi_1(I) = \\sum_{y=1}^{H} \\sum_{x=1}^{W} f(x,y)$$\nCentering prior : objects of interest ted to be located close to the geometrical center of an image. So, incorporate such centering prior using coefficients $\\alpha(w,h)$ , (Gaussian)\n$$ \\psi_2(I) = \\sum_{y=1}^{H} \\sum_{x=1}^{W} \\alpha(x,y)f(x,y)$$\nPost processing : The obtained representation $\\psi(I)$ is subsequently l2 normalized, then PCA compression and whitening are performed.\n3.3 MAC, RMAC, Maximum Activation of Convolution (2015) Introduction : revisit both retrieval stages, namely initial search and reranking\nMethod :\nMaximum Activation of Convolutions (MAC) : the feature vector constructed by a spatial max-pooling over all feature map $\\chi_i$ from last conv.\n$$ \\mathbb{f_{\\Omega}} = [f_{\\Omega,1}, f_{\\Omega,2}, \u0026hellip; ,f_{\\Omega,K}]^T, with f_{\\Omega,i} = max \\chi_i(p)$$\nregional MAC : divide conv feature map to multiple regions(for WxH dim not C) and apply MAC for each regions and post-process it. (l2 and PCA-whitening)\nTwo images are compared with the cosine similarity of the K-dim vector produced as described above.\n3.4 GeM, Generalized Mean Pooling (2017) Introduction : propose a novel trainable Generalized Mean Pooling layer that generalizes max and average pooling and show that it boosts retrieval performance\nMethod :\nConvNet Backbone : given an input image, the output is a 3D tensor $\\chi$ of $W \\times H \\times K$ dimensions\nGeM : add a pooling layer that takes $\\chi$ as an input and produces a vector $\\mathbb{f}$ as an output of the pooling process.\n$$ \\mathbb{f_{\\Omega}} = [f_{\\Omega,1}, f_{\\Omega,2}, \u0026hellip; ,f_{\\Omega,K}]^T, f_{\\Omega,k} = ( \\frac{1}{|\\chi_k|} \\sum_{x \\in \\chi_k}x^{p_k})^{\\frac{1}{p_k}}$$\n4.1 Combination of Multiple Global Descriptors (2019) Introduction : Ensembling different models and combining multiple global descriptors lead to performance improvement. However, these processes are not only difficult but also inefficient with respect to time and memory. Here, authors propose a novel framework that exploits multiple global descriptors to get an ensemble effect while it can be trained in an end-to-end manner.\nMethod : Proposed framework consists of a CNN backbone and two modules. The first main module learns an image representation, which is a combination of multiple global descriptors. Next, an auxiliary module to fine-tune a CNN with a classification loss.\nBackbone Network : can use any CNN such as Inception, ShuffleNet, Resnet. authors use ResNet50 as a baseline backbone.\nMain Module - Multiple Global Descriptors : main module has multiple branches that output each image representation by using different global descriptors (SPoC, MAC, GeM) on the last conv layer. And these discriptions are concatenated after whitening(PCA, FC) and l2 normalization .\nAuxiliary Module : finetunes the CNN backbone based on the first global descriptor of the main module by using a classification loss (train a CNN backbone with a classification loss and then fine-tune the network with a triplet loss). Additional temperature scaling and label smoothing for performance improvement.\n","id":9,"section":"Research","summary":"Introduction Tasks: Image Retrieval : aims to find similar images to a query image among an image dataset. Tech Trend : Conventional Methods : relying on local descriptor matching (scale invariant features - local image descriptors - reranking with spatial verifications) using FC layers : after several conv layers as global descriptors [A Babenko et al, A Gordo et al.] using global pooling methods : from the activations of conv","tags":null,"title":"MLCV #6 | Image Retrieval","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv06-image-retrieval/","year":"2018"},{"content":"Introduction Tasks:\nAction Classification : The task classfying an action in video sequences according to its spatio-temporal content. Benchmark Set\nUCF-101 : is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. HMDB-51 Kinetics : has 400 human action classes with more than 400 examples for each class, each from a unique YouTube video. Methods\nCNN + RNNs\n3D Convolutional Networks\nResNeXt-101 : 6GFLOPs for 112x112x16 Two Stream Network (RGB + Optical Flow)\nTwo Stream 3D ConvNets\nFeature Engineering with pre-extracted frame-level featue using CNN\nSkeleton Based Recognition using GCN\nST-GCN : 16 GFLOPs for one action sample 1. LRCN : Long-term Recurrent Convolutional Networks (2014) Introduction : previous models assume a fixed visual representation or perform simple temporal averaging for sequential processing (such as action recog, image captioning, or etc).\nMethod : Long-term Recurrent Convolutional Networks that can learn compositional representations in space and time.\n2. C3D (2014) Introduction : 3D Convolutional Network for learning spatiotemporal feature from a large scale video dataset\nMethod : 3D ConvNets are just like standard convolutional networks, but with spatio-temporal filters (3x3x3)\n3. Two Stream Network (2014) Introduction : investigate architectures to capture the complementary information on appearance from still frames and motion between frames (optical flow).\nMethod : averaging the predictions from a single RGB frame and a stack of 10 externally computed optical flow frames, after passing them through two replicas of an ImageNet pre-trained ConvNet.\n4. I3D (2017) Introduction : A number of successful image classification architectures have been developed over the years through painstaking trial and error. Instead of repeating the process for spatio-temporal models, authors propose to simply convert successful image(2D) classification models into 3D ConvNets.\nMethod : Two-Stream Infalted 3D ConvNet (I3D) that is based on 2D ConvNet inflation\nInflating 2D into 3D : filters and pooling kernels of 2D ConvNets for image classification are just expanded into 3D.\nTwo 3D Streams : with one I3D network trained on RGB inputs and another on optical flow inputs. Authors trained two networks separately and averaged their predictions at test time.\n5. ActionVLAD (2017) Introduction : 3D CNN or two stream architectures disregard the long-term temporal structure of video. For example, a basketball shoot, can be confused with other actions such as running, dribbling, jumping, throwing, with only few consecutive frames. So we need a global descriptor for the entire video.\nMethods:\nsample frames from the entire video and get top-conv features using a pretrained CNN from RGB and flow each.\nActionVLAD : is a learnable spatio temporal aggregation layers. While max or average pooling are good for similar features, actionVLAD aggregates their residuals from nearest cluster centers.\n$$ V = \\sum_{t=1}^{T} \\sum_{i=1}^{N} {\\frac{e^{-\\alpha || x_{it}-c_k||^2}}{ \\sum_{k\u0026rsquo;} {e^{-\\alpha || x_{it} - c_{k\u0026rsquo;}||^2}}}} (x_{it}[j] - c_k[j]) $$\ncombine VLADs from each stream (get video-level fixed length vector) and pass it through a classifier that outputs the final classification scores.\nDifferent pooling strategies for a collection of diverse features. Points correspond to features from a video and colors correspond to different sub-actions in the video. 6. LOUPE : 1st place at 2017 Youtube-8M (2017) Introduction : Current method for video analysis often extract frame-level features using pre-trained CNNs. Such features are then aggregated over time e.g., by simple temporal averaging or more sophisticated recurrent neural networks such as LSTM or GRU. This work first explore clustering-based aggregation layers.\nMethod :\nCNN Feature Extraction: The input features (frame-level) are extracted from video and audio signals.\nCreate Local feature: The pooling module (e.g. netVLAD) aggregates the extracted features into a single compact (e.g. 1024 dim) representation for the entire video.\nFeature Enhancing: The aggregated representation is then enhanced by the Context Gating Layer.\nClassification: Classification module takes the resulting representation as input and output scores for a pre-defined set of labels.\n7. 3D ResNext (2017) Introduction : Conventional research has only explored relatively shallow 3D architectures. Authors examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets.\nMethod : training 3D CNNs such as ResNet, ResNext, DenseNet on UCF101, HMDB-51 and so on.\n8. SlowFast Networks (2018) Introduction : The recognition of the categorical semantics (colors, textures, lighting etc.) can be refreshed relatively slowly. On the other hand, the motion being performed can evolve much faster. So authors present a two-pathway SlowFast model for video recognition\nMethod : simply can be described as a single stream architecture that operates at two different framerates.\nSlow pathway : can be any spatiotemporal conv model. key concept is a large temporal stride τ (typically 16) on input frames, i.e., it processes only one out of τ frames.\nFast pathway : another conv model which have a small temporal stride\nLateral Connections : The information of the two pathways is fused by lateral connections which have been used to fuse optical flow based, two-stream networks.\n9. ST GCN (2018) Introduction : propose a novel model of dynamic skeletons called ST-GCN\nMethods:\nPose Estimation : construct a spatiotemporal graph with the joints as graph nodes and natural connectivities in both human structures and times as graph edges.\nSkeleton Graph Construntion : The node set $V$ has all the joints in a sequence including estimated coordinates and estimation confidence. The edge set $E$ is composed of two subset, that depicts the intra skeleton connetcion and inter-frame edges.\nSpatial GCN : The feature map $f^t_{in} : V_t \\rightarrow R^c $ has a vector on each node of the graph.\non image, convolution opertion can be written as below with sampling function $p$ and weight function $w$.\n$$ f_{out}(\\mathbb{x}) = \\sum_h \\sum_w f_{in} (p(\\mathbb{x},h,w)) \\cdot w(h,w) $$\nsampling function : On image, neigboring pixels are defined by x as center using kernel size. On graph, neighbor nodes are defined by the minimum length of path from x.\nweight function : is similart to the kernel of 2d convolution. But we have a mapper\n$$ f_{out}(v_{ti}) = \\sum_{v_{tj} \\in B(v_{ti})} \\frac{1}{Z_{ti}(v_{tj})} f_{in}(v_{tj}) \\cdot \\mathbb{w}(l_{ti}(v_{tj})) $$\nspatiotemporal modeling : Until now, we formulated spatial GCN, this can be expanded in temporal dimension simply. By extending the concept of neighborhood to also include temporally connected joints\nPartition strategies : design a partitioning strategy to implement the label map $l$.\n(d) spatial configuration partitioning. The nodes are labeled according to their distances to the skeleton gravity center (black cross), root(green), near(blue), longer(yellow) Limitations : cannot model the correlation between the joints that located further away than the maximum distance D. (left hand and right foot) : resolved by Actional Structural GCN\n10. Shift GCN (2020) Introduction : propose a novel shift graph convolutional network to overcome conventional shortcomings\nComputational complexity of GCN based methods are pretty heavy.\nThe receptive fields of both spatial graph and temporal graph are inflexible\n11. ViViT : A Video Vision Transformer (2021, Goog Res) Introduction : We propose a pure-transformer architecture for video classification, inspired by the recent success of such models for images like ViT.\nMethods :\nEmbedding video clips : two simple methods for mapping a video to a sequence of tokens $\\hat{z}$ (Uniform, Tubelet) and then add the positional embedding and reshape into $z$, the input to the transformer\n$$ \\mathbf{V} \\in \\mathbb{R}^{T\\times H\\times W \\times C} \\mapsto \\hat{z} \\in \\mathbb{R}^{n_t \\times n_h \\times n_w \\times n_d} \\rightarrow z \\in \\mathbb{R}^{N \\times d} $$\nUniform frame sampling (Figure2) : simply sample $n_t$ frames, and embed each 2D frame independently following ViT (Conv2D + Concat)\nTubelet Embedding (Figure3) : extension of ViT\u0026rsquo;s embedding to 3D and corresponds to a 3D convolution.\nTransformer Models for VIdeo\nModel 1) Spatio-temporal attention : simply forwards all spatio-temporal tokens extracted from the video\nAs it models all pairwise interactions, Multi-Headed Self Attention has quadratic complexity with respect to the number of tokens.\nmotivates the development of more efficient architectures\nModel 2) Factorised encoder : consists of two separate transformer encoders\nspatial encoder : only models interactions between tokens extracted from the same temporal index.\ntemporal encoder: consisting ofLttransformer layers to model in-teractions between tokens from different temporal indices.\nResults\nInput Encoding : tubelet embedding initialised using the “central frame” method (Eq. 9) performs well, outperforming the others (Table1)\nModel Variants : The unfactorised model (Model 1) performs the best on Kinetics 400. However, it can also overfit on smaller datasets such as Epic Kitchens, where we find our “Factorised Encoder” (Model 2) to perform the best\n12. TimeSformer : Is Space-Time Attention All You Need for Video Understanding (2021) Introduction : We present a convolution-free approach to video classification\nMethods : TimeSformer (Time-Space Transformer)\nPreprocessing\nInput Clip : TimeSformer takes input clip $X \\in \\mathbb{R}^{H \\times W \\times 3 \\times F}$\nDecomposition into patches ; each frame is decomposed into $N$ non-overlapping patches of $\\mathbf{x_{(p,t)}} \\in \\mathbb{R}^{3P^2}$\nLinear embedding : embedding vector $\\mathbb{z}{(p,t)} \\in \\mathbb{R}^D$ by means of a learnable matrix $E \\in \\mathbb{R}^{D \\times 3P^2}$ with trainable positional embedding $e^{pos}{(p,t)} \\in \\mathbb{R}^D$ : $\\mathbf{z}{(p,t)} = E\\mathbf{x}{(p,t)} + e^{pos}_{(p,t)}$\nClassification Embedding : The final clip embedding is obtained from the final block for the classification token, On top of this representation we append a 1-hidden-layer MLP, which is used to predict the final video classes.\noutput을 다시 aggregation 해서 Modelling\nQuery Key Value computation : At each block $l$, a query/key/value vector is computed for each patch from the representation $z^{(l−1)}_{(p,t)}$encoded by the preceding block\nSelf-attention computation : via dot-product of query and key vector\nEncoding : The encoding is obtained by computing the weighted sum of value vectors using self-attention coefficients from each attention head\nSpace-Time Self Attention Models : temporal attention and spatial attention are separately applied one after the other\nwe first compute temporal attention by comparing each patch(p,t) with all the patches at the same spatial location in the other frames Conclusion : conceptually simple, achieves state of the art results on major action recognition tasks, has low training and inference cost, adn can be applied to clips of over one minute, thus enabling long-term video modeling.\nclass PatchEmbed(nn.Module): self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+1, embed_dim)) ... def forward_features(self, x): x, T, W = self.patch_embed(x) cls_tokens = self.cls_token.expand(x.size(0), -1, -1) x = torch.cat((cls_tokens, x), dim=1) x = x + self.pos_embed ... class Attention(nn.Module): self.qkv = nn.Linear(dim, dim * 3) self.norm1 = norm_layer(dim) ... def forward(self, x): q, k, v = qkv[0], qkv[1], qkv[2] attn = (q @ k.transpose(-2, -1)) * self.scale attn = softmax(dim=-1) x = (attn @ v).transpose(1, 2).reshape(B, N, C) ","id":10,"section":"Research","summary":"Introduction Tasks: Action Classification : The task classfying an action in video sequences according to its spatio-temporal content. Benchmark Set UCF-101 : is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. HMDB-51 Kinetics : has 400 human action classes with more than 400 examples for each class, each from a unique YouTube video. Methods CNN + RNNs 3D Convolutional Networks ResNeXt-101 :","tags":null,"title":"MLCV #7 | Action Classification","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv07-video-classification/","year":"2018"},{"content":"\n8. When Models Meet Data In the first part of the book, we introduced the mathematics that form the foundations of many machine learning methods\nThe second part of the book introduces four pillars of machine learning:\nRegression (Chapter 9) Dimensionality reduction (Chapter 10) Density estimation (Chapter 11) Classification (Chapter 12) 8.1 Data, Models, and Learning Three major components of a machine learning system: data, models, and learning. Good models : should perform well on unseen data. There are two different senses in which we use the phrase “machine learning algorithm”: training and prediction. 8.1.1 Data as Vectors\nWe assume that a domain expert already converted data appropriately, i.e., each input $x_n$ is a D-dimensional vector of real numbers, which are called features, attributes, or covariates by following steps: Raw Data format : In recent years, machine learning has been applied to many types of data that do not obviously come in the tabular numerical format Numerical Representation : Even when we have data in tabular format, there are still choices to be made to obtain a numerical representation. Normalization : Even numerical data that could potentially be directly read into a machine learning algorithm should be carefully considered for units, scaling, and constraint 8.1.2 Models as Functions\nA predictor is a function that, when given a particular input example (in our case, a vector of features), produces an output 8.1.3 Models as Probability Distributions\nInstead of considering a predictor as a single function, we could consider predictors to be probabilistic models, 8.1.4 Learning is Finding Parameters\nThe goal of learning is to find a model and its corresponding parameters such that the resulting predictor will perform well on unseen data. we will consider two schools of machine learning in this book, corresponding to whether the predictor is a function or a probabilistic model. We would like to find good predictors for given training data, with two main strategies : based on some measure of quality (sometimes called finding a point estimate), using Bayesian inference. (requires probabilistic models) 8.2 Empirical Risk Minimization In this section, we consider the case of a predictor that is a function 8.2.1 Hypothesis Class of Functions\nFor given data, we would like to estimate a predictor $f(·,θ):RD →R$, parametrized by $θ$. We hope to be able to find a good parameter $θ^∗$ such that we fit the data well, that is,\n$$ f(x_n,θ^∗) ≈ y_n \\text{ for all } n = 1,\u0026hellip;,N. $$\nAffine functions (linear functions) are used as predictors for conventional ML methods Instead of a linear function, we may wish to consider non-linear functions as predictors. Given the class of functions, we want to search for a good predictor. We now move on to the second ingredient of empirical risk minimization: how to measure how well the predictor fits the training data.\n8.2.2 Loss Function for Training\nEquation below is called the empirical risk and depends on three arguments, the predictor $f$ and the data $X, y$. $$ R_{emp}(f,X,y) = \\frac{1}{N} \\sum_{n}^{N} l(y_n, \\hat{y}_n) $$\nEmpirical risk minimization : This general strategy for learning is called empirical risk minimization 8.2.3 Regularization to Reduce Overfitting\nThe aim of training a ML predictor is so that we can perform well on unseen data Overfitting : the predictor fits too closely to the training data and does not generalize well to new data Regularization : introduce a penalty term that makes it harder for the optimizer to return an overly flexible predictor $$ \\text{min}_θ \\frac{1}{N}∥y − Xθ∥^2 \\rightarrow \\text{min}_θ \\frac{1}{N}∥y − Xθ∥^2 + λ∥θ∥^2 $$\n8.2.4 Cross-Validation to Assess the Generalization Performance\npartitions the data into K chunks, K-1 of which form the trainig set, and the last chunk serves as the validation set. We cycle through all possible partitionings of validation and training sets and compute the average generalization error of the predictor 8.3 Parameter Estimation In this section, we will see how to use probability distributions to model our uncertainty due to the observation process and our uncertainty in the parameters of our predictors. 8.3.1 Maximum Likelihood Estimation\nThe idea behind MLE is to define a function of the parameters that enables us to find a model that fits the data well.\nNegative log-likelihood for data represented by a random variable $x$ and a family of probability densities $p(x|θ)$ is given by :\nThe notation $L_x(θ)$ emphasizes the fact that the parameter $θ$ is varying and the data $x$ is fixed. to find a good parameter vector $θ$ that explains the data well, minimize the negative log-likelihood $L(θ)$ with respect to $θ$. $$ L_x(\\theta) = −logp(x|θ) $$\nMinimizing NLL $L(θ)$ of Gaussian, corresponds to solving the least-squares problem (example 8.5)\nLikelihood detail in section 6.3\n8.3.2 Maximum A Posteriori Estimaion\nIf we have prior knowledge about the distribution of the parameters θ, we can multiply an additional term to the likelihood. using Bayes\u0026rsquo; theorem (6.3) we can compute a posterior distbution $p(θ|x)$, with prior distribution $p(\\theta)$, and likelihood $p(x|θ)$ $$ p(θ|x) = p(x|θ)p(θ) / p(x) $$\n8.3.3 Model Fitting\nOverfitting : refers to the situation where the parametrized model class is too rich to model the dataset Underfitting : we encounter the opposite problem where the model class is not rich enough 8.4 Probabilistic Modeling and Inference We often build models that describe the generative process that generates the observed data. A coin-flip experiment can be described in two steps, (1), we define a parameter μ, which describes the probability of “heads” as the parameter of a Bernoulli distribution (2) we can sample an outcome x ∈ {head, tail} from the Bernoulli distribution $p(x | μ) = Ber(μ)$ In this chapter, we will discuss how probabilistic modeling can be used for this purpose. 8.4.1 Probabilistic Models\nProbabilistic models represent the uncertain aspects of an experiment as probability distributions. Only the joint distribution has this property. Therefore, a probabilistic model is specified by the joint distribution of all its random variables The prior and the likelihood (product rule, Section 6.3) The marginal likelihood p(x), which will play an important role in model selection (Section 8.6), can be computed by taking the joint distribution and integrating out the parameters (sum rule, Section 6.3) The posterior, which can be obtained by dividing the joint by the marginal likelihood. 8.4.2 Bayesian Inference\nIn Section 8.3.1, we already discussed two ways for estimating model parameters θ using maximum likelihood or maximum a posteriori estimation.\nOnce these point estimates θ* are known, we use them to make predictions. More specifically, the predictive distribution will be p(x | θ*), where we use θ* in the likelihood function.\nHaving the full posterior distribution around can be extremely useful and leads to more robust decisions.\n=\u0026gt; Bayesian inference is about finding this posterior distribution\nThe key idea is to exploit Bayes’ theorem to invert the relationship between the parameters θ and the data X (given by the likelihood) to obtain the posterior distribution p(θ | X ).\n8.4.3 Latent Variable Models\nIn practice, it is sometimes useful to have additional latent variables z (besides the model parameters θ) as part of the model. Latent variables may describe the data-generating process, thereby contributing to the interpretability of the model. The conditional distribution $p(x | θ,z)$ allows us to generate data for any model parameters and latent variables. 8.5 Directed Graphical Models In this section, we introduce a graphical language for specifying a probabilistic model, called the directed graphical model provides a compact and succinct way to specify probabilistic models, allows the reader to visually parse dependencies between random variables This section relies on the concepts of independence and conditional independence, as described in Section 6.4.5. Probabilistic graphical models have some convenient properties: They are a simple way to visualize the structure of a probabilistic model. They can be used to design or motivate new kinds of statistical models. Inspection of the graph alone gives us insight into properties, e.g., conditional independence. Complex computations for inference and learning in statistical models can be expressed in terms of graphical manipulations. 8.6 Model Selection In machine learning, we often need to make high-level modeling decisions that critically influence the performance of the model More complex models are more flexible in the sense that they can be used to describe more datasets A polynomial of degree 1 (a line) can only be used to describe linear relations between inputs x and observations y. A polynomial of degree 2 can additionally describe quadratic relationships between inputs and observations. However, we also need some mechanisms for assessing how a model generalizes to unseen test data. =\u0026gt; Model selection is concerned with exactly this problem. Summary In this chapter, We will see how ML algorithms work from a mathematical point of view. Section 8.1, Data, Model, Learning We saw the three main components of ML algorithm : Data, Model, Learning Section 8.2, Empirical Risk Minimization We learned about the perspective of seeing ML model as a functional optimization Loss Function (ex. MSE =\u0026gt; Least Square Problem) Regularization, additional penalty term in loss function Section 8.3, Parameter Estimation We learned about the parameter estimation of data distribution in terms of ML model training. Likelihood (loss function) prior (regularization term) Section 8.4 Probabilistic Modeling and Inference We learned about the Bayesian inference in terms of ML Model inference Section 8.5 Directed Graphical Models Directed Graphical Models can be used to specify a probabilistic model. Section 8.6 Model Selection How can we choose the model that works best for unseen data among trained models. ","id":11,"section":"Mathematics","summary":"8. When Models Meet Data In the first part of the book, we introduced the mathematics that form the foundations of many machine learning methods\nThe second part of the book introduces four pillars of machine learning:\nRegression (Chapter 9) Dimensionality reduction (Chapter 10) Density estimation (Chapter 11) Classification (Chapter 12) 8.1 Data, Models, and Learning Three major components of a machine learning system: data, models, and learning. Good models : should perform well on unseen data.","tags":null,"title":"Mathematics for ML #8 | Introduction Part.II","uri":"https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml08-when-models-meet-data/","year":"2022"},{"content":"Introduction Tasks : Pose Estimation : The task aims to detect the locations of human anatomical keypoints (e.g., elbow, wrist, etc) 1. Deep Pose (2014) Introduction : The first major paper that applied Deep Learning to Human pose estimation\nMethod :\nDNN-based regression : Alexnet backend (7 layers) with an extra final layer that outputs 2k joint coordinates (where $k$ is the number of joints).\nCascade of pose regressors : refinement of the predictions using cascaded regressors.\nSince the ground truth pose vector is defined in absolute image coordinates and poses vary in size from image to image, authors normalize their training set (coordinates)\nlinear regression on top of the last network layer to predict a pose vector by minimizing $L_2$ distance between the prediction and the true pose vector.\n2. Efficient Object Localization Using ConvNets (2015) Introduction : ConvNet architecture which outpus a heatmap, describing the likelihood of a joint occurring in each spatial location\nMethod : Using an additional ConvNet to refine the localization result of the coarse heat-map.\nCoarse Heat-Map Regression Model : Multi-resolution ConvNet that receives multiple input images with the same content but different sizes\nFine Heat Map Regression Model : Siamese Network with $k$ heads($k$ is the number of joint instance)\n*Figure 4. Overview of our Cascaded Architecture 3. Simple Baselines for Human Pose Estimation and Tracking (2018) Introduction : This work provides simple and effective baseline method for human pose estimation(Task1) \u0026amp; pose tracking(Task2).\nMethod :\nModel Architecture : Simply adds a few deconv layers over the last conv layer in the ResNet.\nTraining Strategy : Use the label (heatmap, $H^k$ for joint $k$ is generated by applying a 2D Gaussian centered on the $k^{th}$ joint\u0026rsquo;s ground truth location with std-dev=1 pixel).\nFlow-Based Pose Tracking: Two different kinds of human boxes, one is from a human detector and the other are boxes generated from previous frames using optical flow.\ninput, label = frames, keypoints_to_hmap(keypoints) bbox= Human_Detector(input) keypoints = hmap_to_coord(Pose_Estimator(input, bbox_det)) for i in range(len(input)): bbox_det = Human_Detector(input) bbox_flow = FlowBox_Generator() # Non-maximum suppression : unify detection and flow boxes bbox_unified = NMS(bbox_det, bbox_flow) joints = Pose_Estimator(input[1], bbox_det[1]) sim_matrix = calc_sim(output[i-1], joints) pose = (sim_matrix, id) output.append(pose) # update the output list 4. HR-Net, Microsoft (2019) Introduction : Existing approaches consist of a stem subnetwork, which decreases the resolution based on high-to-low design pattern.\nMethod : Novel network architecture that connects high-to-low subnetworks in parallel that can maintains high-resolution representations through the whole process for spatially precise heatmap estimation.\n*Figure1. Illustrating the architecture of the proposed HRNet. 5. Higher HR-Net Top-Down methods : take a dependency on person detector to detect person instances to reduce the problem.\nHowever, they are normally computationally intensive and not truly end-to-end systems\n","id":12,"section":"Research","summary":"Introduction Tasks : Pose Estimation : The task aims to detect the locations of human anatomical keypoints (e.g., elbow, wrist, etc) 1. Deep Pose (2014) Introduction : The first major paper that applied Deep Learning to Human pose estimation Method : DNN-based regression : Alexnet backend (7 layers) with an extra final layer that outputs 2k joint coordinates (where $k$ is the number of joints). Cascade of pose regressors :","tags":null,"title":"MLCV #8 | Pose Estimation","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv08-pose-estimation/","year":"2019"},{"content":"\n9. Linear Regression In the following, we will apply the mathematical concepts from previous chapters, to solve linear regression (curve fitting) problems.\nIn regression, we aim to find a function $f$ that maps inputs $x∈R^D$ to corresponding function values $f(x)∈R.$\nWe are given a set of training inputs $x_n$ and corresponding noisy observations $y_n=f(x_n) + \\epsilon$,\nwhere $\\epsilon$ is an i.i.d random variable that describes measurement and observation noise =\u0026gt; simply zero-mean Gaussian noise (not further in this chapter)\nSolving a regression function requires a variety of problems:\n(1) Choice of the model (type) and the parametrization of the regression function.\n(2) Finding good parameters.\n(3) Overfitting and model selection.\n(4) Relationship between loss functions and parameter priors.\n(5) Uncertainty modeling\n9.1 Problem Formulation Objective : is to find a function that is close to the unknown function $f$ that generated the observed data (and that generalizes well).\nObservation Noise\nThe functional relationship between input x and y is given as below. Noise $ε∼N(0, σ^2)$ is independent, identically distributed (i.i.d.) Gaussian measurement noise with zero-mean and variance of $σ^2$. $$ y = f (x) + ε $$\nDefine Likelihood function :\nBecause of the presence of observation noise, we will adopt a probabilistic approach and explicitly model the noise using a likelihood function.\nLikelihood function would be Gaussian with mean, $y|f(x)$ and variance, $\\sigma^2$ :\n$$ p(y|x) = N(y|f(x), σ^2) $$\nParametrization :\nWe consider the special case that the parameters $\\theta$ appear linearly in our model. $$ p(y|x,θ) = N(y|x^⊤θ, σ^2) \\newline \\leftrightarrow y=x^⊤θ+ε, ε∼N(0,σ^2) $$\n9.2 Parameter Estimation In the following, we will have a look at parameter estimation by maximizing the likelihood, a topic that we already covered to some degree in Section 8.3. 9.2.1 Maximum Likelihood Estimation (1) Maximum Likelihood Estimation A widely used approach to finding the desired parameters $θ_{ML}$ where we find parameters $θ_{ML}$ that maximize the likelihood. $$ θ_{ML} = \\text{argmax}_\\theta \\ p(Y |X,θ). $$\n(2) Likelihood Factorization : For given training set, $y_i$ and $y_j$ are conditionally independent given their respective inputs $x_i, x_j$ so that the likelihood factorizes according to : $$ p(Y|X,θ) = p(y_1,\u0026hellip;,y_N |x_1,\u0026hellip;,x_N,θ) $$\n$$ = \\prod p(y_n |x_n,θ) = \\prod N(y_n |x^⊤_n θ, σ^2) \\tag{9.5b} $$\n(3) Log-Transformation : Since the likelihood (9.5b) is a product of N Gaussian distributions, the log-transformation is useful To find $\\theta_{ML}$, we can minimize the negative log-likelihood, which is equal to some of squared errors $$ −logp(Y|X,θ) = −log \\prod p(y_n|x_n,θ) = − \\sum logp(y_n|x_n,θ) $$\n$$ logp(y_n|x_n,θ)= − \\frac{1}{2\\sigma^2} (y_n−x^⊤_nθ)^2 +const = L(\\theta) $$\n(4) Global Optimum : We can find the global optimum by computing the gradient of $L$, setting it to 0 and solving for $θ$ $$ \\frac{dL}{d\\theta} = 0^⊤ \\Longleftrightarrow θ_{ML} = (X^⊤X)^{−1}X^⊤y $$\nPolynomial Regression (MLE with Features) : straight lines are not sufficiently expressive when it comes to fitting more interesting data Estimating the Noise Variance : We can also use the principle of MLE to obtain the maximum likelihood estimator $\\sigma_{ML}^2$ for the noise variance 9.2.2 Overfitting in Linear Regression Model Selection : we can use the RMSE (or the negative log-likelihood) to determine the best degree of the polynomial that minimizes the objective.\npolynomials of low degree fit the data poorly\nhigh-degree polynomials oscillate wildly and are a poor representation of the underlying function that generated the data, such that we suffer from overfitting\n9.2.3 Maximum A Posteriori Estimation (1) Prior and Posterior :\nThe posterior over the parameters θ, given the training data $X,Y$ is obtained by applying Bayes’ theorem (Section 6.3) as : $$ p(θ|X,Y)= \\frac{p(Y|X,θ)p(θ)}{p(Y | X )} $$\nSince the posterior explicitly depends on the parameter prior $p(θ)$, the prior will have an effect on the parameter vector we find as the maximizer of the posterior. (2) Log-Transformation :\nTo find the MAP estimate, We start with the log-transform and compute the log-posterior $$ log p(θ | X , Y) = log p(Y | X , θ) + log p(θ) + const , $$\nWith a (conjugate) Gaussian prior $p(θ) = N(0, b^2I)$􏰂on the parameters θ, we obtain the negative log posterior $$ −logp(θ|X,Y)= \\frac{1}{2\\sigma^2}(y−Φθ)^⊤(y−Φθ)+ \\frac{1}{2b^2} θ^⊤θ+const. $$\nAnd then, we minimize the negative log-poterior distribution with respect to $\\theta$\n$$ θ_{MAP} ∈ argmin_\\theta[−logp(Y|X,θ)−logp(θ)]. $$\n(3) Gradient :\nGet gradient of the negative log-posterior with respect to θ, and we can see the first term on the right-had side as the gradient of the negative log-lgikelihood\n$$ − \\frac{dlogp(θ|X,Y)}{d\\theta} = −\\frac{dlogp(Y|X,θ)}{d\\theta} − \\frac{dlogp(θ)}{d\\theta} $$\n(4) Global Optimum : We will find $θ_{MAP}$ by setting this gradient to $0^⊤$ and solving for $θ_{MAP}$\n$$ θ_{MAP} = (Φ^⊤Φ+\\frac{σ^2}{b^2}I)^{-1}Φ^⊤y $$\n9.2.4 MAP Estimation as Regularization Regularized least squares :\nIn Regularized Least Squares, we consider the loss function as below, which we minimize with respect to θ (see Section 8.2.3) $$ ∥y − Φθ∥^2 + λ ∥θ∥^2_2 $$\nthe first term is a data-fit term, which is proportional to the negative log-likelihood; see (9.10b).\nThe second term is called the regularizer, and the regularization parameter λ\u0026gt;0 controls the “strictness” of the regularization =\u0026gt; can be interpreted as a negative log-Gaussian prior\n9.3 Bayesian Linear Regression Bayesian linear regression : pushes the idea of the parameter prior a step further does not even attempt to compute a point estimate of the parameters, but instead the full posterior distribution over the parameters is taken into account when making predictions 9.3.1 Model Prior, $p(θ) = N(m_0, S_0)$\nlikelihood, $p(y | x, θ) = N (y | φ^⊤(x)θ, σ^2)$\nFull probabilistic model : the joint distribution of observed and unobserved random variables, y and θ, respectively, is:\n$$ p(y,θ|x) = p(y|x,θ)p(θ) $$\n9.3.2 Prior Predictions Introduction\nwe are usually not so much interested in the parameter θ themselves.\nInstead, our focus often lies in the predictions we make with those parameter values.\nIn a Bayesian setting, we take the parameter distribution and average over all plausible parameter settings when we make predictions.\nMore specifically, to make predictions at an input x∗, we integrate out θ and obtain :\n$$ p(y_∗| x_∗) = \\int p(y_∗| x_∗, θ)p(θ)dθ = E_θ[p(y_∗|x_∗, θ)] $$\nSummary\nSo far, we looked at computing preds using the parameter prior $p(θ)$.\nHowever, when we have a parameter posterior (given some training data X, Y) =\u0026gt; we just need to replace the prior $p(θ)$ with the posterior $p(θ|X,Y)$.\n9.3.3 Posterior Prediction Bayes\u0026rsquo; theorem : Given a training set of inputs $x_n ∈ R^D$ and corresponding observations $y_n ∈ R$, we compute the posterior over the parameters using Bayes’ theorem : $$ p(θ|X,Y) = \\frac{p(Y|X,θ)p(θ)}{p(Y|X)} $$\nMarginal Likelihood : independent of the parameters θ and ensures that the posterior is normalized, the likelihood averaged over all possible parameter settings\nPosterior Predictions\nIn principle, predicting with the parameter posterior p(θ|X,Y) is not fundamentally different given that in our conjugate model the prior and posterior are both Gaussian (with different parameters).\nTherefore, by following the same reasoning as in Section 9.3.2, we obtain the (posterior) predictive distribution\nSKIP DETAILS\n9.4 Maximum Likelihood as Orthogonal Projection Introduction : we will now provide a geometric interpretation of MLE. Let us consider a simple linear regression setting. $$ y=xθ+ε, ε∼N(0,σ^2), \\tag{9.65} $$\nProblem Definition :\nThe parameter θ determines the slope of the line.\nWe obtained (9.2.1) the MLE for the slope parameter as (9.66)\nThis means, $θ_{ML}$ is the approximation with the minimum least-squares error between y and Xθ. (9.67)\n$$ θ_{ML} = (X^⊤X)^{−1}X^⊤y \\tag{9.66} $$\n$$ Xθ_{ML} = (X^⊤X)^{−1}XX^⊤y = \\frac{XX^⊤}{X^⊤X}y \\tag{9.67} $$\nSolution : As we are looking for a solution of $y = Xθ$, we can think of linear regression as a problem for solving systems of linear equations\nIn particular, looking carefully at (9.67) we see that the $θ_{ML}$ in our example from (9.65) effectively does an orthogonal projection of y onto the one-dimensional subspace spanned by X\nRecalling the results on orthogonal projections from Section 3.8, we identify $\\frac{X X^⊤}{X⊤X}$ as the projection matrix, Therefore, the maximum likelihood solution provides also a geometrically optimal solution by finding the vectors in the subspace spanned by X that are “closest” to the corresponding observations y,\nIn the general (polynomial) linear regression case, we again can interpret the maximum likelihood result as a projection onto a K-dimensional subspace of $R^N$.\nSummary In this chapter, we will see how to find a function that maps input $x$ to $y$ in observed training data set.\nSection 9.1, \u0008Problem Formulation\nFunctional Relationship : Objective is to find a funtion that is close to the unknown function $f$ that generated the observed data\nObservation noise : Set the observation noise $ε∼N(0, σ^2)$ as (i.i.d.) Gaussian measurement with zero-mean and variance of $σ^2$.\nSection 9.2, Parameter Estimation\nMaximum Liklihood Estimation : We find the model parameters $\\theta_{ML}$ that maximize the likelihood.\n(1) Define the likelihood function : with gaussian function\n(2) Likelihood Factorization : training samples are independent to each other, the likelihood factorizes according to $\\prod N(y_n |x^⊤_n θ, σ^2) $\n(3) Log Transformation :\nLikelihood, a product of N Gaussian Distributions, can be transformed to sum of N log gaussian dist. The problem also transformed to minimize the negative log-likelihood (4) Global Minimum : We can find the global optimum by computing the gradient\nMaximum A Posterior : MLE can lead to severe overfitting, MAP addresses this issue by placing a prior on the params that play the role of a regularizer\nSection 9.3, Bayesian Linear Regression\nBayesian Regression : From section 9.2, we focused on estimating appropriate parameters that work well, However in section 9.3, we focus on prediction itself, not parameters, so we just use parameter distribution and average over all plausible parameter settings when we make predictions\nPrior Prediction : to make predictions at an input $x_∗$, we integrate out θ.\nPosterior Prediction : when we have a parameter posterior (given some training data X, Y) then, we just need to replace the prior $p(θ)$ with the posterior $p(θ|X,Y)$\nSection 9.4, Maximum Likelihood as Orthogonal Projection\nThe MLE solution provides also a geometrically optimal solution by finding the vectors in the subspace spanned by X that are “closest” to the corresponding observations y ","id":13,"section":"Mathematics","summary":"9. Linear Regression In the following, we will apply the mathematical concepts from previous chapters, to solve linear regression (curve fitting) problems.\nIn regression, we aim to find a function $f$ that maps inputs $x∈R^D$ to corresponding function values $f(x)∈R.$\nWe are given a set of training inputs $x_n$ and corresponding noisy observations $y_n=f(x_n) + \\epsilon$,\nwhere $\\epsilon$ is an i.i.d random variable that describes measurement and observation noise =\u0026gt; simply zero-mean Gaussian noise (not further in this chapter)","tags":null,"title":"Mathematics for ML #9 | Linear Regression","uri":"https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml09-linear-regression/","year":"2022"},{"content":"Introduction Tasks :\n3D object detection : classifies the object category and estimates oriented 3D bounding boxes of physical objects from 3D sensor data. Applications : By extending prediction to 3D, one can capture an object’s size, position and orientation in the world, leading to a variety of applications in robotics, self-driving vehicles, image retrieval, and augmented reality\nBenchmarks\nKITTI (car, cyclist, pedestrian easy, mod, hard) : for autonomous driving ScanNet : for indoor scene understanding task SUN-RGBD : Scene Understanding Benchmarks YCB-V : 21 objects from the YCB dataset captured in 92 videos with 133,827 frames. Approches\nApproaches based on Point Clouds (LiDAR)\nProjection based methods : projects point clouds to 2D planes such as bird view, front view, etc. + RPN Volumetric methods : voxelization + heavy 3D CNN Point-net based methods : extract features from raw data Graph based methods : construct graphs to learn the order-invariant of point clouds Approaches based on monocular/stereo images\nApproaches based on cheaper monocular or stereo imagery data have resulted in drastically lower accuracies until now. A key ingredient for image-based 3D object detection methods is a reliable depth estimation approach to replace LiDAR. Existing algorithms are largely built upon 2D object detection, imposing extra geometric constraints to create3D proposals. apply stereo-based depth estimation to obtain the true 3D coordinates of each pixel. 1. SUN-RGBD(2017) An RGB-D benchmark suite for all major scene understanding tasks\nDataset is captured by four different sensors and contains 10,335 RGB-D images.\nDensely annotated and includes 146k 2d polygons, and 64k 3D bboxes with accurate object orientations, as well as a 3D room layout and scene category for each images.\n2. ScanNet(2017) An RGB-D Video Dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. 3. Pseudo LiDAR(2019) Introduction : Mono/stereo depth estimation based methods show worse performance than LiDAR based methods because of poor precision of image based depth estimation.\nContrib : convert image-based depth maps to pseudo-LiDAR representations\nMethods :\nconvert the estimated depth map from stereo or monocular imagery into a 3D point cloud, which we refer to as pseudo-LiDAR\nthen take advantage of ex-isting LiDAR-based 3D object detection pipelines\n4. Realtime 3D Obj Detection on Mobile Dev with Media Pipe (Google, 2020) Introduction : detects objects in 2D images, and estimates their poses and sizes through a ML model, trained on a newly created 3D dataset\nObtaining real world 3D Training Data : using ARCore, Google\u0026rsquo;s AR Platform, integrated in most smartphones, which can provide the information of the camera pose, sparse 3D point clouds, estimated lighting, and planar surfaces.\nAR Synthetic Data Generation : places virtual objects into scenes that have AR session data.\nAn ML Pipeline : a single-stage model to predict the pose and physical size of an object from a single RGB image\n5. SVGA-Net(2020) Introduction : graph based methods for point clouds data, 2020 SOTA in KITTI datasets\nMethods : Spars Voxel Graph Attention Network\nAppendix. LiDAR, ToF and Point Clouds Introduction : LiDAR systems send out pulses of light just outside the visible spectrum and register how long it takes each pulse to return. Once the individual readings are processed and organised, the LiDAR data becomes point cloud data. A 3D point cloud is a collection of data points analogous to the real world in three dimensions. Each point is defined by its own position and (sometimes) colour. Photogrammetric point clouds : have an RGB value for each point, resulting in a colourised point cloud (acqutision using digital camera rather than LiDAR, worse than LiDAR in terms of accuracy). Difference between LiDAR and TOF : A ToF(most android phones) is a scannerless LiDAR system, relying on a single pulse of light to map an entire space, while Apple is using scanner LiDAR, which uses multiple points of light to take these readings much more frequently and with greater accuracy. LiDAR \u0026amp; TOF in mobile : LG G8 (2019) , Galaxy Note10, S20, IPAD pro 4, iphone12 , Huawei Mate30, Sony Xperia2 Appendix. AR Applications on smartphones Google AR Core Depth API : Single Camera Depth Estimation\nIKEA Place : allows the user to directly insert furniture into a picture of the room\nmodiFace : virtual beauty counter\nmeasure kit : AR Ruler\npixie : find lost key through the iPhone screen. (with Bluetooth hardware)\nmagic sudoku : Solve sudoku puzzles in AR\nAppendix. Stereo/monocular image based depth Estimation Benchmarks\nNYU Depth (Mono) : includes 1449 densely labeled pairs of aligned RGB and depth images KITTI Eigen Split (Mono) : part of the KITTI dataset proposed by D.Eigen et al. for monocular depth estimation task BTS (monocular) : a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase.\nDORN (monocular) : combine multi-scale features with ordinal regression to predictpixel depth with remarkably low errors\nPSMNet : applies Siamese networks for disparity estimation, followed by 3D convolutions for refinement\nY Wang et al. : has made these methods more efficient, enabling accurate disparity estimation to run at 30 FPS on mobile devices.\nAppendix. 3D Modeling IKEA 3D Models : includes a lot of simple CAD models, but there\u0026rsquo;s no additional information required for detection like RGBD, point clouds, etc. These 3D Models can be used to estimate the 3D pose of specific objects in image ( Lim et al. ICCV2013 ) ","id":14,"section":"Research","summary":"Introduction Tasks : 3D object detection : classifies the object category and estimates oriented 3D bounding boxes of physical objects from 3D sensor data. Applications : By extending prediction to 3D, one can capture an object’s size, position and orientation in the world, leading to a variety of applications in robotics, self-driving vehicles, image retrieval, and augmented reality Benchmarks KITTI (car, cyclist, pedestrian","tags":null,"title":"MLCV #9 | 3D Object Detection","uri":"https://koreanbear89.github.io/research/3.-computer-vision/cv09-3d-object-detection/","year":"2020"},{"content":"\n10. Dimensionality Reduction with PCA Introduction : Working directly with high-dimensional data comes with some difficulties It is hard to analyze, interpretation is difficult, visualization is nearly impossible, and storage of the data vectors can be expensive In this chapter, we derive PCA from first principles, drawing on our understanding of basis and basis change (Sections 2.6.1 and 2.7.2), projections (Section 3.8), eigenvalues (Section 4.2), Gaussian distributions (Section 6.5), and constrained optimization (Section 7.2) 10.1 Problem Setting Objective : is to find projections $\\hat{x_n}$ of data points $x_n$ that are as similar to the original data points as possible, but which have a significantly lower intrinsic dimensionality.\nDataset : we consider an i.i.d. dataset $X = [x_1 , . . . , x_N], x_n ∈ \\mathbb{R}^D,$ with mean 0\npossesses the data covariance matrix $S$ (6.42)\na low-dimensional compressesd representation $z_n$ with the projection matrix $B$\n$$ S = \\frac{1}{N} \\sum{x_n}{x_n^⊤} $$\n$$ z_n = B^⊤x_n ∈ \\mathbb{R}^M $$\n$$ B := [b_1,\u0026hellip;,b_M] ∈ \\mathbb{R}^{D×M} $$\nOverview\nWe assume that the columns of $B$ are orthonormal (Definition 3.7) so that $b^⊤_i b_j =0 $ if and only if $i \\neq j$ and $b^⊤_i b_i =1$.\nWe seek an M-dimensional subspace $U ⊆ \\mathbb{R}^D,$ $dim(U)=M \u0026lt; D $ onto which we project the data.\nWe denote the projected data by $\\hat{x}_n ∈ U,$ and their coordinates (with respect to the basis vectors $b_1,\u0026hellip;,b_M$ of $U$ by $z_n. $\nOur aim is to find projections $ \\hat{x_n} ∈ \\mathbb{R}^D $ (or equivalently the codes $z_n$ and the basis vectors $b_1, . . . , b_M $) so that they are as similar to the original data $x_n$ and minimize the loss due to compression.\n10.2 Maximum Variance Perspective Introduction : We know that the variance is an indicator of the spread of the data, (Section 6.4.1), And we can derive PCA as a dimensionality reduction algorithm that maximizes the variance in the low-dimensional representation of the data to retain as much information as possible. Centered Data : the variance of the low-dimensional code does not depend on the mean of the data (eq 10.6) Therefore, we assume without loss of generality that the data has mean $0$. 10.2.1 Direction with Maximal Variance (1) Objective : maximize the variance $V_1$ of dataset projected onto single vector $b_1$\n(2) Variance $V_1$ : expectation of the squared deviation\nDeviation $z_{1n}$ : $b_1^⊤ x_n$ (section 3.8)\n$$ V_1=\\frac{1}{N} \\sum z_{1n}^2 = \\frac{1}{N} \\sum b^⊤_1 x_n x_n^⊤ b_1 = b^⊤_1 (\\frac{1}{N} \\sum x_nx_n^⊤)b_1 = b^⊤_1 S b_1 $$\n(3) Maximize Variance : solve the constrained optimization problem (section 7.2)\nby Setting the partial derivatives to 0, we see that $b_1$ should be an eigenvector of the data covariance matrix $S$ $$ Sb_1 = λ_1 b_1 $$\n(4) Eigenvalue of Covariance Matrix : is same with the variance ($V_1$) of the data projected onto a 1D subspace spanned by $b_1$\n$$ V_1 = b^⊤_1 S b_1 = b^⊤_1 λ_1 b_1 = λ_1 $$\n(5) Principal Component : the basis vector associated with the largest eigenvalue of the data covariance matrix\nTherefore, to maximize the variance, we should find the basis vector associated with the largest eigenvalue (Principal Components), and then project given data onto them. 10.2.2 M-dimensional Subspace with Maximal Variance Extending to $M$dimensional subspace:\nThe variance of the data, when projected onto an $M$ dimensional subspace, equals the sum of the eigenvalues that are associated with the corresponding eigenvectors of the data covariance matrix. $$ V_m = b_m^⊤ S b_m = λ_m b_m^⊤ b_m = λ_m . $$\n10.3 Projection Perspective Introduction :Not something new, we will just derive PCA in another view point\nIn the previous section, we derived PCA by maximizing the variance in the projected space to retain as much information as possible.\nIn the following, we will look at the difference vectors between the original data $x_n$ and their projection $\\tilde{x}_n$ and minimize this distance so that $x_n$ and $\\tilde{x}_n$ are as close as possible.\n10.3.1 Setting and Objective Objective : minimizing the average squared Euclidean distance between $x$ and $\\tilde{x}_n$ (projection error) $$ J_M := \\frac{1}{N} \\sum ∥ x_n − \\tilde{x}_n∥^2 $$\nSolution : To find the coordinates $z_n$ and the OrthoNormalBasis of the principal subspace, we follow a two-step approach (1) we optimize the coordinates $z_n$ for a given ONB (10.3.2) (2) second, we find the optimal ONB. (10.3.3) 10.3.2 Finding Optimal Coordinates Introduction : Let us start by finding the optimal coordinates $z_{1n} , . . . , z_{Mn}$ of the projections $\\tilde{x}_n$ for $n = 1,\u0026hellip;,N$ that minimizes the distance between $\\tilde{x} − x$.\nConsequence :\nThe optimal linear projection $\\tilde{x}_n$ of $x_n$ is an orthogonal projection. The coordinates of $\\tilde{x}_n$ with respect to the basis $(b_1,\u0026hellip;,b_M) $ are the coordinates of the orthogonal projection of $x_n$ onto the principal subspace. An orthogonal projection is the best linear mapping given the objective. Summary :\nSo far we have shown that for a given ONB we can find the optimal coordinates by an orthogonal projection onto the principal subspace.\nIn the following, we will determine what the best basis is.\n10.3.3 Finding the Basis of the Principal Subspace Consequence: after solving the equation from (10.35) to (10.43b), we finally get the consequences below.\nMinimizing the average squared reconstruction (projection) error is equivalent to minimizing the variance of the data when projected onto the subspace we ignore, i.e., the orthogonal complement of the principal subspace The basis of the principal subspace comprises the eigenvectors $b_1, \u0026hellip; b_M$ that are associated with the largest M eigenvalues of the data covariance matrix. 10.4 Eigenvector Computation and Low-Rank Approximations Introduction\nIn the previous sections, we obtained the basis of the principal subspace as the eigenvectors that are associated with the largest eigenvalues of the data covariance matrix To get the eigenvalues (and the corresponding eigenvectors) of covmat\nWe perform an eigendecomposition (see Section 4.2)\nWe use a singular value decomposition (see Section 4.5)\nResults : The relationship between the eigenvalues of $S$ and the singular values of $X$ provides the connection between the maximum variance view (Section 10.2) and the singular value decomposition.\n10.4.1. PCA using Low-Rank Matrix Approximations Eckart-Young theorem: states that $\\tilde{X}_M$, is given by truncating the SVD at the top-M singular value.\nDiagonal entries of diagonal matrix $\\Sigma_m$ are the $M$ largest singular values of $X$ $$ \\tilde{X}_M = U_M \\Sigma_M V^T_M $$\n10.4.2. Practical Aspects Find eigenvalues and eigenvectors :\n(1) Roots of the characteristic polynomial (section 4.2) : practically impossible for large matrix.\n(2) eigendecomposition (or SVD) : we only require a few eigenvectors, so It would be wasteful to compute the full decomposition\n(3) Iterative methods : If we are interested in only the first few eigenvectors, then iterative processes, which directly optimize these eigenvectors, are computationally efficient\nPower Iteration (Google PageRank) : is very efficient In the extreme case of only needing the first eigenvector. 10.5 PCA in High Dimensions Introduction : In the following, we provide a solution to the problem for the case that we have substantially fewer data points than dimensions\nThe number of data $N$ is smaller than the dimension of the data $D$. The rank of the covmat is $N$, $S$ has $D-N+1$ eigenvalues are zero. We will exploit this and turn the $D×D$ covariance matrix into an $N × N$ covariance matrix whose eigenvalues are all positive. Consequences : we can compute the eigenvalues and eigenvectors much more efficiently than for the original $D \\times D$ data covmat\n10.6 Key Steps of PCA in Practice Introduction : n the following, we will go through the individual steps of PCA using a running example, which is summarized in Figure 10.11 (1) Mean Subtraction : centering the data by computing the mean and subtracting it from every single data point =\u0026gt; dataset has zero-mean (2) Standardization : Divide the datapoints by the standard deviation $σ_d$ of the dataset for every dimension. (3) Eigendecomposition of the covariance matrix : Compute the data covariance matrix and its eigenvalues and corresponding eigenvectors (4) Projection : We can project any data point $x_∗ ∈ R^D$ onto the principal subspace Fig10.11 Steps of PCA 10.7 Latent Variable Perspective Introduction : In the previous sections, we derived PCA without any notion of a probabilistic model Probabilistic PCA : By introducing a continuous-valued latent variable $z ∈ R^M$ it is possible to phrase PCA as a probabilistic latent-variable model (skip details) Summary In this chapter, we will discuss principal component analysis, an algorithm for linear dimensionality reduction\nSection 10.1, \u0008Problem Setting\nOur objective is to find projections $\\hat{x_n}$ of data points $x_n$ that are as similar to the original data points as possible, but which have a significantly lower intrinsic dimensionality. Section 10.2, Maximum Variance Perspective\nWe saw PCA as a way of dim-reduction that results in the largest variance after projection\nthe variance of projected vectors are same with the eigenvalue of covariance matrix\nSo we can find the best basis vector to project by choose the vector whose eigenvalue is the largest\nSection 10.3, Projection Perspective\nWe looked at the difference vectors between the original data $x_n$ and their projection $\\tilde{x}_n$ and minimize this distance so that $x_n$ and $\\tilde{x}_n$ are as close as possible.\nObjective : minimizing the average squared Euclidean distance between $x$ and $\\tilde{x}_n$ (projection error)\nAn orthogonal projection is the best linear mapping given the objective.\nSection 10.4, Eigenvector Computation and Low-Rank Approximations\nWe saw computational efficient ways to find Eigenvector in practical aspect. Section 10.5, PCA in high dimensions\nWe provided a solution to the problem for the case that we have substantially fewer data points than dimensions Section 10.6, Key steps of PCA in practice\n(1) Mean Subtraction\n(2) Standardization\n(3) Eigendecomposition of the covariance matrix\n(4) Projection\nsection 10.7, Latent Variable Perspective\nWe saw PCA as probabilistic latent-variable model ","id":15,"section":"Mathematics","summary":"10. Dimensionality Reduction with PCA Introduction : Working directly with high-dimensional data comes with some difficulties It is hard to analyze, interpretation is difficult, visualization is nearly impossible, and storage of the data vectors can be expensive In this chapter, we derive PCA from first principles, drawing on our understanding of basis and basis change (Sections 2.6.1 and 2.7.2), projections (Section 3.8), eigenvalues (Section 4.2), Gaussian distributions (Section 6.5), and constrained optimization (Section 7.","tags":null,"title":"Mathematcs for ML #10 | Dimensionality Reduction with PCA","uri":"https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml10-dimensionality-reduction-and-pca/","year":"2022"},{"content":"\nIntroduction Functions in Python have a variety of extra features that make a programmer’s life easier.\nSome are similar to capabilities in other programming languages, but many are unique to Python. These extras can make a function’s purpose more obvious.\nThey can eliminate noise and clarify the intention of callers.\nThey can significantly reduce subtle bugs that are difficult to find.\nItem 19. Never Unpack More Than Three Variables When Functions Return Multiple Values You can have functions return multiple values by putting them in a tuple and having the caller take advantage of Python’s unpacking syntax.\nMultiple return values from a function can also be unpacked by catch-all starred expressions.\nUnpacking into four or more variables is error prone and should be avoided; instead, return a small class or namedtuple instance.\n# example def get_avg_ratio(numbers): average = sum(numbers) / len(numbers) scaled = [x / average for x in numbers] scaled.sort(reverse=True) return scaled longest, *middle, shortest = get_avg_ratio(lengths) Item 12. Avoid Striding and Slicing in a Single Expression Specifying start, end, and stride in a slice can be extremely confusing.\nPrefer using positive stride values in slices without start or end indexes. Avoid negative stride values if possible.\nAvoid using start, end, and stride together in a single slice. If you need all three parameters, consider doing two assignments (one to stride and another to slice) or using islice from the itertools built-in module.\nItem 13: Prefer Catch-All Unpacking Over Slicing Unpacking assignments may use a starred expression to catch all values that weren’t assigned to the other parts of the unpacking pattern into a list. Starred expressions may appear in any position, and they will always become a list containing the zero or more values they receive. When dividing a list into non-overlapping pieces, catch-all unpacking is much less error prone than slicing and indexing. # AS-IS oldest = car_ages_descending[0] second_oldest = car_ages_descending[1] others = car_ages_descending[2:] # TO-BE oldest, second_oldest, *others = car_ages_descending Item14. Sort by Complex Criteria Using the key Parameter The sort method of the list type can be used to rearrange a list’s contents by the natural ordering of built-in types like strings, integers, tuples, and so on.\nThe sort method doesn’t work for objects unless they define a natural ordering using special methods, which is uncommon.\nThe key parameter of the sort method can be used to supply a helper function that returns the value to use for sorting in place of each item from the list.\nReturning a tuple from the key function allows you to combine multiple sorting criteria together. The unary minus operator can be used to reverse individual sort orders for types that allow it.\nFor types that can’t be negated, you can combine many sorting criteria together by calling the sort method multiple times using different key functions and reverse values, in the order of lowest rank sort call to highest rank sort call.\n# 1) natural ordering numbers = [93, 86, 11, 68, 70] numbers.sort() # \u0026gt;\u0026gt; [11, 68, 70, 86, 93] # 3) sorting by key param tools = [ Tool('level', 3.5),Tool('hammer', 1.25), Tool('screwdriver', 0.5), Tool('chisel', 0.25) ] tools.sort(key=lambda x: x.name) # 4) multiple criteria, reverse sort power_tools = [Tool('drill', 4), Tool('circular saw', 5), Tool('jackhammer', 40),Tool('sander', 4),] power_tools.sort(key=lambda x: (x.weight, x.name)) # \u0026gt;\u0026gt; Tool('drill',4), Tool('sander',4), Tool('circular saw', 5), Tool('jackhammer', 40) power_tools.sort(key=lambda x: (-x.weight, x.name)) # \u0026gt;\u0026gt; Tool('jackhammer', 40), Tool('circular saw', 5), Tool('drill', 4), Tool('sander', 4) Item15. Be Cautious When Relying on dict Insertion Ordering Since Python 3.7, you can rely on the fact that iterating a dict instance’s contents will occur in the same order in which the keys were initially added.\nPython makes it easy to define objects that act like dictionaries but that aren’t dict instances. For these types, you can’t assume that insertion ordering will be preserved.\nThere are three ways to be careful about dictionary-like classes:\nWrite code that doesn’t rely on insertion ordering,\nexplicitly check for the dict type at runtime,\nor require dict values using type annotations and static analysis.\nItem16. Prefer get Over in and KeyError to Handle Missing Dictionary Keys There are four common ways to detect and handle missing keys in dictionaries: using in expressions, KeyError exceptions, the get method, and the setdefault method.\nThe get method is best for dictionaries that contain basic types like counters, and it is preferable along with assignment expressions when creating dictionary values has a high cost or may raise exceptions.\nWhen the setdefault method of dict seems like the best fit for your problem, you should consider using defaultdict instead. (Item 17)\n# before : in if key in counters: count = counters[key] else: count = 0 counters[key] = count + 1 # before : KeyError try: count = counters[key] except KeyError: count = 0 counters[key] = count + 1 # after : get count = counters.get(key, 0) counters[key] = count + 1 Item17. Prefer defaultdict Over setdefault to Handle Missing Items in Internal State If you’re creating a dictionary to manage an arbitrary set of potential keys, then you should prefer using a defaultdict instance from the collections built-in module if it suits your problem.\nIf a dictionary of arbitrary keys is passed to you, and you don’t control its creation, then you should prefer the get method to access its items. However, it’s worth considering using the setdefault method for the few situations in which it leads to shorter code.\n# before : using \u0026quot;in\u0026quot; expression def countLetters(word): counter = {} for letter in word: if letter not in counter: counter[letter] = 0 counter[letter] += 1 return counter # after1 : using dict.setderault() def countLetters(word): counter = {} for letter in word: # flaw : call setdefalut for every loop iteration counter.setdefault(letter, 0) counter[letter] += 1 return counter # after2 : using collections.defaultdict from collections import defaultdict def countLetters(word): counter = defaultdict(int) for letter in word: counter[letter] += 1 return counter Item 18. Know How to Construct Key-Dependent Default Values with __missing__ The setdefault method of dict is a bad fit when creating the default value has high computational cost or may raise exceptions.\nThe function passed to defaultdict must not require any arguments, which makes it impossible to have the default value depend on the key being accessed.\nYou can define your own dict subclass with a __missing__ method in order to construct default values that must know which key was being accessed.\n","id":16,"section":"Engineering","summary":"Introduction Functions in Python have a variety of extra features that make a programmer’s life easier.\nSome are similar to capabilities in other programming languages, but many are unique to Python. These extras can make a function’s purpose more obvious.\nThey can eliminate noise and clarify the intention of callers.\nThey can significantly reduce subtle bugs that are difficult to find.\nItem 19. Never Unpack More Than Three Variables When Functions Return Multiple Values You can have functions return multiple values by putting them in a tuple and having the caller take advantage of Python’s unpacking syntax.","tags":null,"title":"Effective Python | #3 Functions","uri":"https://koreanbear89.github.io/engineering/9.-study/effective-python-03/","year":"2023"},{"content":"\nIntroduction In Python, the most common way to automate repetitive tasks is by using a sequence of values stored in a list type.\nA natural complement to lists is the dict type, which stores lookup keys mapped to corresponding values (in what is often called an associative array or a hash table\nItem 11. Know How to Slice Sequences Avoid being verbose when slicing: Don’t supply 0 for the start index or the length of the sequence for the end index.\nSlicing is forgiving of start or end indexes that are out of bounds, which means it’s easy to express slices on the front or back boundaries of a sequence (like a[:20] or a[-20:])\nAssigning to a list slice replaces that range in the original sequence with what’s referenced even if the lengths are different.\nItem 12. Avoid Striding and Slicing in a Single Expression Specifying start, end, and stride in a slice can be extremely confusing.\nPrefer using positive stride values in slices without start or end indexes. Avoid negative stride values if possible.\nAvoid using start, end, and stride together in a single slice. If you need all three parameters, consider doing two assignments (one to stride and another to slice) or using islice from the itertools built-in module.\nItem 13: Prefer Catch-All Unpacking Over Slicing Unpacking assignments may use a starred expression to catch all values that weren’t assigned to the other parts of the unpacking pattern into a list. Starred expressions may appear in any position, and they will always become a list containing the zero or more values they receive. When dividing a list into non-overlapping pieces, catch-all unpacking is much less error prone than slicing and indexing. # AS-IS oldest = car_ages_descending[0] second_oldest = car_ages_descending[1] others = car_ages_descending[2:] # TO-BE oldest, second_oldest, *others = car_ages_descending Item14. Sort by Complex Criteria Using the key Parameter The sort method of the list type can be used to rearrange a list’s contents by the natural ordering of built-in types like strings, integers, tuples, and so on.\nThe sort method doesn’t work for objects unless they define a natural ordering using special methods, which is uncommon.\nThe key parameter of the sort method can be used to supply a helper function that returns the value to use for sorting in place of each item from the list.\nReturning a tuple from the key function allows you to combine multiple sorting criteria together. The unary minus operator can be used to reverse individual sort orders for types that allow it.\nFor types that can’t be negated, you can combine many sorting criteria together by calling the sort method multiple times using different key functions and reverse values, in the order of lowest rank sort call to highest rank sort call.\n# 1) natural ordering numbers = [93, 86, 11, 68, 70] numbers.sort() # \u0026gt;\u0026gt; [11, 68, 70, 86, 93] # 3) sorting by key param tools = [ Tool('level', 3.5),Tool('hammer', 1.25), Tool('screwdriver', 0.5), Tool('chisel', 0.25) ] tools.sort(key=lambda x: x.name) # 4) multiple criteria, reverse sort power_tools = [Tool('drill', 4), Tool('circular saw', 5), Tool('jackhammer', 40),Tool('sander', 4),] power_tools.sort(key=lambda x: (x.weight, x.name)) # \u0026gt;\u0026gt; Tool('drill',4), Tool('sander',4), Tool('circular saw', 5), Tool('jackhammer', 40) power_tools.sort(key=lambda x: (-x.weight, x.name)) # \u0026gt;\u0026gt; Tool('jackhammer', 40), Tool('circular saw', 5), Tool('drill', 4), Tool('sander', 4) Item15. Be Cautious When Relying on dict Insertion Ordering Since Python 3.7, you can rely on the fact that iterating a dict instance’s contents will occur in the same order in which the keys were initially added.\nPython makes it easy to define objects that act like dictionaries but that aren’t dict instances. For these types, you can’t assume that insertion ordering will be preserved.\nThere are three ways to be careful about dictionary-like classes:\nWrite code that doesn’t rely on insertion ordering,\nexplicitly check for the dict type at runtime,\nor require dict values using type annotations and static analysis.\nItem16. Prefer get Over in and KeyError to Handle Missing Dictionary Keys There are four common ways to detect and handle missing keys in dictionaries: using in expressions, KeyError exceptions, the get method, and the setdefault method.\nThe get method is best for dictionaries that contain basic types like counters, and it is preferable along with assignment expressions when creating dictionary values has a high cost or may raise exceptions.\nWhen the setdefault method of dict seems like the best fit for your problem, you should consider using defaultdict instead. (Item 17)\n# before : in if key in counters: count = counters[key] else: count = 0 counters[key] = count + 1 # before : KeyError try: count = counters[key] except KeyError: count = 0 counters[key] = count + 1 # after : get count = counters.get(key, 0) counters[key] = count + 1 Item17. Prefer defaultdict Over setdefault to Handle Missing Items in Internal State If you’re creating a dictionary to manage an arbitrary set of potential keys, then you should prefer using a defaultdict instance from the collections built-in module if it suits your problem.\nIf a dictionary of arbitrary keys is passed to you, and you don’t control its creation, then you should prefer the get method to access its items. However, it’s worth considering using the setdefault method for the few situations in which it leads to shorter code.\n# before : using \u0026quot;in\u0026quot; expression def countLetters(word): counter = {} for letter in word: if letter not in counter: counter[letter] = 0 counter[letter] += 1 return counter # after1 : using dict.setderault() def countLetters(word): counter = {} for letter in word: # flaw : call setdefalut for every loop iteration counter.setdefault(letter, 0) counter[letter] += 1 return counter # after2 : using collections.defaultdict from collections import defaultdict def countLetters(word): counter = defaultdict(int) for letter in word: counter[letter] += 1 return counter Item 18. Know How to Construct Key-Dependent Default Values with __missing__ The setdefault method of dict is a bad fit when creating the default value has high computational cost or may raise exceptions.\nThe function passed to defaultdict must not require any arguments, which makes it impossible to have the default value depend on the key being accessed.\nYou can define your own dict subclass with a __missing__ method in order to construct default values that must know which key was being accessed.\n","id":17,"section":"Engineering","summary":"Introduction In Python, the most common way to automate repetitive tasks is by using a sequence of values stored in a list type.\nA natural complement to lists is the dict type, which stores lookup keys mapped to corresponding values (in what is often called an associative array or a hash table\nItem 11. Know How to Slice Sequences Avoid being verbose when slicing: Don’t supply 0 for the start index or the length of the sequence for the end index.","tags":null,"title":"Effective Python | #2 Lists and Dictionaries","uri":"https://koreanbear89.github.io/engineering/9.-study/effective-python-02/","year":"2023"},{"content":"\nIntroduction The Pythonic style isn’t regimented or enforced by the compiler. It has emerged over time through experience using the language and working with others.\nPython programmers prefer to be explicit, to choose simple over complex, and to maximize readability. Item 1. Know Which Version of Python You’re Using Throughout this book, the majority of example code is in the syntax of Python 3.7 (released in June 2018).\nThis book also provides some examples in the syntax of Python 3.8 (released in October 2019)\nItem 2. Follow the PEP 8 Style Guide Always follow the Python Enhancement Proposal #8 (PEP 8) style guide when writing Python code.\nSharing a common style with the larger Python community facili- tates collaboration with others.\nUsing a consistent style makes it easier to modify your own code later.\nWhitespace\nIn a file, functions and classes should be separated by two blank lines.\nIn a class, methods should be separated by one blank line.\nNaming\nProtected instance attributes should be in _leading_underscore format.\nPrivate instance attributes should be in __double_leading_underscore format.\nImports\nImports should be in sections in the following order: standard library modules, third-party modules, your own modules. Each subsection should have imports in alphabetical order. Item 3. Know the Differences Between bytes and str bytes contains sequences of 8-bit values, and str contains sequences of Unicode code points.\nstr.encode() : convert Unicode data to binary data\nbytes.decode() : convert binary data to Unicode data\na = b'h\\x65llo' print(list(a)) # [104, 101, 108, 108, 111] print(a) # b'hello' a = 'a\\u0300 propos' print(list(a)) # ['a', '`', ' ', 'p', 'r', 'o', 'p', 'o', 's'] print(a) # à propos Use helper functions to ensure that the inputs you operate on are the type of character sequence that you expect (8-bit values, UTF-8-encoded strings, Unicode code points, etc). # always returns a str def to_str(bytes_or_str): if isinstance(bytes_or_str, bytes): value = bytes_or_str.decode('utf-8') else: value = bytes_or_str return value # Instance of str # always returns a bytes def to_bytes(bytes_or_str): if isinstance(bytes_or_str, str): value = bytes_or_str.encode('utf-8') else: value = bytes_or_str return value # Instance of bytes bytes and str instances can’t be used together with operators (like \u0026gt;, ==, +, and %).\nIf you want to read or write binary data to/from a file, always open the file using a binary mode (like \u0026lsquo;rb\u0026rsquo; or \u0026lsquo;wb\u0026rsquo;).\nIf you want to read or write Unicode data to/from a file, be careful about your system’s default text encoding. Explicitly pass the encoding parameter to open if you want to avoid surprises.\nUnicodeDecodeError: 'utf-8' codec can't decode byte ... : the file was opened in read text mode (\u0026lsquo;r\u0026rsquo;) instead of read binary mode (\u0026lsquo;rb\u0026rsquo;). When a handle is in text mode, it uses the system’s default text encoding (utf-8) to interpret binary data Item4. Prefer Interpolated F-Strings Over C-style Format Strings and str.format C-style format strings that use the % operator suffer from a variety of gotchas and verbosity problems.\nThe str.format method introduces some useful concepts in its formatting specifiers mini language, but it otherwise repeats the mistakes of C-style format strings and should be avoided.\nF-strings are a new syntax for formatting values into strings that solves the biggest problems with C-style format strings.\nF-strings are succinct yet powerful because they allow for arbitrary Python expressions to be directly embedded within format specifiers.\nItem5. Write Helper Functions Instead of Complex Expressions Python’s syntax makes it easy to write single-line expressions that are overly complicated and difficult to read.\nMove complex expressions into helper functions, especially if you need to use the same logic repeatedly.\nAn if/else expression provides a more readable alternative to using the Boolean operators or and and in expressions.\n# before red = my_values.get('red', [''])[0] or 0 green = my_values.get('green', [''])[0] or 0 opacity = my_values.get('opacity', [''])[0] or 0 # after def get_first_int(values, key, default=0): found = values.get(key, ['']) if found[0]: return int(found[0]) return default \u0008Item6. Prefer Multiple Assignment Unpacking Over Indexing Python has special syntax called unpacking for assigning multiple values in a single statement.\nUnpacking is generalized in Python and can be applied to any iterable, including many levels of iterables within iterables.\nReduce visual noise and increase code clarity by using unpacking to avoid explicitly indexing into sequences.\nitem = ('Peanut butter', 'Jelly') first, second = item # Unpacking Item7. Prefer enumerate Over range enumerate provides concise syntax for looping over an iterator and getting the index of each item from the iterator as you go\nPrefer enumerate instead of looping over a range and indexing into a sequence.\nYou can supply a second parameter to enumerate to specify the number from which to begin counting (zero is the default).\n# before for i in range(len(flavor_list)): flavor = flavor_list[i] print(f'{i + 1}: {flavor}') # after for i, flavor in enumerate(flavor_list, 1): print(f'{i}: {flavor}') Item 8. Use zip to Process Iterators in Parallel The zip built-in function can be used to iterate over multiple iterators in parallel.\nzip creates a lazy generator that produces tuples, so it can be used on infinitely long inputs.\nzip truncates its output silently to the shortest iterator if you supply it with iterators of different lengths.\nUse the zip_longest function from the itertools built-in module if you want to use zip on iterators of unequal lengths without truncation.\nItem 9: Avoid else Blocks After for and while Loops Python has special syntax that allows else blocks to immediately follow for and while loop interior blocks.\nThe else block after a loop runs only if the loop body did not encounter a break statement.\nAvoid using else blocks after loops because their behavior isn’t intuitive and can be confusing\nItem 10: Prevent Repetition with Assignment Expressions Assignment expressions use the walrus operator (:=) to both assign and evaluate variable names in a single expression, thus reducing repetition.\nWhen an assignment expression is a subexpression of a larger expression, it must be surrounded with parentheses.\nAlthough switch/case statements and do/while loops are not available in Python, their functionality can be emulated much more clearly by using assignment expressions.\n# before while True: fresh_fruit = pick_fruit() if not fresh_fruit: break ... # after while fresh_fruit := pick_fruit(): ... ","id":18,"section":"Engineering","summary":"Introduction The Pythonic style isn’t regimented or enforced by the compiler. It has emerged over time through experience using the language and working with others.\nPython programmers prefer to be explicit, to choose simple over complex, and to maximize readability. Item 1. Know Which Version of Python You’re Using Throughout this book, the majority of example code is in the syntax of Python 3.7 (released in June 2018).\nThis book also provides some examples in the syntax of Python 3.","tags":null,"title":"Effective Python | #1 Pythonic Thinking","uri":"https://koreanbear89.github.io/engineering/9.-study/effective-python-01/","year":"2023"},{"content":"\n1. Introduction GlusterFS : is a Scalable Network Filesystem Brick : is the basic unit of storage in GlusterFS, Volume : is a logical collection of bricks. Create volume with bricks from several nodes (peer) And mount volume to the specific location to use volume like virtual Storage References Tutorial: Create a Docker Swarm with Persistent Storage Using GlusterFS – The New Stack # install GlusterFS on CentOS sudo yum install centos-release-gluster sudo yum install glusterfs-server sudo systemctl start glusterd # start daemon sudo systemctl enable glusterd # restart daemon when reboot the machine # Configure Gluster Volume gluster peer probe \u0026lt;HOST_NAME\u0026gt; # connect HOST as peer gluster pool list # show gluster peers # Create Volume sudo mkdir -p /\u0026lt;BRICK_DIR\u0026gt; # run on all machines sudo gluster volume create \u0026lt;VOLUME_NAME\u0026gt; replica \u0026lt;N\u0026gt; \u0026lt;HOST1\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST2\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST3\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; force # run only on the master # ex) sudo gluster volume create gluster_pvs replica 4 pgsca2x0350:/home1/irteam/whome/__oss/gs_pvs pgsca2x0351:/home1/irteam/whome/__oss/gs_pvs pgsca2x0352:/home1/irteam/whome/__oss/gs_pvs pgsca2x0353:/home1/irteam/whome/__oss/gs_pvs force # Start Volume sudo gluster volume start \u0026lt;VOLUME_NAME\u0026gt; # run only on the master sudo mount.glusterfs localhost:/\u0026lt;VOLUME_NAME\u0026gt; \u0026lt;MOUNT_LOCATION\u0026gt; # run on all machines # ex) sudo mount.glusterfs localhost:/gluster_pvs /home1/irteam/mnt_gluster # Manage Gluster Volumes gluster volume info # show information of volume and bricks gluster volume stop \u0026lt;VOLUME_NAME\u0026gt; gluster volume delete \u0026lt;VOLUME_NAME\u0026gt; ","id":19,"section":"Engineering","summary":"1. Introduction GlusterFS : is a Scalable Network Filesystem Brick : is the basic unit of storage in GlusterFS, Volume : is a logical collection of bricks. Create volume with bricks from several nodes (peer) And mount volume to the specific location to use volume like virtual Storage References Tutorial: Create a Docker Swarm with Persistent Storage Using GlusterFS – The New Stack # install GlusterFS on CentOS sudo yum install centos-release-gluster sudo yum install glusterfs-server sudo systemctl start glusterd # start daemon sudo systemctl enable glusterd # restart daemon when reboot the machine # Configure Gluster Volume gluster peer probe \u0026lt;HOST_NAME\u0026gt; # connect HOST as peer gluster pool list # show gluster peers # Create Volume sudo mkdir -p /\u0026lt;BRICK_DIR\u0026gt; # run on all machines sudo gluster volume create \u0026lt;VOLUME_NAME\u0026gt; replica \u0026lt;N\u0026gt; \u0026lt;HOST1\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST2\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; \u0026lt;HOST3\u0026gt;:\u0026lt;BRICK_DIR\u0026gt; force # run only on the master # ex) sudo gluster volume create gluster_pvs replica 4 pgsca2x0350:/home1/irteam/whome/__oss/gs_pvs pgsca2x0351:/home1/irteam/whome/__oss/gs_pvs pgsca2x0352:/home1/irteam/whome/__oss/gs_pvs pgsca2x0353:/home1/irteam/whome/__oss/gs_pvs force # Start Volume sudo gluster volume start \u0026lt;VOLUME_NAME\u0026gt; # run only on the master sudo mount.","tags":null,"title":"CheatSheet | GlusterFS","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-glusterfs/","year":"2022"},{"content":"\nIntroduction \u0026ldquo;#!\u0026rdquo;, defines shell to run script like #!/bin/sh\n/bin/sh is not anymore bash, dash (Ubuntu\u0026gt;6.06)\n$ ls -al /bin/sh lrwxrwxrwx 1 root root 4 6M 30 19:44 /bin/sh -\u0026gt; dash Operators \u0026lt; : input redirection, to redirect the input given \u0026gt; : output redirection, overwriting an already existing file or creates a new file if not exist. \u0026gt;\u0026gt; : output redirection, appends an already existing file or creates a new file if not exist. For loop for i in (seq 0 2 10) ; do echo \u0026quot;Running loop seq \u0026quot;$i # 0, 2, 4, 6, 8, 10 done Special Characters ~ : Home Directory $ : allocate value of variable \u0026amp; : for running background task # : annotation * : wild card ? : wild card , single character \\ : generalize next character ( : start child shell ) : finish child shell ! : NOT ; : separate shell command ``cmd` : reverse quotes, run command in command | : use pipe command output as input of other command [ : start string set ] : finish string set ( #ls [ a-d]*.sh ) ","id":20,"section":"Engineering","summary":"Introduction \u0026ldquo;#!\u0026rdquo;, defines shell to run script like #!/bin/sh\n/bin/sh is not anymore bash, dash (Ubuntu\u0026gt;6.06)\n$ ls -al /bin/sh lrwxrwxrwx 1 root root 4 6M 30 19:44 /bin/sh -\u0026gt; dash Operators \u0026lt; : input redirection, to redirect the input given \u0026gt; : output redirection, overwriting an already existing file or creates a new file if not exist. \u0026gt;\u0026gt; : output redirection, appends an already existing file or creates a new file if not exist.","tags":null,"title":"Shell | Snippet ","uri":"https://koreanbear89.github.io/engineering/2.-languages/shell-snippet/","year":"2021"},{"content":"\n1. Setup Jupyter-Lab $ conda install -c conda-forge jupyter jupyterlab nbconvert # nodejs\u0026gt;12.0.0 needed $ conda install -c conda-forge nodejs $ conda install -c conda-forge/label/gcc7 nodejs $ conda install -c conda-forge/label/cf201901 nodejs $ conda install -c conda-forge/label/cf202003 nodejs # v13.10 $ conda update nodejs # v16.13 # if above not workconda uninstall --force nodejs $ conda uninstall --force nodejs # remove all $ conda install nodejs -c conda-forge --repodata-fn=repodata.json $ pip install jupyterlab-unfold # theme $ pip install theme-darcular # do not need new node version $ conda install -c conda-forge theme-darcula $ jupyter labextension install @arbennett/base16-nord # theme ","id":21,"section":"Engineering","summary":"\n1. Setup Jupyter-Lab $ conda install -c conda-forge jupyter jupyterlab nbconvert # nodejs\u0026gt;12.0.0 needed $ conda install -c conda-forge nodejs $ conda install -c conda-forge/label/gcc7 nodejs $ conda install -c conda-forge/label/cf201901 nodejs $ conda install -c conda-forge/label/cf202003 nodejs # v13.10 $ conda update nodejs # v16.13 # if above not workconda uninstall --force nodejs $ conda uninstall --force nodejs # remove all $ conda install nodejs -c conda-forge --repodata-fn=repodata.json $ pip install jupyterlab-unfold # theme $ pip install theme-darcular # do not need new node version $ conda install -c conda-forge theme-darcula $ jupyter labextension install @arbennett/base16-nord # theme ","tags":null,"title":"CheatSheet | Jupyter","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-jupyter/","year":"2021"},{"content":"\nIntroduction reference. https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf # including kor/eng/num/- \u0026quot;[가-힣|a-z|0-9|\\-]+\u0026quot; 1. Special Characters ^ | Matches the expression to its right at the start of a string. It matches every such instance before each \\n in the string. $ | Matches the expression to its left at the end of a string. It matches every such instance before each \\n in the string. . | Matches any character except line terminators like `\\n`` ``` | Escapes special characters or denotes character classes A|B | Matches expression A or B. If A is matched first, B is left untried + | Greedily matches the expression to its left 1 or more times * | Greedily matches the expression to its left 0 or more times ? | Greedily matches the expression to its left 0 or 1 times. But if ? is added to qualifiers (+, *, and ? itself) it will perform matches in a non-greedy manner {m} | Matches the expression to its left m times, and not less {m,n} | Matches the expression to its left m to n times, and not less. {m,n}? | Matches the expression to its left m times, and ignores n. See ? above. 2. Character Classes (a.k.a. Special Sequences) \\w | Matches alphanumeric characters, which means a-z, A-Z, and 0-9. It also matches the underscore, _. \\d | Matches digits, which means 0-9. \\D | Matches any non-digits. \\s | Matches whitespace characters, which include the \\t, \\n, \\r, and space characters. \\S | Matches non-whitespace characters. \\b | Matches the boundary (or empty string) at the start and end of a word, that is, between \\w and \\W. \\B | Matches where \\b does not, that is, the boundary of \\w characters. \\A | Matches the expression to its right at the absolute start of a string whether in single or multi-line mode. \\Z | Matches the expression to its left at the absolute end of a string whether in single or multi-line mode. 3. Sets [ ] | Contains a set of characters to match. [amk] | Matches either a, m, or k. It does not match amk. [a-z] | Matches any alphabet from a to z. [a\\-z] | Matches a, -, or z. It matches - because \\ escapes it. [a-] | Matches a or -, because - is not being used to indicate a series of characters. [-a] | As above, matches a or -. [a-z0-9] | Matches characters from a to z and also from 0 to 9. [(+*)] | Special characters become literal inside a set, so this matches (, +, *, and ). [^ab5] | Adding ^ excludes any character in the set. Here, it matches characters that are not a, b, or 5. 4. Groups ( ) | Matches the expression inside the parentheses and groups it.\n(? ) | Inside parentheses like this, ? acts as an extension notation. Its meaning depends on the character immediately to its right.\n(?PAB) | Matches the expression AB, and it can be accessed with the group name.\n(?aiLmsux) | Here, a, i, L, m, s, u, and x are flags:\na — Matches ASCII only i — Ignore case L — Locale dependent m — Multi-line s — Matches all u — Matches unicode x — Verbose (?:A) | Matches the expression as represented by A, but unlike (?PAB), it cannot be retrieved afterwards.\n(?#...) | A comment. Contents are for us to read, not for matching.\nA(?=B) | Lookahead assertion. This matches the expression A only if it is followed by B.\nA(?!B) | Negative lookahead assertion. This matches the expression A only if it is not followed by B.\n(?\u0026lt;=B)A | Positive lookbehind assertion. This matches the expression A only if B is immediately to its left. This can only matched fixed length expressions.\n(?\u0026lt;!B)A | Negative lookbehind assertion. This matches the expression A only if B is not immediately to its left. This can only matched fixed length expressions.\n(?P=name) | Matches the expression matched by an earlier group named “name”.\n(...)\\1 | The number 1 corresponds to the first group to be matched. If we want to match more instances of the same expresion, simply use its number instead of writing out the whole expression again. We can use from 1 up to 99 such groups and their corresponding numbers.\n5. Popular Python re Module Functions re.findall(A, B) | Matches all instances of an expression A in a string B and returns them in a list. re.search(A, B) | Matches the first instance of an expression A in a string B, and returns it as a re match object. re.split(A, B) | Split a string B into a list using the delimiter A. re.sub(A, B, C) | Replace A with B in the string C. ","id":22,"section":"Engineering","summary":"Introduction reference. https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf # including kor/eng/num/- \u0026quot;[가-힣|a-z|0-9|\\-]+\u0026quot; 1. Special Characters ^ | Matches the expression to its right at the start of a string. It matches every such instance before each \\n in the string. $ | Matches the expression to its","tags":null,"title":"CheatSheet | Regex","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-regex/","year":"2021"},{"content":"\nIntroduction Nginx : light-weight Web server\ngenerally used as HTTP Web Server, Reverse Proxy Server, Load Balancer Reverse Proxy\nProxy server : is a server that acts as an intermediary between a client and the server\nLoad balancing : Proxies distribute requests to a group of servers,\nEncryption : Proxies encrypt the data to keep it from being read during transfer.\nCaching : Proxies speed up access to information by storing the results of user requests in the server’s cache for a specified time\nLoad Balancer\nGeneral Usage\nNginx as a independent docker container\nNginx as a process in web framework container\n1. Start Nginx as a independent docker container Run Nginx in a independent docker container\ndocker pull nginx\ndocker run --name nginx-server -d -p 80:80 nginx\nCustomize nginx container : docker build -t nginx:test .\n# /nginx/nginx.conf # default.conf file includes *.conf by \u0026quot;include /etc/nginx/conf.d/*.conf;\u0026quot; upstream app { server flask:5000; # UPSTREAM_CONTAINER_NAME:PORT } server { listen 80; # NGINX_PORT location / { proxy_pass http://app; # we defined upstream app above } } # Dockerfile.nginx FROM nginx:latest COPY default.conf /etc/nginx/conf.d/default.conf CMD [\u0026quot;nginx\u0026quot;, \u0026quot;-g\u0026quot;, \u0026quot;daemon off;\u0026quot;] Run docker-compose : docker-compose up\nversion: '3' services: flask: container_name: flask image: \u0026quot;flask:test\u0026quot; ports: - \u0026quot;5000:5000\u0026quot; networks: - backend nginx: container_name: nginx image: \u0026quot;nginx:test\u0026quot; ports: - \u0026quot;80:80\u0026quot; networks: - backend networks: # basic bridge network backend: driver: bridge 2. Start Nginx as a process in web app (WSGI) container Prepare nginx.conf\n```nginx # /nginx/nginx.conf # default.conf file includes *.conf by \u0026quot;include /etc/nginx/conf.d/*.conf;\u0026quot; upstream app { server localhost:{MAIN_PORT}; # upstream is running in same container } server { listen ${NGINX_PORT}; # NGINX_PORT will be filled later location / { proxy_pass http://app; # we defined upstream \u0026quot;app\u0026quot; above } } Install Nginx in Dockerfile\n# Dockerfile.web ARG PYTHON_VERSION FROM python:${PYTHON_VERSION}-slim USER root RUN apt-get -y update \\ \u0026amp;\u0026amp; apt-get -y install nginx \\ \u0026amp;\u0026amp; apt-get install gettext-base RUN pip install redis\\ gunicorn\\ uvicorn\\ fastapi RUN mkdir -p /opt COPY bin /opt/bin COPY src /opt/src COPY nginx.conf /opt/nginx.conf Run web app server in background and run Nginx\n# Run WAS in background gunicorn -k uvicorn.workers.UvicornWorker main:app --bind 0.0.0.0:${MAIN_PORT} -w ${MAIN_WORKER} --timeout ${MAIN_TIMEOUT} --daemon # Insert env-variables in .conf file and run nginx envsubst \u0026lt; /opt/nginx.conf \u0026gt; /etc/nginx/conf.d/default.conf \u0026amp;\u0026amp; nginx -g 'daemon off;' ","id":23,"section":"Engineering","summary":"Introduction Nginx : light-weight Web server\ngenerally used as HTTP Web Server, Reverse Proxy Server, Load Balancer Reverse Proxy\nProxy server : is a server that acts as an intermediary between a client and the server\nLoad balancing : Proxies distribute requests to a group of servers,\nEncryption : Proxies encrypt the data to keep it from being read during transfer.\nCaching : Proxies speed up access to information by storing the results of user requests in the server’s cache for a specified time","tags":null,"title":"CheatSheet | Nginx","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-nginx/","year":"2021"},{"content":"\nHTTP request using ajax \u0026lt;script\u0026gt; $.ajax({url: \u0026quot;\u0026lt;HOST_URL\u0026gt;\u0026quot;, method : \u0026quot;GET\u0026quot;, datatype : \u0026quot;JSON\u0026quot;}) .done(function(data) { alert('Done');}) .fail(function(xhr, status, error){alert('Failed');}); }); \u0026lt;/script\u0026gt; ","id":24,"section":"Engineering","summary":"\nHTTP request using ajax \u0026lt;script\u0026gt; $.ajax({url: \u0026quot;\u0026lt;HOST_URL\u0026gt;\u0026quot;, method : \u0026quot;GET\u0026quot;, datatype : \u0026quot;JSON\u0026quot;}) .done(function(data) { alert('Done');}) .fail(function(xhr, status, error){alert('Failed');}); }); \u0026lt;/script\u0026gt; ","tags":null,"title":"HTML | Snippet","uri":"https://koreanbear89.github.io/engineering/2.-languages/html-snippet/","year":"2021"},{"content":"\nIntroduction Matplotlib Interface\npyplot interface : Rely on pyplot to automatically create and manage the figures and axes, and use pyplot functions for plotting. (MATLAB-like interface)\nobject-oriented interface : Explicitly create figures and axes, and call methods on them\nimport matplotlib.pyplot as plt import numpy as np # pyplot interface (MATLAB-like, rely on pyplot) x = np.linspace(0,2,100) plt.plot(x,x**2) # Recommended) object-oriented interface (create figures and axes manually) fig, ax = plt.subplots(figsize=(15,4)) ax.plot(x,x**2) Plot fig, ax = plt.subplots() ax.plot(x,x**2) Axes # Axes fig, ax = plt.subplots() # set axes ax.set_title(\u0026quot;Title\u0026quot;) ax.set_xlabel('xlabel') # set limits ax.set_ylim([0, 1e5]) # rotate tick ax.tick_params(axis = 'x', labelrotation =90) Pandas Groupby # select columns df_channel = df[['COLUMN_A', \u0026quot;COLUMN_1\u0026quot;, \u0026quot;COLUMN_2\u0026quot;, \u0026quot;COLUMN_3\u0026quot;, \u0026quot;COLUMN_4\u0026quot;]] # groupby =\u0026gt; will return aggregated list of each columns by COLUMN_A df_channel_group = df_channel.groupby('COLUMN_A').agg(lambda x: list(x)) ","id":25,"section":"Engineering","summary":"Introduction Matplotlib Interface\npyplot interface : Rely on pyplot to automatically create and manage the figures and axes, and use pyplot functions for plotting. (MATLAB-like interface)\nobject-oriented interface : Explicitly create figures and axes, and call methods on them\nimport matplotlib.pyplot as plt import numpy as np # pyplot interface (MATLAB-like, rely on pyplot) x = np.linspace(0,2,100) plt.plot(x,x**2) # Recommended) object-oriented interface (create figures and axes manually) fig, ax = plt.","tags":null,"title":"Python #3 | 3rd-Party Modules","uri":"https://koreanbear89.github.io/engineering/2.-languages/python-3-3rd-party-modules/","year":"2021"},{"content":"\n1. Introduction 1.1 Terminologies\nJob : A piece of code that reads some input (HDFS or local), performs some computation and writes output.\nStages : Jobs are divided into stages. Stages are classified as a Map or reduce stages. Stages are divided based on computational boundaries, all computations cannot be Updated in a single Stage. It happens over many stages.\nTasks : Each stage has some tasks. One task is executed on one partition of data on one executor (machine).\nDAG : DAG stands for Directed Acyclic Graph, in the present context its a DAG of operators.\nExecutor : The process responsible for executing a task.\nMaster : The machine on which the Driver program runs\nSlave : The machine on which the Executor program runs\n1.2 Spark Components\nSpark Driver\nseparate process to execute user applications\ncreates SparkContext to schedule jobs execution and negotiate with cluster manager\nExecutors\nrun tasks scheduled by driver\nstore computation results in memory, on disk or off-heap\ninteract with storage systems\nCluster Manager\nApache Mesos : a general cluster manager that can also run Hadoop MapReduce and service applications.\nHadoop YARN : the resource manager in Hadoop.\nSpark Standalone : a simple cluster manager included with Spark that makes it easy to set up\n1.3 How Spark Works?\nSpark has a small code base and the system is divided in various layer.\nEach layers has some responsibilities. The layers are independent of each other\nInterpreter : The first layer is the interpreter, Spark uses a Scala interpreter, with some modifications. As you enter your code in spark console (creating RDD’s and applying operators), Spark creates a operator graph.\nDAG Scheduler : When the user runs an action (like collect), the Graph is submitted to a DAG Scheduler. The DAG scheduler divides operator graph into (map and reduce) stages. A stage is comprised of tasks based on partitions of the input data. The DAG scheduler pipelines operators together to optimize the graph. For e.g. Many map operators can be scheduled in a single stage. This optimization is key to Sparks performance. The final result of a DAG scheduler is a set of stages.\nTask Schedular : The stages are passed on to the Task Scheduler. The task scheduler launches tasks via cluster manager. (Spark Standalone/Yarn/Mesos). The task scheduler doesn’t know about dependencies among stages.\n2. Install Spark Locally 2.1 Prerequisite\nCheck java version first\nThen, just install miniconda3 and create virtual environment based on python 3.6\njava -version # The command above might show something like below \u0026gt;\u0026gt; openjdk version \u0026quot;1.8.0_262\u0026quot; \u0026gt;\u0026gt; OpenJDK Runtime Environment (build 1.8.0_262-b10) \u0026gt;\u0026gt; OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode) 2.2 Download Spark\n# Download spark wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz # unzip tar -xvzf spark-3.0.1-bin-hadoop2.7.tgz # move to home and rename mv spark-3.0.1-bin-hadoop2.7 ~/spark 2.3 Install pyspark\npip install pyspark 2.4 Change the execution path for pyspark\nexport SPARK_HOME=\u0026quot;/your_home_directory/spark/\u0026quot; export PATH=\u0026quot;$SPARK_HOME/bin:$PATH\u0026quot; 2.5 Test\n$ pyspark 2.6 PySpark in Jupyter\n# add below lines in .bashrc export PYSPARK_DRIVER_PYTHON=jupyter export PYSPARK_DRIVER_PYTHON_OPTS='notebook' # And run $ pyspark \u0026gt;\u0026gt; will give you a url for jupyter notebook 3. Cheat Sheet 3.1 Initialization \u0026amp; Configuration SparkContext : provides connection to Spark with the ability to create RDDs\nSQLContext : procides connection to Spark with the ability to run SQL queries on data\nSparkSession : all encompassing context which includes coverage for SparkContext, SQLContext and HiveContext\nimport pyspark from pyspark import SparkContext from pyspark.sql import SparkSession from pyspark.sql import SQLContext # create a SparkSession instance with the name 'appname' spark = SparkSession.builder.appName(\u0026quot;appname\u0026quot;).getOrCreate() # create a SparkContext instance which allows the Spark Application to access sc = SparkContext.getOrCreate() # create a SQLContext instance to access the SQL query engine built on top of Spark sqlContext = SQLContext(spark) 3.2 Dataframe Explore Dataframe # Prints out the schema in the tree format. df.printSchema() # To get the number of rows df.count() Modifying Dataframe # Create a column df = df.withColumn('column_new', F.lit('abc')) # with the defalut value 'abc' df = df.withColumn('column_new', 2*F.col('exisisting column')) # using an existing column # Change Column Name df.withColumnRenamed(\u0026quot;column_ori\u0026quot;, \u0026quot;column_new\u0026quot;) # add index column from pyspark.sql.functions import monotonically_increasing_id df_index = df.select(\u0026quot;*\u0026quot;).withColumn(\u0026quot;id\u0026quot;, monotonically_increasing_id()) # filter (=where) df.filter(df.column_float.between(7.5,8.2)) # remove column df.drop(\u0026quot;column_drop\u0026quot;) # grouped data : GroupBy allows you to group rows together based off some column value df.groupBy(\u0026quot;column_name\u0026quot;). .count() # Returns the count of rows for each group. .mean() # Returns the mean of values for each group. .max() # Returns the maximum of values for each group. .min() # Returns the minimum of values for each group. .sum() # Returns the total for values for each group. .avg() # Returns the average for values for each group. .agg() # Using agg() function, we can calculate more than one aggregate at a time. .pivot() # This function is used to Pivot the DataFrame Read and Write # create a SparkSession instance with the name 'appname' spark = SparkSession.builder.appName(\u0026quot;appname\u0026quot;).getOrCreate() # read (csv, json, text, parquet) df = spark.read.csv('PATH_CSV') # write df.coalesce(1).write.csv(\u0026quot;sample.csv\u0026quot;) 3.3 Join Inner join : essentially removes anything that is not common in both tables. It returns all data that has a match under the join condition(on expression is true) from both sides of the table Outer join : allows us to include in the result rows of one table for which there are no matching rows round in another table Left join : all rows of the left table remain unchanged, regardless of whether there is a match in the right table or not. When a id match is found in the right table, it will be returned or null otherwise Right join : performs the same task as the left join, but for the right table. df1.join(df2, on='column', how='inner') 3.4 SQL functions from pyspark.sql import functions as F # Aggregate function: returns a set of objects with duplicate elements eliminated. F.collect_set() # Bucketize rows into one or more time windows given a timestamp specifying column. F.window(timeColumn='timestamp', windowDuration='20 minute') # Creates a Column of literal value. F.lit() # Replace all substrings of the specified string value that match regexp with rep. F.regexp_replace() 3.5 User Defined Function input arguments for udf should be column name as you can see in line no.3 or you might get an TypeError: Invalid argument, not a string or column: 2 of type \u0026lt;class ‘int’\u0026gt;. For column literals, use ‘lit’, ‘array’, ‘struct’ or ‘create_map’ function\n# User Defined Functions def func(x): return x test_udf = F.udf(lambda x : func(x), ArrayType(StringType())) df_udf = df.withColumn('udf', test_udf('input_column_name')) 3.6 Others $\u0026quot;colname\u0026quot; is converted to Column by SQLContext.implicits$.StringToColumn # zeppelin plot def show(p): import io img = io.StringIO() p.savefig(img, format='svg') img.seek(0) print(\u0026quot;%html \u0026lt;div style='width:400px'\u0026gt;\u0026quot; + img.getvalue() + \u0026quot;\u0026lt;/div\u0026gt;\u0026quot;) References PySpark Cheat Sheet, TowardsDataScience\nIntroduction to PySpark join types\n","id":26,"section":"Engineering","summary":"1. Introduction 1.1 Terminologies\nJob : A piece of code that reads some input (HDFS or local), performs some computation and writes output.\nStages : Jobs are divided into stages. Stages are classified as a Map or reduce stages. Stages are divided based on computational boundaries, all computations cannot be Updated in a single Stage. It happens over many stages.\nTasks : Each stage has some tasks. One task is executed on one partition of data on one executor (machine).","tags":null,"title":"CheatSheet | Spark","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-spark/","year":"2020"},{"content":"\nIntroduction Contents\nInstall docker in centos\nFrequently used docker commands\nFIle sharing in docker (bind, volume)\nDockerfile instruction\nDocker compose\nDocker swarm\nGlossary\nswarm : almost same with word \u0026ldquo;cluster\u0026rdquo; node (manager/worker) : A unit of server in a cluster. You can run swarm commands only on the manager node. service : A unit of modules in project, a basic distribution unit, stack : You can think of it as a unit of a project, and containers grouped into one stack basically belong to the same overlay network. 1. Install docker in centOS Install Docker Engine on CentOS | Docker Documentation # (1) Set up the repository $ sudo yum install -y yum-utils $ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # (2) Install Docker Engine $ sudo yum install docker-ce docker-ce-cli containerd.io $ sudo yum install docker-ce-19.03.13 docker-ce-cli-19.03.13 containerd.io docker-compose-plugin Post-installation steps for Linux | Docker Documentation # (3) Create the docker group. # $ sudo groupadd docker # (4)Add your user to the docker group. # sudo usermod -aG docker irteam $ sudo /usr/sbin/usermod -aG docker irteam $ sudo /usr/sbin/usermod -aG docker irteamsu # (5) Run and Stop docker before change root directory $ sudo systemctl start docker $ sudo systemctl stop docker # (5) change root directory (storage for default docker directory is not enough) # add {\u0026quot;data-root\u0026quot;: \u0026quot;/home1/irteam/docker-data\u0026quot;} in /etc/docker/daemon.json $ sudo vim /etc/docker/daemon.json # (6) Run docker $ sudo systemctl start docker # if got permission error for /var/run/docker.sock $ sudo chmod 666 /var/run/docker.sock # check root directory $ docker info | grep Root $ docker run hello-world 2. Frequently used Docker Commands Dockerize an SSH service $ docker --version # check docker version $ docker build -t [TAG] . # build using Dockerfile in cwd $ docker images # show docker images $ docker ps -a # show docker containers $ docker ps --format '{{.Names}}' $ docker rm -f [CONTAINER_NAME] # remove docker container $ docker rmi [IMAGE_NAME] # remove docker image $ docker run --dit --rm -p 22:22 --name [CONTAINER_NAME] -v [SRC]:[DST] [IMAGE_NAME] # run docker container $ ctrl p q # exit without removing container $ docker attach [CONTAINER_NAME]] # attach to docker container $ docker exec -u 0 -it [CONTAINER_NAME] bash # exec on root $ docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] $ docker push [REPOSITORY[:TAG]] $ docker container prune # prune docker container $ docker image prune # prune docker images $ docker system prune -a # remove all building cache $ docker logs -f \u0026lt;CONTAINER_NAME\u0026gt; --tail=1000 2\u0026gt;\u0026amp;1 | grep complete # show logs in container 3. File sharing in docker container Bind-mount : are files mounted from your host machine (the one that runs your docker daemon) onto your container.\nVolume : are like storage spaces totally managed by docker. In fact, volumes are managed in the hidden(?) path of host machine such as \u0026lsquo;/var/lib/docker/volumes/VOLUME_NAME\u0026rsquo;\nnamed volumes : you provide the name of it\nanonymous volumes : usual UUID names from docker, like you can find them on container or untagged images\n$ docker volume ls $ docker volume rm $ docker volume inspect VOLUME_NAME 4. Dockerfile Instructions # Set base image FROM ubuntu:16.04 # argument used only in build time ARG PYVERSION # run shell cmd using \u0026quot;bin/sh -c\u0026quot; in docker image RUN [\u0026quot;apt-get\u0026quot;, \u0026quot;install\u0026quot;, \u0026quot;-y\u0026quot;, \u0026quot;nginx\u0026quot;] # set expose port EXPOSE 8080 # set env-var, env-var can be used as $variable_name ENV FOO /bar # set user of docker image USER nginx # volume mount from host to docker container VOLUME [\u0026quot;opt/project\u0026quot;] # copy files from host to docker image ADD file /some/dir/file # almost same with ADD, do not unzip zipped files automatically, cannot use URL as source of file COPY file /some/dir/file # cmd to run when docker container starts CMD [\u0026quot;python\u0026quot;, \u0026quot;main.py\u0026quot;] 5. Docker Registry $ docker login # docker-hub $ docker login -u \u0026lt;ID\u0026gt; reg.*********.com # private registry $ docker push ${REG_HOST}/IMAGE_TO_PUSH:${TAG} 6. Docker Compose $ docker-compose up -d $ docker-compose down $ docker-compose stop [CONTAINER_NAME] $ docker-compose ps version: '3.7' services: my_service1: build: # if wanna build a image context: ./ dockerfile: ./Dockerfile image: # image name hostname: # host name tty: true # docker run -t container_name: \u0026lt;my_cont1\u0026gt; # container_name volumes: # mout volumes - ./src:/myproject/src networks: - myproject ports: - 2003:3003 user: celery command: python -m black /myproject/src -t py39 depends_on: - black volumes: rabbitmq: driver: local redis: driver: local networks: myproject: 7. Docker swarm Server Orchestration\nScheduling : Distribute multiple containers to each server, and when the server dies, it is deployed to another server so that there is no disruption to the service. Clustering : Multiple servers can be used like one server. By adding/removing new servers to the cluster, scattered containers can communicate easily as if they were on the same server using a virtual network. Service Discovery Logging, Monitoring Why Docker Swarm?\nWhen you build an API server and traffic increases =\u0026gt; one server cannot handle it,\nWhat if the images constituting the container are updated. Should I delete all currently running containers and create a new container again with docker-compose =\u0026gt; Rolling update of Docker Swarm\nSwarm was developed separately from Docker, and since v1.12, it was merged under the name of Swarm Mode.\n# (1) init docker swarm $ docker swarm init # run on manager node # This will return the following command. To add a worker to this swarm, just run that command on the worker node \u0026gt; docker swarm join --token ............. # use \u0026lt;***.nfra.io:2377\u0026gt; rather than inner ip \u0026gt; docker swarm join-token worker # will return the token message again # (2) deploy $ docker stack deploy --compose-file \u0026lt;docker-compose.yml\u0026gt; \u0026lt;STACK_NAME\u0026gt; # deploy using docker-compse.yml # (3) manage swarm $ docker node ls # show all nodes joined to current node $ docker stack ls # show all stacks in current node (manager) $ docker service ls # show all services (including worker nodes) managed by current node $ docker service ps \u0026lt;SERVICE_NAME\u0026gt; # Update node metadata $ docker node update --label-add \u0026lt;LABEL_KEY\u0026gt;=\u0026lt;LABEL_VALUE\u0026gt; \u0026lt;HOSTNAME\u0026gt; ","id":27,"section":"Engineering","summary":"Introduction Contents\nInstall docker in centos\nFrequently used docker commands\nFIle sharing in docker (bind, volume)\nDockerfile instruction\nDocker compose\nDocker swarm\nGlossary\nswarm : almost same with word \u0026ldquo;cluster\u0026rdquo; node (manager/worker) : A unit of server in a cluster. You can run swarm commands only on the manager node. service : A unit of modules in project, a basic distribution unit, stack : You can think of it as a unit of a project, and containers grouped into one stack basically belong to the same overlay network.","tags":null,"title":"CheatSheet | Docker","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-docker/","year":"2020"},{"content":"\nIntroduction Hadoop : A framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models\nBased on Java\nSave data in Google File System (2003) and process this big data with Map Reduce (2004)\nModules :\nHadoop Distributed File System : A distributed file system that provides high-throughput access to application data\nHadoop YARN : A framework for job scheduling and cluster resource management\nHadoop MapReduce : A YARN-based system for parallel processing of large data sets.\n1. Hadoop Distributed File System (HDFS) Components :\nMaster (Name-node)\n(1) Managing Meta data\n(2) Monitoring data-node : data-node sends heartbeat signal for status and block-information every 3secs.\n(3) process client request\nSlave (Data-node)\nCharacteristics:\nBlock File System :\nSave data into splited block (128MB/block - in Hadoop 2.0)\nMetadata (file name, trees, \u0026hellip; ) of these blocks are saved in Name node\n$ hdfs dfs -ls $ hdfs dfs -put [LOCAL_FILE_NAME] [DEST] $ hdfs dfs -get [FILE_NAME or FOLDER_PATH] [DEST_LOCAL] $ hdfs dfs -rm [-f] [-r|-R] [-skipTrash] URI [URI ...] ","id":28,"section":"Engineering","summary":"Introduction Hadoop : A framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models\nBased on Java\nSave data in Google File System (2003) and process this big data with Map Reduce (2004)\nModules :\nHadoop Distributed File System : A distributed file system that provides high-throughput access to application data\nHadoop YARN : A framework for job scheduling and cluster resource management","tags":null,"title":"CheatSheet | Hadoop","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-hdfs/","year":"2020"},{"content":"\n1. Setup vim Install Vundle\ngit clone [GitHub - VundleVim/Vundle.vim: Vundle, the plug-in manager for Vim](https://github.com/VundleVim/Vundle.vim.git) ~/.vim/bundle/Vundle.vim color scheme setup\nJellybean color scheme official install\nAnd simply add line color jellybean in .vimrc\nmkdir -p ~/.vim/colors cd ~/.vim/colors curl -O https://raw.githubusercontent.com/nanotech/jellybeans.vim/master/colors/jellybeans.vim write .vimrc\nwget https://raw.githubusercontent.com/jjeaby/jscript/master/.vimrc\nAdded Plugin 'preservim/nerdcommenter'\ninstall plugins in vimrc\n:PluginInstall Shell\n# install Vundle git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim # setup jellybean color scheme mkdir -p ~/.vim/colors cd ~/.vim/colors curl -O https://raw.githubusercontent.com/nanotech/jellybeans.vim/master/colors/jellybeans.vim # copy .vimrc cp ~/configuration/vim/.vimrc ~/.vimrc vim -c 'PluginInstall' -c 'qa!' Reference\nVim as IDE 2. shortcuts Basic commands - a # write text to the cursor - A # write text at the end of the current line - i # write text to the cursor - I # write text to the front of the current line - ^ # move cursor to the front of the line - $ # move cursor to the back of the line - ESC # shift to command mode - x # in command mode, remove the text - u # in command mode, undo - :w # in command mode, save (write on the disk) - :q # in command mode, quit - y # copy in visual mode - p # paste in visual mode Commands for multiple windows - :split # split in horizontal - :vsplit # split in vertical - ctrl + w + w # move to next window - ctrl + w + p # move to previous window Fold and Expand - :set foldmethod=indent - zi # toggle folding Indent \u0026amp; Un-indent # insert mode - ctrl + d - ctrl + t # visual mode : if 1 or more lines are selected - \u0026lt; - \u0026gt; 3. Plugins NerdTree :nerd + tab # turn on nerdtree ctrl+w+w # move focus between editor and file tree m + a # add a child file ","id":29,"section":"Engineering","summary":"1. Setup vim Install Vundle\ngit clone [GitHub - VundleVim/Vundle.vim: Vundle, the plug-in manager for Vim](https://github.com/VundleVim/Vundle.vim.git) ~/.vim/bundle/Vundle.vim color scheme setup\nJellybean color scheme official install\nAnd simply add line color jellybean in .vimrc\nmkdir -p ~/.vim/colors cd ~/.vim/colors curl -O https://raw.githubusercontent.com/nanotech/jellybeans.vim/master/colors/jellybeans.vim write .vimrc\nwget https://raw.githubusercontent.com/jjeaby/jscript/master/.vimrc\nAdded Plugin 'preservim/nerdcommenter'\ninstall plugins in vimrc\n:PluginInstall Shell\n# install Vundle git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim # setup jellybean color scheme mkdir -p ~/.vim/colors cd ~/.","tags":null,"title":"Cheat Sheet | VIM","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-vim/","year":"2020"},{"content":"\n5.1 Singular Value decomposition (SVD) singular value decomposition (SVD) : is a factorization of a real or complex matrix that generalizes the eigendecomposition (EVD) of a square normal matrix to any $m \\times n$ matrix via an extension of the polar decomposition.\n$$ A = U \\Sigma V^T$$ $A \\in \\mathbb{R}^{m \\times n}$ : A given rectangular matrix $U \\in \\mathbb{R}^{m\\times m} $ : matrices with orthonormal columns, providing an orthonormal basis of $Col(A)$ $V \\in \\mathbb{R}^{n \\times n}$ : matrices with orthonormal columns, providing an orthonormal basis of $Row(A)$ $\\Sigma \\in \\mathbb{R}^{m \\times n}$ : a diagonal matrix whose entries are in a decreasing order, i.e., $\\sigma_1 \\geq \\sigma_2 \\geq \u0026hellip; \\geq \\sigma_{min(m,n)}$ 5.2 SVD as Sum of Outer Products SVD as Sum of Outer Products : $A$ can also be represented as the sum of outer products $$ A = U \\Sigma V^T = \\Sigma_{i=1}^n \\sigma_i \\mathbf{u_j v_i^T}$$\nFrom now on, we just want to show the equation below is true in this chapter (5.2) for the next chapter (5.3)\n$$AV = U\\Sigma \\Longleftrightarrow A=U \\Sigma V^T$$\nAnother Perspective of SVD : We can easily find two orthonormal basis sets, {$u_1, \u0026hellip; , u_n$} for $Col(A)$ and {$v_1, \u0026hellip; v_n$} for $Row (A)$, by using, Gram-Schmidt orthogonalization.\nAre these unique orthonormal basis sets? No, then can we jointly find them such that\n$$ A \\mathbf{v_i} = \\sigma_i \\mathbf{u_i}$$\nLet us denote $ U = \\begin{bmatrix} \\mathbf{u_1 \\ u_2 \\ \u0026hellip; \\ u_m} \\end{bmatrix} \\in \\mathbb{R}^{m\\times n}$, $ V = \\begin{bmatrix} \\mathbf{v_1 \\ v_2 \\ \u0026hellip; \\ v_n} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}$ and $ \\Sigma = diag(\\sigma_n) \\in \\mathbb{R}^{n \\times n}$\n$AV = U\\Sigma \\Longleftrightarrow \\begin{bmatrix} A\\mathbf{v_1} \\ A\\mathbf{v_2} \\ \u0026hellip; \\ A\\mathbf{v_n} \\end{bmatrix} = \\begin{bmatrix} \\ \\sigma_1 \\mathbf{u_1} \\ \\sigma_2 \\mathbf{u_2} \\ \u0026hellip; \\ \\sigma_n \\mathbf{u_n} \\end{bmatrix}$\n$V^{-1} = V^T $ since $\\mathbb{R}^{n \\times n}$ has orthonormal columns.\nThus $AV = U\\Sigma \\Longleftrightarrow A=U \\Sigma V^T$\n5.3 Geometrical Interpretation of SVD Geometrical Interpretation : For every linear transformation (rectangular matrix) $A$ , one can find $n$ dimensional orthonormal basis of $V$ and transformed orthonormal basis $U$ $$A = U \\Sigma V^T \\Longleftrightarrow AV = U\\Sigma$$\nFurthermore : The figure below shows the orthogonal vector set $x, y$ and the linearly transformed one $Ax, Ay$.\nAnd you can see two geometrical features :\nThere could be one or more sets of orthogonal {$Ax, Ay$}. After transformed by matrix $A$, the lengths of $x,y$ are scaled by scaling factor. It is called singular value (like eigenvalue at EVD) and represented as $\\sigma_1, \\sigma_2, \\sigma_3, \u0026hellip;$ starting with the larger value. Figure. the orthogonal vector set $x, y$ and the linearly transformed one $Ax, Ay$. [reference] 5.4 Computing SVD First, we form $AA^T$ , $A^T A$ and then get SVD of each as followings : $$ AA^T = U \\Sigma V^T V \\Sigma^T U^T = U \\Sigma \\Sigma^T U^T = U \\Sigma^2 U^T$$\n$$ A^T A = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^T \\Sigma U^T = V \\Sigma^2 V^T$$\nFrom the above, you can see the form of result equation is very similar to EVD: $U$ Is a orthogonal matrix which is from the eigen-decomposition of $AA^T$, and it is called left singular vector. $\\Sigma$ is a diagonal matrix whose diagonal entries are equal to the square root of the eigenvalues from $AA^T$. $V$ is a orthogonal matrix which is from the eigen-decomposition of $A^TA$, and it is called right singular vector. 5.5 Application of SVD Recall SVD as Sum of Outer Products : matrix $A$ can also be represented as the sum of outer products $$ A = U \\Sigma V^T = \\Sigma_{i=1}^n \\sigma_i \\mathbf{u_j v_i^T} = \\sigma_1 \\mathbf{u_1 v_1^T} + \\sigma_2 \\mathbf{u_2 v_2^T} + \\sigma_3 \\mathbf{u_3 v_3^T} \u0026hellip;$$\nHere, $\\mathbf{u_j v_i^T} $ is $m \\times n$ matrix and we can see matrix $A$ as a sum of multiple layers via SVD.\nThat is, we can reconstruct $A$ with only a few singular values rather than using all of them ($\\Sigma$). And this can be used for dimensionality reduction like task such as image compression.\nFigure. Partial reconstruction of matrix using SVD [reference] Figure. image compression using SVD (original, p=20, p=50, p=100) [reference] ","id":30,"section":"Mathematics","summary":"5.1 Singular Value decomposition (SVD) singular value decomposition (SVD) : is a factorization of a real or complex matrix that generalizes the eigendecomposition (EVD) of a square normal matrix to any $m \\times n$ matrix via an extension of the polar decomposition. $$ A = U \\Sigma V^T$$ $A \\in \\mathbb{R}^{m \\times n}$ : A given rectangular matrix $U \\in \\mathbb{R}^{m\\times m} $ : matrices with orthonormal columns, providing an","tags":null,"title":"Linear Algebra for ML #5 | Singular Value Decomposition","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec5/","year":"2019"},{"content":"\n4.0 Introduction Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource.\nThe above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$\n$D$ is a diagonal matrix with eigenvalues in diagonal entries\n$V$ is a matrix whose column vectors are eigenvectors\n$A$ is a given square matrix $A \\in \\mathbb{R}^{n \\times n}$\nConsider the linear transformation $T(\\mathbf{x}) = A \\mathbf{x} = VDV^{-1} \\mathbf{x} $, it can be seen to be a series of following transformation. (4.5.1+ 4.5.2 + 4.5.3)\nChange of Basis\nElement-wise Scaling\nBack to Original Basis\n4.1 Eigenvectors and Eigenvalues An eigenvector of a square matrix $A \\in \\mathbb{R}^{n \\times n}$ is a nonzero vector $\\mathbf{x} \\in \\mathbb{R}^n$ such that $A \\mathbf{x} = \\lambda \\mathbf{x}$ for some scalar $\\lambda$. In this case $\\mathbf{\\lambda}$ is called an eigenvalue of $A$, and such an $\\mathbf{x}$ is called an eigenvector.\n$A \\mathbf{x}$ can be considered as a linear transformation $T( \\mathbf{x})$. If $\\mathbf{x} $ is an eigenvector, then $T(x) = A \\mathbf{x} = \\lambda \\mathbf{x} $, which means the output vector has the same direction as $\\mathbf{x}$, but the length is scaled by a factor of $\\lambda$ Fig1. Example of eigenvector and eigenvalue And here, $8 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $ is faster than $ \\begin{bmatrix} 2\\ 6 \\\\ 5 \\ 3 \\end{bmatrix}\\begin{bmatrix} 1\\ 1 \\end{bmatrix} $\nThe equation $A \\mathbf{x} = \\lambda \\mathbf{x}$ can be re-written as $$(A-\\lambda I)\\mathbf{x}=\\mathbf{0} $$\n$\\lambda$ is an eigenvalue of an $n \\times n$ matrix $A$ if and only if this equation has a nontrivial solution. (since $\\mathbf{x}$ should be a nonzero vector).\nThe set of all solutions of the above equation is the null space of the matrix $A-\\lambda I$, which we call the eigenspace of A corresponding to $\\lambda$\nThe eigenspace consists of the zero vector and all the eigenvectors corresponding to $\\lambda$, satisfying the above equation.\n4.2 Null Space Null Space : The null space of a matrix $A \\in \\mathbb{R}^{m\\times n}$ is the set of all solutions of a homogeneous linear system, $A \\mathbf{x = 0}$ For $A = \\begin{bmatrix} \\mathbf{a_1^T} \\\\ \u0026hellip; \\\\ \\mathbf{a_m^T} \\end{bmatrix}$, $\\mathbf{x}$ should satify the following $ \\mathbf{ a_1^Tx =a_2^Tx= \u0026hellip; =a_m^Tx }=0$, That is, $\\mathbf{x} $ should be orthogonal to every row vector in $A$ Orthogonal Complement : The set of all vectors $\\mathbf{z}$ that are orthogonal to $W$ is called the orthogonal complement of $W$ and is denoted by $W ^ \\perp$. 4.3 Characteristic equation $$(A - \\lambda I) \\mathbf{x = 0}$$\nThe set of all solutions of the above equation = null space of the matrix $(A - \\lambda I)$ = eigenspace of $A$ corresponding to $\\lambda$\nThe eigensapce consists of the zero vector and all the engienvectors corresponding to $\\lambda$\nHow can we find the eigenvalues?\nIf $(A-\\lambda I)\\mathbf{x = 0}$ has a nontrivial solution, then the columns of $(A - \\lambda I)$ should be noninvertible.\nIf it is invertible, $\\mathbf{x}$ cannot be a nonzero vector since\n$$ (A - \\lambda I)^{-1} (A - \\lambda I) \\mathbf{x} = (A - \\lambda I)^{-1} \\mathbf{0} \\longrightarrow \\mathbf{x = 0} $$\nThus, we can obtain eignevalues by solving characteristic equation : $$ det(A - \\lambda I) = 0$$\nAlso, the solution is not unique, and thus $A-\\lambda I$ has linearly dependent columns.\nOnce obtaining eigenvalues, we compute the eigenvectors for each $\\lambda$ by solving : $$ (A-\\lambda I) \\mathbf{x = 0}$$\nEigenspace : Note that the dimension of the eigenspace (corresponding to a particular $\\lambda$) can be more than one. In this case, any vector in the eigenspace satisfies\n$$ T\\mathbf{(x)} = A \\mathbf{(x)} = \\lambda \\mathbf{(x)}$$\nIn summary, we can find all the possible eigenvalues and eignevectors, as follows.\nFirst, find all the eigenvalue by solving the characteristic equation : $ det(A-\\lambda I) = 0$\nSecond, for each eigenvalue $\\lambda$, solve for $(A-\\lambda I) \\mathbf{x=0}$ and obtain the set of basis vectors of the corresponding eigenspace.\n4.4 Diagonalization Diagonal matrix : matrix in which the entries outside the main diagonal are all zero\nDiagonalization : We want to change a given square matrix $A \\in \\mathbb{R}^{n\\times n}$ into a diagonal matrix via the following form : $D = V^{-1} A V $\nFor $A$ to be diagonalizable, an invertible $V$ should exist such that $V^{-1}AV$\nHow can we find an invertible $V$ and the resulting diagonal matrix $D$\n$V = [\\mathbf{v_1, v_2 , \u0026hellip; , v_n}]$ where $\\mathbb{v_i}$ are column vectors of $V$\n$D = diag[\\lambda_1, \\lambda_2 , \u0026hellip; , \\lambda_n ]$\n$$ D = V^{-1}AV $$\n$$ VD = AV$$\n$$AV = A[ \\mathbf{v_1, v_2 , \u0026hellip; v_n }] = [A\\mathbf{v_1}, A\\mathbf{v_2},\u0026hellip;, A\\mathbf{v_n}], (\\text{column vector matmul})$$\n$$VD = [\\lambda \\mathbf{v_1},\\lambda \\mathbf{v_2}, \u0026hellip; ,\\lambda \\mathbf{v_n}]$$\n$$AV=VD \\longleftrightarrow [A\\mathbf{v_1}, A\\mathbf{v_2},\u0026hellip;, A\\mathbf{v_n}] = [\\lambda \\mathbf{v_1},\\lambda \\mathbf{v_2}, \u0026hellip; ,\\lambda \\mathbf{v_n}]$$\nFrom above, we obtain\n$$A \\mathbf{v_1} = \\lambda \\mathbf{v_1}, A \\mathbf{v_2} = \\lambda \\mathbf{v_2}, \u0026hellip; , A \\mathbf{v_n} = \\lambda \\mathbf{v_n}$$\nThus $\\mathbf{v_1, v_2, \u0026hellip; v_n} $ should be eigenvectors and $\\lambda_1, \\lambda_2, \u0026hellip;. \\lambda_n$ should be eigenvalues. Then, For $VD = AV \\longrightarrow D = V^{-1}AV$ to be true, $V$ should invertible.\nIn this case, the resulting diagonal matrix D has eigenvalues as diagonal entries\nDiagonalizable matrix : For $V$ to be invertible, $V$ should be a square matrix in $\\mathbb{R}^{n \\times n}$, and $V$ should have $n$ linearly independent columns : Hence, $A$ should have $n$ linearly independent eigenvectors. 4.5 Eigen-Decomposition and Linear Transfomation Eigendecomposition : If $A$ is diagonalizable, we can write $A = V^{-1}DV$, and we can also write $A = VDV^{-1}$, which we call eigendecomposition of $A$. So, $A$ being diagonalizable is equilvalent to $A$ having Eigendecomposition.\nSuppose $A$ is diagonalizable, thus having eigendecomposition $ A = VDV^{-1}$, Consider the linear transformation $T(\\mathbf{x}) = A \\mathbf{x}$, it can be seen to be a series of following transformation. (4.5.1+ 4.5.2 + 4.5.3)\n$$T(\\mathbf{x}) = A\\mathbf{x} = VDV^{-1}\\mathbf{x} = V(D(V^{-1}\\mathbf{x}))$$\n4.5.1 Change of Basis\nLet $\\mathbf{y} = V^{-1}\\mathbf{x}$ then $V \\mathbf{y=x}$ $\\mathbf{y}$ is a new coordinate of $\\mathbf{x}$ with respect to a new basis of eigenvectors {$\\mathbf{v_1, v_2}$} https://angeloyeo.github.io/2020/12/07/change_of_basis.html 4.5.2 Element-wise and Dimension-Wise Scaling\n$T(\\mathbf{x}) = V(D(V^{-1}\\mathbf{x})) = V(D\\mathbf{y})$\nLet $ z=D\\mathbf{y}$. This computation is a simple Element-wise scaling of $\\mathbf{y}$\n4.5.3 Back to Original Basis\n$T(\\mathbf{x}) = V(D\\mathbf{y}) = Vz$\n$z$ is still a coordinate based on the new basis $\\mathbf{v_1, v_2}$\n$Vz$ converts $z$ to another coordinates based on the original standard basis.\nThat is, $Vz$ is a linear combination of $\\mathbf{v_1}$ and $\\mathbf{v2}$ using the coefficient vector $z$.\nThat is,\n$$Vz = \\begin{bmatrix} \\mathbf{v_1} \\ \\mathbf{v_2} \\end{bmatrix} \\begin{bmatrix} z_1 \\ z_2 \\end{bmatrix} = \\mathbf{v_1}z_1 + \\mathbf{v_2}z_2$$\nFig3. Eigendecomposition as a series of transformation Example : $A = VDV^{-1} = \\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} -1\u0026amp;0\\\\ 0\u0026amp;2 \\end{bmatrix}\\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix}^{-1}, \\text{ and } \\bf{x}=\\begin{bmatrix} 4\\\\ 3 \\end{bmatrix} $\n$$ y = \\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix}^{-1}\\begin{bmatrix} 4\\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} $$\n$$ z = \\begin{bmatrix} -1 \u0026amp; 0\\\\ 0 \u0026amp;2 \\end{bmatrix} \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -2\\\\ 2 \\end{bmatrix}$$\n$$ Vz = \\begin{bmatrix} 3\u0026amp;{-2}\\\\ 1\u0026amp;1 \\end{bmatrix}\\begin{bmatrix} -2\\\\ 2 \\end{bmatrix} = \\begin{bmatrix} -10\\\\ 0 \\end{bmatrix} $$\n4.5.4 Linaer Transformation via $A^k$\nNow, consider recursive transformation $A \\times A \\times A \u0026hellip; \\times A \\mathbf{x} = A^k \\mathbf{x} $\nIf $A$ is diagonalizable, $A$ has Eigendecomposition\n$$ A = VDV^{-1}$$\n$$ A^k = (VDV^{-1})(VDV^{-1})(VDV^{-1})\u0026hellip;(VDV^{-1}) = (VD{^k}V^{-1})$$\nHere, $D^k$ is simply computed as $diag[\\lambda_1^k, \\lambda_2^k\u0026hellip; , \\lambda_n^k]$\nIt is much faster to compute $V(D^k(V^{-1}\\mathbf{x}))$ than to compute $A^k \\mathbf{x}$\nSummary Eigenvalue and Eigenvectors\nNull space, Column space, and orthogonal complement in $\\mathbb{R}^n$\nDiagonalization and Eigendecomposition\nLinear transforamtion via eigendecomposition\n","id":31,"section":"Mathematics","summary":"4.0 Introduction Goal : We want to get a diagonalized matrix $D$ of a given matrix $A$ in the form of $ D = V^{-1}AV$ for some reasons such as computation resource. The above diagonalization process is also called eigendecomposition ($A = VDV^{-1}$) because we can find the followings from above equation, $VD=AV$ $D$ is a diagonal matrix with eigenvalues in diagonal entries $V$ is a matrix whose column vectors","tags":null,"title":"Linear Algebra for ML #4 | Eigen Decomposition ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec4/","year":"2019"},{"content":"\n3.0 Least Square Inner Product : Given $ \\mathbf{u,v} \\in \\mathbb{R}^n$, we can consider $ \\mathbf{u,v} $ as $n \\times 1$ matrices. The number $\\mathbf{u^Tv}$ is called inner product or dot product, and it is written as $ \\mathbf{u \\cdot v} $.\nVector Norm : The length or magnitude of $\\mathbf{v}$, can be calculated as $ \\sqrt{ \\mathbf{v} \\cdot \\mathbf{v} }$\n$L_p$ Norm : $ \\Vert \\mathbf{x} \\Vert_p = ( \\Sigma_{i=1}^n \\vert x_i \\vert^p)^{1/p}$\nEuclidean Norm : $L_2$ norm\nFrobenius Norm : can be seen as an expansion of $L_2$ norm to apply to the matrix : $ \\Vert A \\Vert_F = \\sqrt{\\Sigma_{i=1}^m \\Sigma_{j=1}^n} \\vert a_{ij}\\vert^2$\nUnit Vector : A vector whose length is 1\nNormalizing : Given a nonzero vector $\\mathbf{v}$, if we divide it by its length, we obtain a unit vector.\nDistance between vectors : dist($\\mathbf{u,v}$) = norm($\\mathbf{u-v}$) = $\\Vert \\mathbf{u-v} \\Vert$\nInner Product and Angle Between Vectors : $\\mathbf{u \\cdot v} = \\mathbf{\\Vert u \\Vert \\Vert v \\Vert} cos \\theta $\nOrthogonal Vectors : $\\mathbf{u \\cdot v} = \\mathbf{\\Vert u \\Vert \\Vert v \\Vert} cos \\theta = 0$. That is $ (\\mathbf{u \\perp v})$\n3.1 Introduction to Least Squares Problem Over-Determined : number of equations \u0026gt; number of variables (we have much more data examples), usually no solution exists\nMotivation for Least Squares : Even if no solution exists, we want to approximately obtain the solution for an over-determined system.\nThe example below shows how to determine which solution is better between $\\begin{bmatrix} -0.12 \\\\ 16 \\\\ -9.5 \\end{bmatrix}$ and $\\begin{bmatrix} -0.4 \\\\ 20 \\\\ -20 \\end{bmatrix}$ for given Over-Determined System. Least Squares Problem : Given an over-determined system, $A \\mathbf{x \\simeq b}$, a least squares solution $\\hat{x}$ is defined as $$\\mathbf{\\hat{x}} = argmin_x | \\mathbf{b} - A \\mathbf{x} | $$\nThe most important aspect of the least-squares problem is that no matter what $\\mathbf{x}$ we select, the vector $A \\mathbf{x}$ will necessarily be in the column space Col $A$\nThus, we seek for $\\mathbf{x}$ that makes $A \\mathbf{x}$ as the closest point in Col $A$ to $\\mathbf{b}$\n3.2 Geometric Interpretation of Least squares Consider $ \\mathbf{\\hat{x}} $ such that $ \\hat{b} = A\\mathbf{\\hat{x}} $ is the closest point to $\\mathbf{b}$ among all points in Column Space of $A$.\nThat is, $\\mathbf{b}$ is closer to $\\mathbf{\\hat{b}}$ than to $A \\mathbf{x}$ for any other $\\mathbf{x}$.\nTo satisfy this, the vector $\\mathbf{b}- A\\mathbf{\\hat{x}}$ should be orthogonal to Col $A$\nThis means $\\mathbf{b} - A \\mathbf{\\hat{x}}$ should be orthogonal to any vector in Col A:\n$$ \\mathbf{b} - A\\mathbf{\\hat{x}} \\perp (x_1\\mathbf{a}_1 + \u0026hellip;. x_p\\mathbf{a}_n)$$\nOr equivalently, $$ A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = 0$$\n$\\mathbf{b} - A \\mathbf{\\hat{x}} \\perp \\mathbf{a_1} \\rightarrow \u0026gt;\\mathbf{a}_1^T(\\mathbf{b} - A \\mathbf{\\hat{x}})=0 $\n$\\mathbf{b} - A \\mathbf{\\hat{x}} \\perp \\mathbf{a_2} \\rightarrow \u0026gt;\\mathbf{a}_2^T(\\mathbf{b} - A \\mathbf{\\hat{x}})=0 $ \u0026hellip;\n$\\mathbf{b} - A \\mathbf{\\hat{x}} \\perp \\mathbf{a_m} \\rightarrow \u0026gt;\\mathbf{a}_m^T(\\mathbf{b} - A \\mathbf{\\hat{x}})=0 $\nsame with the equation below which is called a normal equation $$A^T A\\hat{\\mathbf{x}}= A^T\\mathbf{b}$$\n3.3 Normal Equation given a least squares problem, $Ax \\simeq \\mathbf{b}$ , we obtain normal equation $$A^T A\\hat{\\mathbf{x}}= A^T\\mathbf{b}$$\nif $A^T A$ is invertible, then the solution is computed as $$\\hat{\\mathbf{x}}= (A^T A)^{-1} A^T\\mathbf{b} $$\nIf $A^T A$ is not invertible, the system has either no solution or infinitely many solutions. However, the solution always exist for this \u0026ldquo;normal\u0026rdquo; equation, and thus infinitely many solutions exist.\nIf and only if the columns of $A$ are linearly dependent, $A^TA $ is not invertible. However, $A^T A$ is usually invertible.\n3.4 Orthogonal Projection consider the orthogonal projection of $\\mathbf{b}$ onto Col $A$ as\n$$ \\mathbf{\\hat{b}} = f(\\mathbf{b})= A{\\mathbf{\\hat{x}}} = A(A^TA)^{-1}A^T\\mathbf{b} $$\nOrthogonal set : A set of vectors {$ \\mathbf{u_1, \u0026hellip; u_p}$} in $\\mathbb{R^n}$ if each pair of distinct vectors from the set is orthogonal. That is, if $\\mathbf{u_i \\cdot u_j}=0$ whenever $i \\neq j$. So, All vectors in the orthogonal set are orthogonal to each other.\nOrthonormal set : A set of vectors {$ \\mathbf{u_1, \u0026hellip; u_p}$} in $\\mathbb{R^n}$ if it is an orthogonal set of unit vectors (norm=1) .\nOrthogonal and Orthonormal Basis : Consider basis {$\\mathbf{v_1, \u0026hellip; , v_p}$} of a p-dimensional subspace $W $ in $\\mathbb{R}^n$. We can make it as an orthogonal(or orthonormal) basis using Gram-Schmidt process.\nOrthogonal Projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto Line : Consider the orthogonal projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto Line (1D subspace $L$). From the above picture, $\\hat{\\mathbf{y}}$ can be represented by multiplication of the norm (length) for $\\hat{\\mathbf{y}}$ (=$ | | \\hat{\\mathbf{y}} | |$) and the unit vector of $\\mathbf{u}$ ( = $\\mathbf{\\frac{u}{||u||}} $ ). And we can calculate $\\hat{\\mathbf{y}}$ from the inner product of $\\mathbf{y} \\text{ and } \\mathbf{u}$ $$ \\hat{\\mathbf{y}} = proj_L(\\mathbf{y})$$\n$$ = | | \\hat{\\mathbf{y}} | | \\cdot \\mathbf{\\frac{u}{||u||}} = \\mathbf{\\frac{y \\cdot u}{||u||}} \\cdot \\mathbf{\\frac{u}{||u||}} $$\n$$= \\mathbf{\\frac{y \\cdot u}{u \\cdot u}} \\cdot \\mathbf{u}, ( \\because \\mathbf{||u||}^2 = \\mathbf{u \\cdot u})$$\nOrthogonal Projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto Plane : Consider the orthogonal projection $\\hat{\\mathbf{y}}$ of $\\mathbf{y}$ onto two-dimensional subspace $W$ $$ W = span ( \\mathbf{u_1, u_2} ) $$\n$$ \\hat{\\mathbf{y}} = proj_L(\\mathbf{y}) = \\mathbf{\\frac{y \\cdot u_1}{u_1 \\cdot u_1}} \\cdot \\mathbf{u_1} +\\mathbf{\\frac{y \\cdot u_2}{u_2 \\cdot u_2}} \\cdot \\mathbf{u_2} $$\nOrthogonal Projection as a Linear Transformation : Consider a transformation of orthogonal projection $\\mathbf{\\hat{b}}$ of $\\mathbf{b}$. If orthonormal basis {$\\mathbf{u_1, u_2}$} of a subspace $W$ is given, both $\\mathbf{u_1, u_2}$ are unit vector and we can get following equation : $$ \\mathbf{\\hat{b}} = f(\\mathbf{b}) = \\mathbf{ (b \\cdot u_1)u_1 + (b \\cdot u_2 )u_2 } $$\n$$ \\mathbf{ = (u_1^Tb)u_1 + (u_2^T b)u_2 = (u_1u_1^T + u_2 u_2^T)b = \\begin{bmatrix} u_1^T \\\\ u_2^T \\end{bmatrix} \\begin{bmatrix} u_1 \\ u_2 \\end{bmatrix} = UU^T b } $$\n3.5 Gram-Schmidt Orthonomalization If two (or more) vectors are linearly independent, these vectors create an vector space. And we want to represent this vector space using a orthogonal (or orthonormal) set in many cases. In the previous chapter, we\u0026rsquo;ve learned orthogonal projection of one vector to the other. From this we can represent a given vector space (linearly independent vector set) with a orthogonal vector set : Gram-Schmidt Example: Let $W=span( \\mathbf{x_1, x_2})$, where $\\mathbf{x_1} = \\begin{bmatrix} 3\\\\ 6\\\\ 0\\end{bmatrix}$, $\\mathbf{x_2} = \\begin{bmatrix} 1\\\\ 2\\\\ 2\\end{bmatrix}$. Construct an orthogonal basis {$\\mathbf{v_1, v_2}$} for $W$\nSolution:\nLet $\\mathbf{v_1} = \\mathbf{x_1}$. And, let $\\mathbf{v_2}$ the component of $\\mathbf{x_2}$ is orthogonal to $\\mathbf{x_1}$, i.e. $$ \\mathbf{v_2 = x_2 - proj_{v1}(x2) = x_2 - \\frac{x_2 \\cdot x_1}{x_1 \\cdot x_1} x_1 }= \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} - \\frac{15}{45}\\begin{bmatrix} 3 \\\\ 6 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 2 \\end{bmatrix} $$ Now, we get orthogonal basis for $W$. That is, $$\\mathbf{v_1} = \\begin{bmatrix} 3\\\\ 6\\\\ 0\\end{bmatrix}, \\mathbf{v_2} = \\begin{bmatrix} 0\\\\ 0\\\\ 2\\end{bmatrix}$$ We can get the orthonormal set $\\mathbf{(u_1,u_2)}$ of these vectors by dividing the norm of each vectors $$\\mathbf{u_1} = \\frac{1}{\\sqrt{45}} \\begin{bmatrix} 3\\\\ 6\\\\ 0\\end{bmatrix}, \\mathbf{u_2} = \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix}$$ 3.6 QR Factorization (QR Decomposition) QR Decomposition : is a decomposition of a matrix $A$ into a orthogonal matrix $Q$ (with Gram-Schmidt) and upper triangular matrix $R$ . For a given lineary independent matrix, we can find orthogonal basis of the vector space using Gram-Schumidt. There should be another matrix that undo the above orthogonalization tranfrom Here, the orthogonalized matrix is represented as $Q$ and the \u0026lsquo;un-doing\u0026rsquo; matrix is represented as $R$. $$A \\rightarrow \\text{Gram-Schumidt} \\rightarrow Q \\rightarrow \\times R \\rightarrow A $$\nExample : Consider the decomposition of linearly independeny matrix $A = \\begin{bmatrix} 12\u0026amp;-51\u0026amp;4\\\\ 6\u0026amp;167 \u0026amp;-68\\\\ -4\u0026amp;24\u0026amp;-41\\end{bmatrix}$\nSolution :\nThen, we can calculate $Q$ by means of Gram-Schmidt as follows:\n$$U = [\\mathbf{u1, u2, u3}] = \\begin{bmatrix} 12\u0026amp;-69\u0026amp;-58/5\\\\ 6\u0026amp;158 \u0026amp;-6/5\\\\ -4\u0026amp;30\u0026amp;-33\\end{bmatrix}$$\n$$ Q = [\\mathbf{\\frac{u_1}{|u_1|}, \\frac{u_2}{|u_2|}, \\frac{u_3}{|u_3|}}] = \\begin{bmatrix} 6/7\u0026amp;-69/175\u0026amp;-58/175\\\\ 3/7\u0026amp;158/175 \u0026amp;-6/175\\\\ -2/7\u0026amp;30/175\u0026amp;-33/35\\end{bmatrix} $$\nThus, we have\n$$R = Q^TA = \\begin{bmatrix} 14\u0026amp;21\u0026amp;-14\\\\ 0\u0026amp;175 \u0026amp;-70\\\\ 0\u0026amp;0\u0026amp;35\\end{bmatrix}$$\nSummary Least Square Problem : No solution exists for the linear equation $Ax=b$\n=\u0026gt; Approximate the solution $\\hat{x}$ minimizes $ b - A \\hat{x}$.\n=\u0026gt; And $ b - A \\hat{x}$ should be orthogonal to Column Space =\u0026gt; $ A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = 0$\n=\u0026gt; From this equation, We get the normal equation $A^T A\\hat{\\mathbf{x}}= A^T\\mathbf{b}$\n=\u0026gt; And if $A^T A$ is invertible, the solution $\\hat{\\mathbf{x}}= (A^T A)^{-1}A^T\\mathbf{b}$\n=\u0026gt; On the other view, $\\hat{b}$ is the orthogonal projection of $b$\n=\u0026gt; Now, we can find orthogonal projection of a vector\n=\u0026gt; we can find orthogonal vector set using given linearly independ vectors (in that vector space)\n","id":32,"section":"Mathematics","summary":"3.0 Least Square Inner Product : Given $ \\mathbf{u,v} \\in \\mathbb{R}^n$, we can consider $ \\mathbf{u,v} $ as $n \\times 1$ matrices. The number $\\mathbf{u^Tv}$ is called inner product or dot product, and it is written as $ \\mathbf{u \\cdot v} $. Vector Norm : The length or magnitude of $\\mathbf{v}$, can be calculated as $ \\sqrt{ \\mathbf{v} \\cdot \\mathbf{v} }$ $L_p$ Norm : $ \\Vert \\mathbf{x} \\Vert_p = (","tags":null,"title":"Linear Algebra for ML #3 | Least Square ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec3/","year":"2019"},{"content":"\n2.1 Linear Equation and Linear System Linear Equation is an equation that can be written in the form $$ a_1x_1 + \u0026hellip;. a_nx_n = b $$ The above equation can be written as $ \\textbf{a}^T \\textbf{x} = b $. Linear System is a collection of one or more linear equations Identity Matrix : $I$ is a square matrix whose diagonal entries are all 1\u0026rsquo;s and all the other entries are zeros.\nInverse Matrix : $A^{-1}$ is defined such that $A^{-1}A = AA^{-1} = I$\n$$A^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d\u0026amp;{-b}\\\\ -c\u0026amp;a \\end{bmatrix}$$\nDeterminant : $det(A) = ad-bc$ determines whether $A$ is invertible\nIf $A$ is non invertible, $A \\textbf{x} = \\textbf{b}$ will have either no solution or infinitely many solutions.\n2.2 Linear Combination For given vectors $\\textbf{v}_1,\\textbf{v}_2, \u0026hellip; \\textbf{v}_p $ and given scalars $ c_1, c_2, \u0026hellip; c_p $,\n$ c_1\\textbf{v}_1 + c_2 \\textbf{v}_2, \u0026hellip; + c_p \\textbf{v}_p $ is called Linear Combination of vectors with weights $c$.\nSpan : Span{$\\textbf{v}_1, \u0026hellip; \\textbf{v}_p$} is defined as the set of all linear combinations of $\\textbf{v}_1, \u0026hellip; \\textbf{v}_p$. That is, Span is the collection of all vectors that can be written in the form $ c_1\\textbf{v}_1 + c_2 \\textbf{v}_2, \u0026hellip; + c_p \\textbf{v}_p $. Geometric Description of Span : $ \\textbf{v}_1 $ and $\\textbf{v}_2$ are in $\\mathbb{R}^3$ then Span is the plane in $\\mathbb{R}^3$ that contains $ \\textbf{v}_1 $ and $\\textbf{v}_2$ Geometric Interpretation of Vector Equation : we can find whether the solution of vector equation exists using the knowledge of span. The solution of below vector equation exists only when $ \\textbf{b} \\in Span(\\textbf{a}_1, \\textbf{a}_2, \\textbf{a}_3) $. $$\\textbf{a}_1 x_1 + \\textbf{a}_2 x_2 + \\textbf{a}_3 x_3 = b $$\nMatrix Multiplications as Linear Combinations of Vectors : $$ \\begin{bmatrix} 60\u0026amp;1.7\u0026amp;1 \\\\ 65\u0026amp;1.6\u0026amp;0 \\\\ 55\u0026amp;1.8\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 60\\\\ 65\\\\ 55\\end{bmatrix} x_1 + \\begin{bmatrix} 1.2 \\\\ 1.6 \\\\ 1.8 \\end{bmatrix} x_2 + \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} x_3 $$\nMatrix Multiplication as Column Combinations $$ \\begin{bmatrix} 1\u0026amp;1\u0026amp;0 \\\\ 1\u0026amp;0\u0026amp;1 \\\\ 1\u0026amp;-1\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 1\\\\ 1\\end{bmatrix}1 + \\begin{bmatrix} 1\\\\ 0\\\\ -1\\end{bmatrix}2 + \\begin{bmatrix} 0\\\\ 1\\\\ 1\\end{bmatrix}3 $$\nMatrix Multiplication as Row Combinations $$ \\begin{bmatrix} 1\u0026amp;2\u0026amp;3 \\end{bmatrix} \\begin{bmatrix} 1\u0026amp;1\u0026amp;0 \\\\ 1\u0026amp;0\u0026amp;1 \\\\ 1\u0026amp;-1\u0026amp;1 \\end{bmatrix} = 1 \\times \\begin{bmatrix} 1\u0026amp;1\u0026amp;0 \\end{bmatrix} + 2\\times \\begin{bmatrix} 1\u0026amp; 0\u0026amp;1 \\end{bmatrix} + 3 \\times \\begin{bmatrix} 1\u0026amp; -1\u0026amp; 1\\end{bmatrix}$$\nMatrix Multiplication as Sum of Outer Products $$\\begin{bmatrix} 1\u0026amp;1 \\\\ 1\u0026amp;-1 \\\\ 1\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} 1\u0026amp; 2\u0026amp; 3 \\\\ 4\u0026amp;5\u0026amp;6 \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 1\\\\ 1\\end{bmatrix} \\begin{bmatrix} 1\u0026amp; 2\u0026amp; 3\\end{bmatrix} + \\begin{bmatrix} 1\\\\ -1\\\\ 1\\end{bmatrix} \\begin{bmatrix} 4\u0026amp; 5\u0026amp; 6\\end{bmatrix} $$\n2.3 Linear Independence The solution for $A \\textbf{x} = \\textbf{b} $ is unique, when $\\textbf{a}_1, \\textbf{a}_2, \\textbf{a}_3 $ are linearly independent\nInfinitely many solutions exists when $\\textbf{a}_1, \\textbf{a}_2, \\textbf{a}_3 $ are linearly dependent.\nLinear Independence (practical) : Given a set of vectors $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ , check if $ \\textbf{v}_j$ can be represented as a linear combination of the previous vectors. If at least one such $ \\textbf{v}_j$ is found then $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ is linearly dependent. Linear Independence (Formal) : Consider $ x_1 \\textbf{v}_1 + \u0026hellip; + x_p \\textbf{v}_p = \\textbf{0} $. Obviously, one solution is $\\textbf{x} = [0\u0026hellip;0]$, which we call a trivial solution. If this is the only solution $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ is the only solution. if this system also has other nontrivial solutions, $ \\textbf {v}_1 , \u0026hellip; , \\textbf {v}_p $ are linearly dependent. 2.4 Basis of a Subspace Subspace : is defined as a subset of $\\mathbb{R}^n$ closed under linear combination\nA subspace is always represented as Span {$\\mathbf{v}_1 , \u0026hellip; , \\mathbf{v}_p$}\nBasis of a subspace : a set of vectors that satisfies both of the following Fully spans the given subspace $H$\nLinearly independent (i.e., no redundancy) Non-Uniqueness of Basis : In the subspace $H$ (green plane), there other set of linearly independent vectors that span the subspace $H$ Dimension of Subspace : Even though different basis exist for $H$, the number of vectors in any basis for $H$ will be unique. This number is the dimension of $H$. Column Space of Matrix : the subspace could be spanned by the column vector of $A$. Rank of Matrix : the dimension of the column space of $A$ Details $$ A = \\begin{bmatrix} 1\u0026amp;0\u0026amp;2 \\\\ 0\u0026amp;1\u0026amp;1 \\\\ 1\u0026amp;0\u0026amp;2 \\end{bmatrix}, \\ \\ \\text{ Column Vector } a_1, a_2, a_3 = \\begin{bmatrix} 1\\\\ 0\\\\ 1\\end{bmatrix} , \\begin{bmatrix} 0\\\\ 1\\\\ 0\\end{bmatrix}, \\begin{bmatrix} 2\\\\ 1\\\\ 2\\end{bmatrix} $$\nColumn Space is subspace of column vectors from matrix $A$. And we need to find basis vector to get column space.\n$a_1, a_2$ is independent each other. $a_3$ is represented as linear combination of $a_1, a_2$ . As a result, $a_3$ is dependent on $a_1, a_2$. So, $a_1, a_2$ become basis vector for colum space of $A$\nHere, Rank of matrix $A$ is two. Because we have only two vectors $a_1, a_2$ that are linearly independent.\nIf a solution for $A \\textbf{x} = \\textbf{b} $ exists, then $b$ must be on the column space of $A$. Because, $b$ is linear combination of column vectors\n$$ Ax = \\begin{bmatrix} 1\u0026amp;0\u0026amp;2 \\\\ 0\u0026amp;1\u0026amp;1 \\\\ 1\u0026amp;0\u0026amp;2 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = \\begin{bmatrix} 1\\\\ 0\\\\ 1\\end{bmatrix} x_1 + \\begin{bmatrix} 0\\\\ 1\\\\ 0\\end{bmatrix} x_2 + \\begin{bmatrix} 2\\\\ 1\\\\ 2\\end{bmatrix} z_3 = \\begin{bmatrix} b_1\\\\ b_2\\\\ b_3\\end{bmatrix} $$\n2.5 Chage of Basis Generally, vectors are defined on cartesian coordinate , and it means they are based on standard basis (1,0), (0,1). But sometimes, we need vectors on different coordinate (basis) for some reasons like computational efficiency. (e.g. eigendecomposition) When the coordinate changes, the vector itself does not changed, but the representation of vector has been changed from (2,2) to (2,0) as in figure below 2.6 Linear Transformation A transformation, function, or mapping maps an input $x$ to an output $y$\nDomain : Set of all the possible values of $x$\nCo-Domain : Set of all the possible values of $y$\nRange : Set of all the output values mapped by each $x$ in the domain\nLinear Transforamtion : A transformation (or mapping) $T$ is linear if : $$ T(c \\mathbf{u}+ d \\mathbf{v}) = cT (\\mathbf{u}) + dT(\\mathbf{v})$$\nMatrix of Linear Transformation : In general, let $T$ : $ \\mathbb{R}^n \\rightarrow \\mathbb{R}^m $ be a linear transformation. Then $T$ is always written as a matrix-vector multiplication. i.e., $T(\\mathbf{x}) = A \\mathbf{x}$\nHere, the matrix $A$ is called the standad matrix of the linear transformation.\n2.7 Linear Transformation in Neural Networks Fully Connected Layers can be thought as a linear transformation\nFully Connected Layers usually involve a bias term, That\u0026rsquo;s why we call it an affine layer, not a linear Layers\nReference : colah\u0026rsquo;s blog\n2.8 ONTO and ONE-TO-ONE ONTO : A mapping $T$ : $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is said to be onto $\\mathbb{R}^m$ if each $\\mathbf{b} \\in \\mathbb{R}^m$ is the image of at least one $x \\in \\mathbb{R}^n$. That is, the range is equal to the co-domain.\nONE-TO-ONE : A mapping $T$ : $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is said to be one-to-one if each $\\mathbf{b} \\in \\mathbb{R}^m$ is the image of at most one $x \\in \\mathbb{R}^n$. That is, each output vector in the range is mapped by only one input vector, no more than that.\n","id":33,"section":"Mathematics","summary":"2.1 Linear Equation and Linear System Linear Equation is an equation that can be written in the form $$ a_1x_1 + \u0026hellip;. a_nx_n = b $$ The above equation can be written as $ \\textbf{a}^T \\textbf{x} = b $. Linear System is a collection of one or more linear equations Identity Matrix : $I$ is a square matrix whose diagonal entries are all 1\u0026rsquo;s and all the other entries are","tags":null,"title":"Linear Algebra for ML #2 | Linear System \u0026 Linear Transform ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec2/","year":"2019"},{"content":"\n1.1 Scalars, Vectors, Matrices, and Tensors Scalars : just a single number, italics in this book, lower-case variable name, such as $x$ (loswercase)\nVectors : an array of numbers, in order, bold lower case name, such as $\\bf x$ (bold lowercase)\nMatrices : 2-D array of numbers, with two indices, bold uppercase variable name, such as $A$ (uppercase)\nTensors : an array of numbers arranged on a regular grid with a variable number of axes.\n1.2 Matrix Additions and Multiplications Transpose : operation on matrices. We can define a vector using the transpose operator e.g. ${\\bf x} = [x_{1}, x_{2}, x_{3}]^{T}$ , for scalar $a = a^{T}$\nAdd : we can add matrices to each other as long as they have the same shape.\nTo define the matrix product of matrices $A$ and $B$, $A$ must have the same number of columns as the number of rows in $B$\nThe matrix product is not commutative ( $AB ≠ BA$ )\n$$C_{i,j} = \\sum_k A_{i,k}B_{k,j}$$\nelement-wise product (product of the individual elements) is denoted as $A\\odot B$\ndot-product between two vectors $x$ and $y$ is the matrix product $x^{T}y$\n1.3 Reference Linear Algebra for AI, Edwith Lay et al. Linear Algebra and Its applications, 5th editionS Ian Good Fellow. Deep Learning Book Gilbert Strang\u0026rsquo;s MIT Lecture [Summary of Gilbert LA](https://catonmat.net/mit-linear-algebra-part-one ","id":34,"section":"Mathematics","summary":"1.1 Scalars, Vectors, Matrices, and Tensors Scalars : just a single number, italics in this book, lower-case variable name, such as $x$ (loswercase)\nVectors : an array of numbers, in order, bold lower case name, such as $\\bf x$ (bold lowercase)\nMatrices : 2-D array of numbers, with two indices, bold uppercase variable name, such as $A$ (uppercase)\nTensors : an array of numbers arranged on a regular grid with a variable number of axes.","tags":null,"title":"Linear Algebra for ML #1 | Introductions ","uri":"https://koreanbear89.github.io/mathematics/1.-linear-algebra/linear-algebra-for-ml-lec1/","year":"2019"},{"content":"\n6.1 Introduction Statistical Inference, or \u0026ldquo;learning\u0026rdquo; as it is called in computer science, is the process of using data to infer distribution that generated the data. 6.2 Parametric and Nonparametric Models A statistical model $\\Im$ is a set of distributions (or densities or regression functions)\nParametric model : is a set of $\\Im$ that can be parameterized by a finite number of parameters\nIf we assume that the data come from a Normal distribution, then It would be two-prarmeter model.\n$$ \\Im = {f(x; \\mu \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}} exp {-\\frac{1}{2 \\sigma^2}(x-\\mu)^2 } $$\nIn general, a parametric model takes the form $$ \\Im = {f(x; \\theta) : \\theta \\in \\Theta }$$\nNon-Parametric model : is a set $\\Im$ that cannot be parameterized by a finited number of parameters.\nFrequentists and Bayesians : The two dominant approaches to statistical inference are called frequentists inference and Bayyesian Inference.\n6.3 Fundamental Concepts in inference Many inferential problems can be identified as being one of three types : estimation, confidence sets, or hypothesis testing.\nPoint Estimation : refers to providing a single \u0026ldquo;best guess\u0026rdquo; of some quantity of interest\nConfidence Sets : A $1-\\alpha$ confidence interval for a parameter $\\theta$ is an interval $C_n = (a,b)$ where $a = a(X_1, \u0026hellip; , X_n)$ and $b = b(X_1, \u0026hellip; , X_n)$\nHypothesis Testing : In hypothesis testing, we start with some default theory , called null hypothesis, and we ask if the data provide sufficient evidence to reject the theory.\n","id":35,"section":"Mathematics","summary":"6.1 Introduction Statistical Inference, or \u0026ldquo;learning\u0026rdquo; as it is called in computer science, is the process of using data to infer distribution that generated the data. 6.2 Parametric and Nonparametric Models A statistical model $\\Im$ is a set of distributions (or densities or regression functions)\nParametric model : is a set of $\\Im$ that can be parameterized by a finite number of parameters\nIf we assume that the data come from a Normal distribution, then It would be two-prarmeter model.","tags":null,"title":"Statistics #6 | Models, Statistical Inference and Learning ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch6/","year":"2019"},{"content":"\n5.1 Introduction The law of large numbers says that the sample average converges in proability to the expectation $ \\mu = \\mathbb{E}(X_i)$\nThe central limit theorem says that $ \\sqrt{n} (\\overline{X - \\mu})$ converges in distribution to a Normal distribution.\n5.2 Types of Convergence $X_n$ converges to $X$ in probability, written $X_n \\xrightarrow{P}{} X$ if for every $\\epsilon \u0026gt; 0$ $$ \\mathbb{P}(|X_n - X| \u0026gt; \\epsilon) \\rightarrow 0$$\n$X_n$ converges to $X$ in distribution, written $X_n \\rightsquigarrow X$ if $$ \\lim_{n\\rightarrow \\infty} F_n(t) = F(t) $$\n$X_n$ converges to $X$ in quadratic mean (also called convergence in $L_2$), written $X_n \\xrightarrow{qm}{} X$, if $$ \\mathbb{E}(X_n - X)^2 \\rightarrow 0 \\ \\text{as} \\ n \\rightarrow \\infty $$\n5.3 The Law of Large Numbers The weak Law of Large Numbers $$ If X_1 , \u0026hellip; , X_n \\ \\text{ are IID, then } \\overline{X}_n \\xrightarrow{P}{} \\mu$$\n5.4 The Central Limit Theorem Let $X_1, \u0026hellip; , X_n$ be IID with mean $\\mu$ and variance $\\sigma^2$. Let $\\overline{X_n} = n^{-1} \\sum_{i=1}^n X_i$ Then, $$Z_n \\equiv \\frac{\\overline{X_n}-\\mu}{\\sqrt{\\mathbb{V(\\overline{X_n})}}} = \\frac{\\sqrt{n}({\\overline{X_n}-\\mu)}}{\\sigma} \\rightsquigarrow Z$$\n","id":36,"section":"Mathematics","summary":"5.1 Introduction The law of large numbers says that the sample average converges in proability to the expectation $ \\mu = \\mathbb{E}(X_i)$\nThe central limit theorem says that $ \\sqrt{n} (\\overline{X - \\mu})$ converges in distribution to a Normal distribution.\n5.2 Types of Convergence $X_n$ converges to $X$ in probability, written $X_n \\xrightarrow{P}{} X$ if for every $\\epsilon \u0026gt; 0$ $$ \\mathbb{P}(|X_n - X| \u0026gt; \\epsilon) \\rightarrow 0$$\n$X_n$ converges to $X$ in distribution, written $X_n \\rightsquigarrow X$ if $$ \\lim_{n\\rightarrow \\infty} F_n(t) = F(t) $$","tags":null,"title":"Statistics #5 | Convergence of Random Variable ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch5/","year":"2019"},{"content":"\n3.1 Expectation of a Random Variable The expected value, or mean, or first moment of $X$ is defined to be $$ \\mathbb{E}(X) = \\int xdF(x) = \\begin{cases} \\sum_x xf(x) \u0026amp; (\\text{if X is discrete})\\ \\int xf(x)dx \u0026amp; ( \\text{if X is continuous}) \\end{cases}$$\n3.3 Variance and Covariance The variance measures the spread of a distribution $$ \\sigma^2 = \\mathbb{E}(X-\\mu)^2 = \\int (x-\\mu)^2 dF(x) $$\nThe covariance and correlation between $X$ and $Y$ measure how strong the linear relationship is between $X$ and $Y$ $$\\text{Cov}(X,Y) = \\mathbb{E}((X-\\mu_X)(Y- \\mu_Y ))$$\n$$ \\text{Correlation} \\ \\rho = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} $$\n3.5 Conditional Expectation The conditional expectation of $X$ given $Y=y$ is $$ \\mathbb{E}(X | Y=y) = \\begin{cases} \\sum x f_{X|Y} (x|y) dx \u0026amp; (\\text{discrete case}) \\ \\int x f_{X|Y}(x|y)dx \u0026amp; (\\text{continuous case}) \\end{cases}$$\n3.6 Moment Generating Functions The moment generating function or Laplace transform , of $X$ is defined by $$ \\psi_X(t) = \\mathbb{E}(e^{tX}) = \\int e^{tx}dF(x) $$\n","id":37,"section":"Mathematics","summary":"3.1 Expectation of a Random Variable The expected value, or mean, or first moment of $X$ is defined to be $$ \\mathbb{E}(X) = \\int xdF(x) = \\begin{cases} \\sum_x xf(x) \u0026amp; (\\text{if X is discrete})\\ \\int xf(x)dx \u0026amp; ( \\text{if X is continuous}) \\end{cases}$$\n3.3 Variance and Covariance The variance measures the spread of a distribution $$ \\sigma^2 = \\mathbb{E}(X-\\mu)^2 = \\int (x-\\mu)^2 dF(x) $$\nThe covariance and correlation between $X$ and $Y$ measure how strong the linear relationship is between $X$ and $Y$ $$\\text{Cov}(X,Y) = \\mathbb{E}((X-\\mu_X)(Y- \\mu_Y ))$$","tags":null,"title":"Statistics #3 | Expectation ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch3/","year":"2019"},{"content":"\n2.1 Introduction How do we link sample spaces and events to data? Random Variable!\nA random variable is a mapping $ X : \\Omega \\rightarrow \\mathbb{R} $ that assigns a real number $X(\\omega)$ to each outcome $\\omega$.\n2.2 Distribuition Functions and Probability Functions Cumulative Distribution Function (CDF) : is the function $F_x : \\mathbb{R} \\rightarrow [0,1] $ defined by $$F_X(x) = \\mathbb{P}(X \\leq x)$$\nProbability Mass Function : A Random Variable $X$ is discrete if it takes countably many values ${x_1, x_2, \u0026hellip;}$. We define probability mass function for $X$ by $$f_X(x) = \\mathbb{P}(X=x).$$\nProbability Density Function : A Random Variabe $X$ is continuous if there exists a function $f_X$ such that $f_X(x) \\geq$ for all $x$, $\\int f_X(x)dx = 1$ and for every $a \\leq b$, the probability Density function is $$ \\mathbb{P}(a\u0026lt;X\u0026lt;b) = \\int_{a}^b f_X(x)dx $$\n2.3 Some Important Discrete Random Variables The Point Mass Distribution $$ F(x) = \\begin{cases} 0 \u0026amp; (x\u0026lt;a)\\ 1 \u0026amp; (x\\geq a) \\end{cases} $$\nThe Discrete Uniform Distribution $$ f(x) = \\begin{cases} 1/k \u0026amp; ( \\text{for} \\ x=1, \u0026hellip; ,k ) \\ 0 \u0026amp; ( \\text{otherwise} ) \\end{cases} $$\nThe Bernoulli Distribution : Let $X$ represent a binary coin flip. Then $\\mathbb{P}(X=1) = p $ and $\\mathbb{P}(X=0) = 1- p$ for some $p \\in [0,1] $. We saye that $X$ has a Bernoulli Distribution written $X \\sim Bernoulli(p)$. $$ f(x) = p^x(1-p)^{1-x} \\ \\text{for} \\ x \\in {0,1}$$\nThe Binomial Distribution $$ f(x) = \\begin{cases} {n \\choose x} p^x (1-p)^{n-x} \u0026amp; ( \\text{for} \\ x=0, \u0026hellip; ,n ) \\ 0 \u0026amp; ( \\text{otherwise} ) \\end{cases} $$\nThe Geometric Distribution $$ \\mathbb{P}(X=k) = p(1-p)^{k-1} , \\ k \\geq 1 $$\nThe Poisson Distribution : The Poisson is often used as a model for counts of rare events like radioactive decay and traffic accidents. $$ f(x) = e^{- \\lambda} \\frac{\\lambda^x}{x!}, \\ x \\geq 0 $$\n2.4 Some Important Continuous Random Variables The Uniform Distribution:\n$$ f(x) = \\begin{cases} \\frac{1}{b-a} \u0026amp; ( \\text{for} \\ x \\in [a,b]) \\ 0 \u0026amp; ( \\text{otherwise} ) \\end{cases} $$\nGaussian Distribution : $X$ has a Normal distribution with parameters $ \\mu $ and $\\sigma$, denoted by $X \\sim N(\\mu, \\sigma^2)$\nExponential Distribution : $X$ has an Exponential distribution with parameter $\\beta$, denoted by $X \\sim \\text{Exp}(\\beta)$, if\n$$f(x) = \\frac{1}{\\beta} e^{-x/\\beta}$$\n2.5 Bivariate Distributions Joint Mass Function : Given a pair of discrete random variables $X$ and $Y$ define the joint mass function by $f(x,y) = \\mathbb{P}(X = x, Y = y)$ 2.6 Marginal Distributions If $(X, Y)$ have joint distribution with mass function $f_{X,Y}$ then the marginal mass function for $X$ and $Y$ is defined by $$ f_X(x) = \\mathbb{P}(X=x) = \\sum_y \\mathbb{P}(X=x, Y=y) = \\sum_y f(x,y) $$\n$$ f_Y(y) = \\mathbb{P}(Y=y) = \\sum_x \\mathbb{P}(X=x, Y=y) = \\sum_x f(x,y) $$\nFor continuous random variables, the marginal densities are $$ f_X(x) = \\int f(x,y) dy, \\ \\text{and} f_Y(y) = \\int f(x,y)dx $$\n2.7 Independent Random Variables Two random variables $X$ and $Y$ are independent if for every $A$ and $B$, $$ \\mathbb{P}(X \\in A, Y \\in B) = \\mathbb{P}(X \\in A) \\mathbb{P}(Y \\in B)$$\n2.8 Conditional Distributions The conditional probability mass function is $$ f_{X|Y}(x|y) = \\mathbb{P}(X=x | Y=y) = \\frac{\\mathbb{P}(X=x, Y=y)}{\\mathbb{P}(Y=y)} = \\frac{f_{X,Y}(x,y)}{f_Y(y)}$$\n2.9 Multivariate Distributions and IID Samples If $X_1, \u0026hellip; , X_n $ are independent and each has the same marginal distribution with CDF $F$ we say that $X_1, \u0026hellip; , X_n$ are IID(independent and identically distributed) and we write $$ X_1 , \u0026hellip; , X_n \\sim F.$$\n2.10 Two Important Multivariate Distributions Multinomial : The multivariate version of a Binomial is called a Multinomial.\nMulticariate Normal : The univariate normal has two parameters, $\\mu$ and $\\sigma$. In the multivariate version, $\\mu$ is a vector and $\\sigma$ is replaced by a matrix $\\Sigma$\n2.11 Transformations of Random variables Suppose that $X$ is a random variable with PDF $f_X$ and CDF $F_X$. Let $Y$ = r(X) be a function of $X$, for example, $Y = X^2$ or $Y = e^X$. We call $Y=r(X)$ a Transformation of $X$. 2.12 Transformations of Several Random variables Three Steps for Transformation\nFor each $z$, find the set $A_z = { (x,y) : r(x,y) \\leq z }$\nFind the CDF\n$$F_Z(z) = \\mathbb{P}(Z \\leq z) = \\mathbb{P}(r(X,Y) \\leq z) = \\mathbb{P}({ (x,y); r(x,y) \\leq z })$$\nThem $f_Z(z) = F\u0026rsquo;_{Z}(z)$\n","id":38,"section":"Mathematics","summary":"2.1 Introduction How do we link sample spaces and events to data? Random Variable!\nA random variable is a mapping $ X : \\Omega \\rightarrow \\mathbb{R} $ that assigns a real number $X(\\omega)$ to each outcome $\\omega$.\n2.2 Distribuition Functions and Probability Functions Cumulative Distribution Function (CDF) : is the function $F_x : \\mathbb{R} \\rightarrow [0,1] $ defined by $$F_X(x) = \\mathbb{P}(X \\leq x)$$\nProbability Mass Function : A Random Variable $X$ is discrete if it takes countably many values ${x_1, x_2, \u0026hellip;}$.","tags":null,"title":"Statistics #2 | Random Variables ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch2/","year":"2019"},{"content":"\n1.1 Introduction Probability is a mathematical language for quantifying uncertatinty 1.2 Sample Spaces and Events Sample Space $\\Omega$ : is the set of possible outcomes of an experiment\nEvents : Subsets of Ω are called Events\n1.3 Probability A function $\\mathbb{P}$ that assigns a real number $ \\mathbb{P}(A) $ to every event $A$ is a probability distribution or a probability measure. 1.4 Probability on Finite Sample Spaces If $\\Omega$ is finite and if each outcome is equally likely, then, the uniform probability distribution is $$ \\mathbb{P} = \\frac{|A|}{| \\Omega |}$$\nAnd we need to count the number of points in an event using combinational methods, to compute probabilities. $${n \\choose x} = \\frac {n \\times (n-1) \\times \u0026hellip; (n-k-1)}{k !} $$\n1.5 Independent Event Two events $A$ and $B$ are independent if $ \\mathbb{P}(AB) = \\mathbb{P}(A) \\mathbb{P}(B)$ 1.6 Conditional probability If $\\mathbb{P}(B)\u0026gt;0$ then the conditional probability of A given B is $$ \\mathbb{P(A|B)} = \\frac{\\mathbb{P}(AB)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} $$\nIn general, $ \\mathbb{P} (A | B) \\neq \\mathbb{P} (B | A) $\n$A$ and $B$ are independent if and only if $ \\mathbb{P}(A | B) = \\mathbb{P}(A) $\n1.7 Bayes\u0026rsquo; Theorem Basis of expert systems and Bayes\u0026rsquo; nets\nBayes\u0026rsquo; Theorem : Let $A_1, \u0026hellip; A_k$ be a partition of $\\Omega$ such that $\\mathbb{P}(A_i) \u0026gt; 0$ for each $i$. If $\\mathbb{P}(B)\u0026gt;0$, then, for each $i = 1 , \u0026hellip; k$,\n$$ \\mathbb{P}(A_i | B) = \\frac{\\mathbb{P}(A_i B)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B | A_i) \\mathbb{P}(A_i)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B | A_i) \\mathbb{P}(A_i)}{ \\sum_j \\mathbb{P}(B | A_j) \\mathbb{P}(A_j)} $$\n","id":39,"section":"Mathematics","summary":"1.1 Introduction Probability is a mathematical language for quantifying uncertatinty 1.2 Sample Spaces and Events Sample Space $\\Omega$ : is the set of possible outcomes of an experiment Events : Subsets of Ω are called Events 1.3 Probability A function $\\mathbb{P}$ that assigns a real number $ \\mathbb{P}(A) $ to every event $A$ is a probability distribution or a probability measure. 1.4 Probability on Finite Sample Spaces If $\\Omega$ is","tags":null,"title":"Statistics #1 | Probability ","uri":"https://koreanbear89.github.io/mathematics/2.-statistics/all-of-statistics-ch1/","year":"2019"},{"content":"\nIntroduction Pull Request Summary\nFork repo to your own repo\nclone, set remote\ngit remote add \u0026lt;remote_name\u0026gt; \u0026lt;URL\u0026gt; Generate branch\ngit checkout -b \u0026lt;feature/issue_number\u0026gt; Add, Commit, Push\ngit push \u0026lt;remote_name\u0026gt; \u0026lt;branch_name\u0026gt; Pull Request\nCode Review\nMerge\nSquash and Merge : squash multiple commits into a new commit\nRebase and Merge : multiple commits will be merged\nFetch upstream\nFetch upstream in local master branch\nremove branch \u0026lt;feature/issue_number\u0026gt;\n1. Setup Git Environment # (1) create new git $ git init # in the working dir, generates .git directory $ git clone \u0026lt;URL\u0026gt; # cloning repo registers remote named origin automatically $ git clone --recursive \u0026lt;URL\u0026gt; # recursive clone including submodule $ git clone -b \u0026lt;BRANCH\u0026gt; \u0026lt;URL\u0026gt; # without \u0026lt;BRANCH\u0026gt;, clone master branch # (2) Add remote git $ git remote # show remotes for current project $ git remote add \u0026lt;remote_name\u0026gt; \u0026lt;URL\u0026gt; # (3) git user check $ git config --list $ git config --global --list # (4) Add user $ git config user.name \u0026quot;Your Name\u0026quot; # only for this project $ git config user.email you@example.com $ git config --global user.name \u0026quot;Your Name\u0026quot; # for global usage $ git config --global user.email you@example.com 2. Changes in source code : Add, Commit, Push # (1) add $ git add * # add changes to index $ git status # Check git status (staged/unstaged) $ git reset HEAD \u0026lt;FILE\u0026gt; # cancle add, unclearstage file. unstage all w/o \u0026lt;FILE # (2) commit $ git commit -m \u0026quot;explain this commit\u0026quot; # commit $ git log # show commit log $ git commit --amend -m \u0026quot;modify commit message\u0026quot; # (3) push $ git push \u0026lt;remote_name\u0026gt; \u0026lt;branch_name\u0026gt; # changes from local to server # (4) pull $ git pull # download and merge $ git fetch # only download, not merge 3. Branch The commited chanes in BRANCH_A do not affect in BRANCH_B # (0) Show all branches $ git branch # (1) create new local branch $ git branch \u0026lt;BRANCH\u0026gt; $ git checkout -b \u0026lt;BRANCH\u0026gt; # Create new branch and change to it # (2) change to the new branch $ git checkout \u0026lt;BRANCH\u0026gt; # (3) Delete branch $ git branch -d \u0026lt;BRANCH\u0026gt; # (3.1) Delete local branch $ git push origin --delete \u0026lt;BRANCH\u0026gt; # (3.2) Delete remote branch # (4) Stash $ git stash $ git stash list $ git stash apply # apply recent stash $ git stash drop # remove recent stash 4. Pull, Merge, Rebase git pull : fetch + merge\ngit pull --rebase \u0026lt;REMOTE\u0026gt; \u0026lt;BRANCH\u0026gt; git merge : performs a 3-way merge between the two latest branch snapshots (C3 and C4) and the most recent common ancestor of the two (C2), creating a new snapshot (and commit).\ngit rebase : take the patch of the change that was introduced in C4 and reapply it on top of C3\nReferenes\nGit - Merge\nGit - Rebase 5. gitignore File # ignore every 'file.ext' in project file.ext # You cannot write annotation in define ignore-rule line (like below) file.ext # not a comment # ends with / will ignore directory only bin/ # the line below will ignore both file named bin and directory named bin bin # You can use Glob pattern. e.g the line below will ignore build/ and Build [bB]uild/ # You can ignore files with certain extension *.apk # exception from ignore list with ! !.gitignore [Reference] https://rogerdudler.github.io/git-guide/index.ko.html)\n","id":40,"section":"Engineering","summary":"Introduction Pull Request Summary\nFork repo to your own repo\nclone, set remote\ngit remote add \u0026lt;remote_name\u0026gt; \u0026lt;URL\u0026gt; Generate branch\ngit checkout -b \u0026lt;feature/issue_number\u0026gt; Add, Commit, Push\ngit push \u0026lt;remote_name\u0026gt; \u0026lt;branch_name\u0026gt; Pull Request\nCode Review\nMerge\nSquash and Merge : squash multiple commits into a new commit\nRebase and Merge : multiple commits will be merged\nFetch upstream\nFetch upstream in local master branch\nremove branch \u0026lt;feature/issue_number\u0026gt;\n1. Setup Git Environment # (1) create new git $ git init # in the working dir, generates .","tags":null,"title":"CheatSheet | Git","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-git/","year":"2018"},{"content":"\nIntroduction Confusion Matrix : is composed of four elements: TP, TN, FP, and FN. T/F in the front indicates whether the model answered correctly, and P/N in the back indicates the predicted value of the model.\nPositive and Negative should be interpreted as predicted values from the model\u0026rsquo;s point of view, excluding the modeler\u0026rsquo;s subjectivity. For example, the predictive value of having cancer is a negative, but for the model, it is only closer to 1 (positive) than 0 (negative).\nIt is intuitive to read from the back due to the word order difference between English and Korean.\nTP (True Positive) : The prediction of the model is postive and it is true (answer is positive)\nTN (True Negative) : The prediction of the model is negative and it is true (answer is negative)\nFP (False Positive) : The prediction of the model is postive and it is False (answer is negative)\nFN (False Negative) : The prediction of the model is negative and it is False (answer is positive)\nEven if there are multiple classes, it is not very different. For example, in the case of the apple class:\nTP is 7, TN is 1+3+2+3, FP is 8+9, and FN is 1+3 1. True Positive Rate (TPR, Sensitivity, Recall) The probability of a positive prediction, conditioned on truly being positive.\nEx1. Proportion of predicted cancer among patients who actually had cancer $$ \\frac{TP}{P} = \\frac{TP}{TP+FN} $$\n2. True Negative Rate (TNR, Specificity) The probability of a negative prediction, conditioned on truly being negative. Ex2. Proportion of predicted normal out of actual normal people $$ \\frac{TN}{N} = \\frac{TN}{TN+FP} $$\n3. False Positive Rate (FPR, Fall-out) The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification). $$ FPR = \\frac{FP}{N} = 1-TNR = 1-Specificity $$\n3. Positive Predictive Value (PPV, Precision) The probability of a true positive prediction, among every positive predictions\nThe ratio of truely cancer among predicted cancer\nIf \u0026ldquo;Sentivity\u0026rdquo; is from the point of view of the answer (label), \u0026ldquo;Precision\u0026rdquo; is the interpretation from the point of view of the model\n$$ \\frac{TP}{TP+FP} $$\n5. F1-Score F1-Score : The harmonic average of Precision and Recall (harmonic average : the more unbalanced the value used for average calculation, the more the penalty is applied so that the average is calculated closer to the smaller value) Micro (Averaged) F1 : is calculated by considering the total TP, total FP and total FN of the model. It does not consider each class individually, It calculates the metrics globally. Macro (Averaged) F1 : calculates metrics for each class individually and then takes unweighted mean of the measures. Weighted F1: it takes a weighted mean of the measures. The weights for each class are the total number of samples of that class. Reference Metrics in Machine Learning - gaussian37 $$ 2 \\times \\frac{Precision \\times Recall}{Precision + Recall } $$\n6. ROC (Receiver Operating Characteristic) Curve Threshold 를 바꿔가며 Recall (TPR) 과 Fall-out (FPR, FP/(FP+TN)) 의 변화를 시각화한것 x축, FPR은 실제 정상인 중에 암이 있다고 예측한 비율 (FP/N) y축, TPR은 실제 암인 환자중에 암이 있다고 예측한 비율 (TP/P) 즉, test data에 대하여 threshold를 0~1로 바꿔가며, (FPR,TPR) 의 좌표를 구하고 이를 그래프위에 나타내면 ROC커브. ","id":41,"section":"Research","summary":"Introduction Confusion Matrix : is composed of four elements: TP, TN, FP, and FN. T/F in the front indicates whether the model answered correctly, and P/N in the back indicates the predicted value of the model. Positive and Negative should be interpreted as predicted values from the model\u0026rsquo;s point of view, excluding the modeler\u0026rsquo;s subjectivity. For example, the predictive value of having cancer is a negative, but for the model,","tags":null,"title":"ML Basic #4 | Evaluation Metrics","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml04-evalutaion-metrics/","year":"2017"},{"content":"\nIntroduction Activation Function : is a function to induce non-linearity into the output of the neuron for the given input.\nNon-Linearity : Functions that do not satisfy the following linearity.\n$ f(x+y) = f(x)+f(y), \\quad f(\\alpha x) = \\alpha f(x)$ Why Non-Linearity ? : because composites of linear functions are linear again\nif output $f(x)$ of neuron is linear form like $wx + b$,\nEven if the depth of the layer increases, it is just modeling another linear function $f(f(f(..(x)) = w\u0026rsquo;x+b'$\nRole of bias : It\u0026rsquo;s similar to the constant b of a linear function y=ax+b. It allow you to move the line up and down to fit the prediction with the data better.\nWithout $b$, the line always goes through the origin (0,0) and you may get a poorer fit. 1. Sigmoid / Logistic Advantages :\nSmooth gradient : preventing \u0026ldquo;jumps\u0026rdquo; in output values\nClear predictions : for $x$ above 2 or below -2, output tends to be at the edge of the curve, very close to 0 or 1.\nDisadvantage:\nVanishing gradient : for very high or low values of X, there\u0026rsquo;s no change to the prediction, causing a vanishing gradient problem. This can result in the network being too slow to reach an accurate prediction.\nOutputs not zero centered\nComputationally expensive\n$$ \\sigma (x) = (1+e^{-x})^{-1} \\ \\sigma\u0026rsquo;(x) = \\sigma(x) (1-\\sigma(x)) $$\n2. Tan H Properties :\nZero Centered : make it easier to model inputs that have strongly negative,neutral, and strongly positive values\nSame with Sigmoid funtion\n$$ tanh(x) = \\frac{sinh(x)}{cosh(x)}=\\frac{e^x-e^{-x}}{e^x+e^{-x}} \\ tanh\u0026rsquo;(x) = 1 - tanh^2(x) $$\n3. ReLU (Rectified Linear Unit) Advantages :\nComputationally efficient : allows the network to converge very quickly\nNon-Linearity: although it looks like a linear function, ReLU has a derivate fuction and allows for backpropagation.\nDisadvantage:\nDying ReLU : when inputs approach zero, or are negative, the gradient of the function becomes zero, the network cannot perform backpropagation and cannot learn. $$ ReLU = \\begin{cases} :x :: (x\u0026gt;0) \\\\ 0 :: (x\u0026lt;0) \\end{cases} $$\n4. Leaky ReLU Properties :\nalleviate Dying ReLU : To resolve dying ReLU problem, leaky ReLU has non zero outputs with negative input, thus the gradient does not become zero. $$ \\text{Leaky ReLU} = \\begin{cases} :x :: (x\u0026gt;0) \\\\ 0.01x :: (x\u0026lt;0) \\end{cases} $$\n5. ELU (Exponential Linear Units) Properties :\nalleviate Dying ReLU : To resolve dying ReLU problem, ELU has non zero outputs with negative input, thus the gradient does not become zero.\nmore computation : ELU has exponential function that ReLU doesn\u0026rsquo;t have, thus it costs bit more than ReLU.\n$$ \\text{Leaky ReLU} = \\begin{cases} :x :: (x\u0026gt;0) \\\\ \\alpha(e^x -1) :: (x\u0026lt;0) \\end{cases} $$\n6. Swish (Scaled Exponential Linear Units) Properties :\nUnboundeness : Unlike sigmoid and Tanh functions, Swish is unbounded above which makes it useful near the gradients with values near to zero. This feature avoids saturation as training becomes slow near zero gradient value.\nSmoothness of the curve : plays an important role in generalization and optimization. Unlike ReLU, Swish is a smooth function which makes it less sensitive to initializeing weights and learning rate.\nBounded Below : Like most of the activation functions out there, Swish is also bounded below which helps in strong regularization effects. Like ReLU and softplus, Swish produces negative outputs for small negative inputs due to its non-monotonicity. The non-monotonicity of Swish increases expressivity and importves gradient flow, Which is important considering that many preactivation fall into this range.\n$$ Swish = x * sigmoid(x) = x * (1+e^{-x})^{-1} $$\n6. SELU (Scaled Exponential Linear Units) Properties :\nSimilar to ReLUs, SELUs enable deep neural networks since there is no problem with vanishing gradients.\nIn contrast to ReLUs, SELUs cannot die\nSELUs on their own learn faster and better than other activation functions, even if they are combined with batch normalization\n$$ selu = \\lambda \\begin{cases}x :: (x\u0026gt;0) \\ \\alpha(e^x-1) :: (x\u0026lt;0) \\end{cases} $$\n6. Soft-argmax To get the position where the intensity is maximal in a vector, we usually use an argmax func. But the problem is, this func has no derivative.\n$$ Softmax(x) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\nUsing Sofmax, we get normalized probabilities for each $x_i$, and the expectation of this is the sum of the indices multiplied by their respective probabilities\n$$ \\mathbb{E[x]} = \\sum_i \\frac{e^{x_i}}{\\sum_j e^{x_j}} i $$\nHowever, this mean value is weak if there\u0026rsquo;s multiple modes. To raise the max and lower the others, we can multiply $x$ by an arbitrarily big $\\beta$.\n$$ \\mathbb{E[x]} = \\sum_i \\frac{e^{\\beta x_i}}{\\sum_j e^{\\beta x_j}} i $$\ndef soft_arg_max_khw(A, dim=1): A_softmax = torch.softmax(A, dim=dim).cuda() indices = torch.arange(start=0, end=A.size()[dim]).float().cuda() return torch.matmul(A_softmax, indices).cuda() ","id":42,"section":"Research","summary":"Introduction Activation Function : is a function to induce non-linearity into the output of the neuron for the given input.\nNon-Linearity : Functions that do not satisfy the following linearity.\n$ f(x+y) = f(x)+f(y), \\quad f(\\alpha x) = \\alpha f(x)$ Why Non-Linearity ? : because composites of linear functions are linear again\nif output $f(x)$ of neuron is linear form like $wx + b$,\nEven if the depth of the layer increases, it is just modeling another linear function $f(f(f(.","tags":null,"title":"ML Basic #1 | Activation Functions","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml01-activation-funtions/","year":"2016"},{"content":"\nchmod $ chmod -R [777] FOLDER_NAME conda # show the list of envs $ conda info --envs # Create conda ENV using specific python version $ conda create --name myenv python=3.5 # Create conda ENV using the existing ENV $ conda create --name myclone --clone myenv # Remove activated #!/usr/bin/env $ conda remove --name myenv --all # Backup conda env $ conda env export \u0026gt; [filename].yaml # create conda env from yaml file $ conda env create -f [filename].yaml cron # *(m) *(h) *(d) *(m) *(w) $ crontab -e # run every minute * * * * * /home/script/test.sh # run every 10min 0/10 * * * * # run every day 0 0 * * * curl # -s, --silent $ curl -s URL # -X, --request \u0026lt;command\u0026gt; Specify request command to use $ curl -X GET URL $ curl -X POST URL envsubst changes ${VARNAME} in string apt-get install gettext-base # put /opt/nginx.conf and redirection output to /etc/ngix... $ envsubst \u0026lt; /opt/nginx.conf \u0026gt; /etc/nginx/conf.d/default.conf kill # kill sends signal to process (-15:quit, -9:force quit) $ kill -15 PID $ kill -9 PID # pkill quit process bt name $ pkill -f PROCESS_NAME ls # the number of directories in working directory $ ls -l | grep ^d | wc -l # the number of files in working directory $ ls -l | grep ^- | wc -l # the number of files in working directory $ ls ./ | wc -l # print all the files in dir ls | gawk \u0026quot;BEGIN {\\\u0026quot;pwd\\\u0026quot; | getline cwd} {printf(\\\u0026quot;%s/%s\\n\\\u0026quot;, cwd, \\$0)}\u0026quot; ps # show all processes ps aux | grep PROCESS_NAME # show the list of process, you can find Process ID using command below $ ps -ef | grep python # show the number of processes $ ps -ef | grep -ic palantir screen # Create new session with name $ screen -S name # Create new session $ Ctrl+a+c # Move to next session $ Ctrl+a+a # Move to previous session $ Ctrl+a+n # quit the current screen session $ Ctrl+d # Detach current screen session $ Ctrl+a+d # Detach screen from remote $ screen -d \u0026lt;SCREENID\u0026gt; # connect to specific screen session using ID $ screen -r \u0026lt;SCREENID\u0026gt; # connect to specific screen session using name $ screen -r name # show the list of screen sessions $ screen -ls # kill the screen session using SCREENID $ screen -X -S SCREENID quit ssh # start ssh connection to remote server $ ssh koreanbear@192.168.0.1 # start ssh connection to remote server using private key $ ssh -i ~/Desktop/key.pem koreanbear@192.168.0.1 # copy test.txt from local PC to remote server $ scp ./test.txt koreanbear@192.168.0.1:~/Desktop $ rsync options source destination tar # tar $ tar -cvf \u0026lt;FILENAME.tar\u0026gt; \u0026lt;DIRNAME\u0026gt; # untar $ tar -xvf \u0026lt;FILENAME.tar\u0026gt; # tar.gz $ tar -zcvf \u0026lt;FILENAME.tar.gz\u0026gt; \u0026lt;DIRNAME\u0026gt; # untar.gz $ tar -zxvf \u0026lt;FILENAME.tar.gz\u0026gt; ","id":43,"section":"Engineering","summary":"chmod $ chmod -R [777] FOLDER_NAME conda # show the list of envs $ conda info --envs # Create conda ENV using specific python version $ conda create --name myenv python=3.5 # Create conda ENV using the existing ENV $ conda create --name myclone --clone myenv # Remove activated #!/usr/bin/env $ conda remove --name myenv --all # Backup conda env $ conda env export \u0026gt; [filename].yaml # create conda env from yaml file $ conda env create -f [filename].","tags":null,"title":"CheatSheet | Linux","uri":"https://koreanbear89.github.io/engineering/1.-cheatsheets/cheatsheet-linux/","year":"2016"},{"content":"\nargparse import argparse if __name__ == \u0026quot;__main__\u0026quot;: parser = argparse.ArgumentParser() parser.add_argument(\u0026quot;--name\u0026quot;, type=str, required=True, help=\u0026quot;help\u0026quot;) args = parser.parse_args() print(args.name) counter from collections import Counter Counter(['apple','red','apple','red','red','pear']) \u0026gt;\u0026gt;\u0026gt; Counter({'red': 3, 'apple': 2, 'pear': 1}) datetime from datetime import datetime datetime.today().strftime(\u0026quot;%Y%m%d%H%M%S\u0026quot;) # YYYYmmddHHMMSS 형태의 시간 출력 flask from flask import jsonify, make_response @application.route('/inference', methods=[\u0026quot;GET\u0026quot;]) def infer(): summary = {'class' : 'cat', 'score':'0.92'} # make response data res = make_response(jsonify(summary), 200) # make Response object res.headers.add(\u0026quot;Access-Control-Allow-Origin\u0026quot;, \u0026quot;*\u0026quot;) # CORS ERROR 대응 return res pandas df = pd.read_pickle('PICKLED_PATH') df.drop(i) # remove i-th row df.sort_values(by, ascending=True) # sort # Filtering df.iloc[[0,1,2,3,4,5]] # get rows by indices # not iloc() =\u0026gt; iloc[] df_new = df.loc[df['Column'].str.contains(\u0026quot;sub_str1|sub_str2\u0026quot;, case=False)] # Filtering rows that contain either sub_str1 or sub_str2 # Groupby agg_functions = {'col1':'first', 'col2' : 'sum', 'col3' : lambda col: ' \u0026amp;\u0026amp; '.join(col), } df_new = df.groupby(df['id']).aggregate(agg_functions) pickle import pickle # load or save object using pickle try: with open(path_pkl, 'rb') as f: obj_pkl = pickle.load(f) except: obj_pkl = [] with open(path_pkl, 'wb') as f: pickle.dump(obj_pkl, f) requests import requests # GET url = 'http://localhost/test' params = {'arg1':'1', 'arg2':'2'} response = requests.get(url=url, params=params).json() # POST response = requests.post(url=url, data=json.dumps(params)) os os.chdir('/opt/vidClassifier/classifier/') # sys.path.append('/opt/vidClassifier/classifier') ","id":44,"section":"Engineering","summary":"argparse import argparse if __name__ == \u0026quot;__main__\u0026quot;: parser = argparse.ArgumentParser() parser.add_argument(\u0026quot;--name\u0026quot;, type=str, required=True, help=\u0026quot;help\u0026quot;) args = parser.parse_args() print(args.name) counter from collections import Counter Counter(['apple','red','apple','red','red','pear']) \u0026gt;\u0026gt;\u0026gt; Counter({'red': 3, 'apple': 2, 'pear': 1}) datetime from datetime import datetime datetime.today().strftime(\u0026quot;%Y%m%d%H%M%S\u0026quot;) # YYYYmmddHHMMSS 형태의 시간 출력 flask from flask import jsonify, make_response @application.route('/inference', methods=[\u0026quot;GET\u0026quot;]) def infer(): summary = {'class' : 'cat', 'score':'0.92'} # make response data res = make_response(jsonify(summary), 200)","tags":null,"title":"Python #2 | Built In Modules","uri":"https://koreanbear89.github.io/engineering/2.-languages/python-2-built-in-modules/","year":"2016"},{"content":"\n1. Matrix $$ \\begin{pmatrix}1 \u0026amp; 2 \u0026amp; 3 \\4 \u0026amp; 5 \u0026amp; 6 \\7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\begin{bmatrix}a \u0026amp; b \u0026amp; c \\d \u0026amp; e \u0026amp; f \\g \u0026amp; h \u0026amp; i \\end{bmatrix} \\begin{pmatrix} a_{1,1} \u0026amp; a_{1,2} \u0026amp; \\cdots \u0026amp; a_{1,n} \\ a_{2,1} \u0026amp; a_{2,2} \u0026amp; \\cdots \u0026amp; a_{2,n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{m,1} \u0026amp; a_{m,2} \u0026amp; \\cdots \u0026amp; a_{m,n} \\end{pmatrix} $$\n\\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 4 \u0026amp; 5 \u0026amp; 6 \\\\ 7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\begin{bmatrix} a \u0026amp; b \u0026amp; c \\\\ d \u0026amp; e \u0026amp; f \\\\ g \u0026amp; h \u0026amp; i \\end{bmatrix} \\begin{pmatrix} a_{1,1} \u0026amp; a_{1,2} \u0026amp; \\cdots \u0026amp; a_{1,n} \\\\ a_{2,1} \u0026amp; a_{2,2} \u0026amp; \\cdots \u0026amp; a_{2,n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m,1} \u0026amp; a_{m,2} \u0026amp; \\cdots \u0026amp; a_{m,n} \\end{pmatrix} ","id":45,"section":"Engineering","summary":"1. Matrix $$ \\begin{pmatrix}1 \u0026amp; 2 \u0026amp; 3 \\4 \u0026amp; 5 \u0026amp; 6 \\7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix} \\begin{bmatrix}a \u0026amp; b \u0026amp; c \\d \u0026amp; e \u0026amp; f \\g \u0026amp; h \u0026amp; i \\end{bmatrix} \\begin{pmatrix} a_{1,1} \u0026amp; a_{1,2} \u0026amp; \\cdots \u0026amp; a_{1,n} \\ a_{2,1} \u0026amp; a_{2,2} \u0026amp; \\cdots \u0026amp; a_{2,n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{m,1} \u0026amp; a_{m,2} \u0026amp; \\cdots \u0026amp; a_{m,n} \\end{pmatrix} $$","tags":null,"title":"LaTeX | Snippet","uri":"https://koreanbear89.github.io/engineering/2.-languages/latex-snippet/","year":"2016"},{"content":"\nIntroduction Basic Python Syntax 1. Data Types 1.1 String Limiting floats to N demicmal points\nround(1.23456, 4)\n\u0026quot;{:.2f} / {:.3f}\u0026quot;.format(1.2345, 1.2345)\nf\u0026quot;{num:.2f}\u0026quot;\n2. Class Class : A set of related variables and methods as a blueprint for creating an object\nObject : declared as a class type,\nInstance : when the object is allocated in memory and is actually used\n2.1 init, call class A: def __init__(self): # 객체를 생성할때 사용 print('init') def __call__(self): # 마치 함수를 호출하는 것처럼 인스턴스를 호출할 수 있도록 만듦 print('call') \u0026gt;\u0026gt;\u0026gt; a = A() # init \u0026gt;\u0026gt;\u0026gt; a() # call 3. Method Instance Method : The most common method type. Able to access data and properties unique to each instance.\nStatic Method : Cannot access anything else in the class. Totally self-contained code.\ncan be defined with decorator @staticmethod can be defined without self in argument can call without generate instance Class Method: Can access limited methods in the class. Can modify class specific details.\nclass Calc: #instance method def add(self, a, b): return a+b @staticmethod def add(a, b): # static method does not need self in arg print(a + b) # We must create instance before we call instance method calc = Calc() calc.add(10,20) # We can call static method https://www.makeuseof.com/tag/python-instance-static-class-methods/without create instance Calc.add(10, 20) Reference Instance vs. Static vs. Class Methods in Python: The Important Differences 4. Lambda Lambda : is a disposable function that is used and discarded.\nInstead of defining a simple function like a general function and using it, use it immediately and throw it away when needed.\nOften used with map(func, iterable) and filter(func, iterable).\nmap returns the result of applying all func to iterable,\nfilter returns only elements that satisfy func among iterable\n\u0026gt;\u0026gt; g = lambda x:x**2 \u0026gt;\u0026gt; g(8) # 64 # with map \u0026amp; filter \u0026gt;\u0026gt;\u0026gt; list(map(lambda x:x+3, [1,2,3,4])) # [4, 5, 6, 7] \u0026gt;\u0026gt;\u0026gt; list(filter(lambda x:x\u0026gt;0, range(-5,5))) #[1, 2, 3, 4] 5. Decorator def time_check(func): def new_func(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print('Elapsed:', end_time-start_time) return result return new_func ##### without decorator ##### def big_number(n): return n**n**n new_func = time_checker(big_number) new_func(6) ##### with decorator ##### @time_check def big_number(n): return n**n**n big_number(6) ","id":46,"section":"Engineering","summary":"Introduction Basic Python Syntax 1. Data Types 1.1 String Limiting floats to N demicmal points round(1.23456, 4) \u0026quot;{:.2f} / {:.3f}\u0026quot;.format(1.2345, 1.2345) f\u0026quot;{num:.2f}\u0026quot; 2. Class Class : A set of related variables and methods as a blueprint for creating an object Object : declared as a class type, Instance : when the object is allocated in memory and is actually used 2.1 init, call class A: def __init__(self): # 객체","tags":null,"title":"Python #1 | Basic","uri":"https://koreanbear89.github.io/engineering/2.-languages/python-1-basic/","year":"2016"},{"content":"title: \u0026quot;Cheat Sheet | Mac OS Setup\u0026quot; date: 2022-05-22 09:00:13 categories: [2. Linux, Favorites] Introduction 어플리케이션 별 단축키 설정\n시스템 환경설정 \u0026gt; 키보드 \u0026gt; 단축키 \u0026gt; 앱단축키 Automount 해제 diskutil info -all 로 Volume UUID 와 type 확인\nsudo vi /etc/fstab 맨 아래에 아래와 같이 추가\nUUID=0D4EE102-A8C3-31A6-A0BD-C82A702697A2 none apfs rw,noauto UUID=BEC5D911-9473-4C3A-97C7-E68999C9ACFD none apfs rw,noauto ","id":47,"section":"Engineering","summary":"title: \u0026quot;Cheat Sheet | Mac OS Setup\u0026quot; date: 2022-05-22 09:00:13 categories: [2. Linux, Favorites] Introduction 어플리케이션 별 단축키 설정 시스템 환경설정 \u0026gt; 키보드 \u0026gt; 단축키 \u0026gt; 앱단축키 Automount 해제 diskutil info -all 로 Volume UUID 와 type 확인 sudo vi /etc/fstab 맨 아래에 아래와","tags":null,"title":"","uri":"https://koreanbear89.github.io/engineering/9.-others/set-up-mac/","year":"0001"},{"content":"title: \u0026quot;MLCV #13 | Multimodal Representation\u0026quot; date: 2021-12-07 09:00:13 categories: [2. Machine Learning] Learing Transferable Visual Models, CLIP (Contrastive Language Image Pretraining Introduction Traditional CV-DL models are trained to predict a fixed set of pre-determined object categories =\u0026gt; limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. Methods Extract feature representations of each modality (image, text) Joint multi-modal embedding Scaled pair-wise cosine similarities Symmetric loss function Conclusion We have investigated whether it is possible to transfer the success of task-agnostic web-scale pre-training in NLP to another domain. Reference OpenAI CLIP: ConnectingText and Images (Paper Explained) - YouTube , [14:40~] CLIP 은 pretrain 방법의 하나로 NLP 분야에서 BERT 등이 자연어로 pretrain 하여 성능을 대폭 향상시킨 방식으로부터 insight를 얻음 이미지+자연어 데이터셋으로 특정 이미지의 feature $I_f$ 와 대응되는 텍스트의 feature $T_f$ 의 inner product $I_f \\cdot T_f$ 가 최대가 되도록 미리 학습해둔뒤 Inference 할 떄는 추가적인 fine-tuning 없이 test-set의 label들 중에 Inner product가 최대가 되는 label을 선택 # extract feature representations of each modality I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # Joint multimodal embedding [n, d_e] # 아래의 inner-product가 최대가 되도록 W_i, W_t를 학습 I_e = np.dot(I_f, W_i) T_e = np.dot(T_f, W_t) # Scaled pairwise cosine similarities [n,n] logits = np.dot(I_e, T_e.T) # Symmetric loss function loss_i = cross_entropy(Logits, labels, axis=0) loss_t = cross_entropy(logits, labels, axis=1) A Local-to-Global Approach to Multi-modal Movie Scene Segmentation Introduction : Recognizing the movie scenes, including the detection of scene boundaries and the understanding of the scene content, facilitates a wide-range of movie understanding tasks such as scene classification, cross movie scene retrieval, human interaction graph and human-centric storyline construction\nTerminologies :\nshots : captured by a camera that operates for an uninterrupted period of time and thus is visually continuous super-shots : collection of shots, roghly segmented from local (adjacent) features scene : comprises a sequence of shots to present a semantically coherent part of the story a plot-based semantic unit, where a certain activ-ity takes place among a certain group of character often happens in a fixed place, it is also possi-ble that a scene traverses between multiple places continually Methods: solve a binary classification problem to determine whether a shot boundary is a scene boundary, by designing a three-level model to incorporate different levels of contextual information based on the shot representation\nShot Representation, $s_i$ place : ResNet50 pretrained on \u0026ldquo;Places\u0026rdquo; dataset cast : Faster-RCNN pretrained on CIM dataset to detect cast instances and ResNet50 pretrained on PIPA dataset to extract cast features action : TSN pretrained on AVA dataset to get action features audio : NaverNet pretrained on AVA-ActiveSpeaker Dataset to separate speech and background sound and stft Shot Boundary Representation : propose a Boundary Network to model the shot boundary, $b_i = \\Beta(S_{i-w_b}, \u0026hellip; S_{i+w_b})$ BNet takes a clip of the movie with $2w_b$ shots as input and outputs a boundary representation $b_i$ BNet consists of two branches, namely $B_d$ and $B_r$, that calculate differences and relations betweent adjacent shots Coarse Prediction at Segment Level : $\\Tau([b_1, \u0026hellip;, b_{n-1}]) = [p_1, \u0026hellip;, p_{n-1}] $ Bi-LSTM predicts a sequence of coarse score $[p_i,\u0026hellip;]$ from sequence of representatives $[b_i,\u0026hellip;]$ with a threshold, we get roughly classified super-shot boundaries $[\\hat{o_i},\u0026hellip;]$ Global Optimal Grouping : $ \\Gamma([\\hat{o_1},\u0026hellip;,\\hat{o_i}]) = [o_1,\u0026hellip;,o_i] $ rmfdabove local segmentation gives us an initial rough scene cut set. Our goal is to merge these super-shots into scenes Conclusion : this framework is very effective and achieves much better performance than existing methods\nMultimodal Transformer for Unaligned Multimodal Language Sequences (2019) Introduction : two major challenges in modeling multimodal human language time-series data\n(1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; (2) long-range dependencies between elements across modalities. Methods : Multimodal Transformer (MulT) for modeling unaligned multimodal language sequences.\nOverall Architecture\nTemporal Convolutions $\\hat{X}$ : pass the input seq through 1D Conv layer, project the features of different modalities to the same dimension $d$ Positional Embeddings $Z$ : enable the sequences to carry temporal information, add PE to $\\hat{X}$ Crossmodal Transformers : enables one modality for receiving information from another modality, based on the cross-modal attention blocks Crossmodal Attention : consider two modalities $\\alpha, \\beta$, calculate attention score using Query from $\\alpha$ and Key from $\\beta$\n$$ \\text{softmax}(\\frac{Q_\\alpha K^T_\\beta}{\\sqrt{d_k}}) V_\\beta $$\nResult : show that MulT exhibits the best performance when compared to prior methods (Multimodal sentiment analysis, CMU-MOSI \u0026amp; MOSEI)\nSelf-Supervised MultiModal Versatile Network (2020, Deepmind) Introduction : learn representations using self-supervision by leveraging three modalities naturally present in videos: visual, audio and language streams Methods : multimodal versatile network Input : unlabelled videos containing different modalities : RGB stream, audio track, linguistic narrations (Automatic Speech Recognition) MultiModal Versatile Networks : Shared space : all modalities are embedded into a single shared vector space $S_{vat}⊂R^{d_s}$ Disjoint spaces : have different visual-audio $S_{va}$ and visual-text $S_{vt}$ spaces Fine and coarse spaces (FAC) : visual and the audio (fine-grained) domains are different from the language domain (semantically coarse-grained) in terms of their granularities fine-grained : there are many visual or sounds of guitars that might be really different to each other coarse-grained : while the textual domain is more coarse as its goal is to abstract away details (e.g. a single “guitar” word) vision and audio are compared in the fine-grained space ($S_{va}$), while text is compared with vision and audio in the lower dimensional coarse-grained space ($S_{vat}$). since the text modality is directly obtained from the audio track using ASR, we do not construct the audio-text space nor the loss that puts them in alignment explicitly Multimodal Contrastive Loss : we construct self-supervised tasks which aim to align pairs of modalities. positive training pairs across two modalities are constructed by sampling the two streams from the same location of a video. negative training pairs are created by sampling streams from different videos. Video to image network deflation Results : exceeds the state-of-the-art for action and audio classification on five challenging benchmarks: HMDB51, UCF101, Kinetics600, ESC-50 and AudioSet VATT : Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text (2021) introduction : present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures\nMethods :\n(1) Tokenization and Positional Encoding : define a modality-specific tokenization layer\nvideo : 3D extension of the patching machanism, linear projection on the entire voxels. audio : the raw audio waveform is a 1D input with length $T\u0026rsquo;$, and we partition it to $[T\u0026rsquo;/t\u0026rsquo;]$ segments each containing $t\u0026rsquo;$ waveform amplitudes and then apply a linear projection with a learnable weight $W\\in\\mathbb{R^{t\u0026rsquo; \\times d}}$ to get a $d$ dimensional vector representation. text : construct a vocabulary of size $v$ and map each word to a $v$-dimensional one-hot vetor followed by a linear projection with a learnable weight $W \\in \\mathbb{R}^{v \\times d}$ Drop Token : a simple and yet effective strategy to reduce the computational complexity during training. The Transformer Architecture : we do not tweak the architecture\nCommon Space Projection\nVideo-audio : maps the video and audio Transformers’ outputs to the video-audio common space $S_{va}$ using Linear Projection video-text : the text Transformer’s outputs and the video embedding in the $S_{va}$ space to video-text common space $S_{vt}$ using Linear Projection Multimodal Contrastive Learning\nvideo-audio : Noise Contrastive Estimaiton to align video-audio pairs.\nvideo-text : Multiple Instance Learning NCE to align to video-text pairs.\noverall per-sample objective : for training the entire model end-to-end is as follows:\n$$ L=NCE(z_{v,va},z_{a,va}) +λ MILNCE(z_{v,vt},[z_{t,vt}]),\n$$\nResults :\nsuggests that Transformers are effective for learning semantic video/audio/text representations even if one model is shared across modalities and multi-modal self-supervised pre-training is promising for reducing their dependency on large-scale labeleddata. Video Action recognition (UCF101, HMDB, Kinetics) Audio Event Classification (ESC50, AudioSet) text to video retrieval (MSR-VTT) CLIPBERT for Video-and-Language Learning via Sparse Sampling Introduction CLIPBERT : enables end-to-end learning for video-and-language tasks, by employing sparse sampling ","id":48,"section":"Research","summary":"title: \u0026quot;MLCV #13 | Multimodal Representation\u0026quot; date: 2021-12-07 09:00:13 categories: [2. Machine Learning] Learing Transferable Visual Models, CLIP (Contrastive Language Image Pretraining Introduction Traditional CV-DL models are trained to predict a fixed set of pre-determined object categories =\u0026gt; limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much","tags":null,"title":"","uri":"https://koreanbear89.github.io/research/2.-machine-learning/ml11-multimodal-representation/","year":"0001"}],"tags":[]}