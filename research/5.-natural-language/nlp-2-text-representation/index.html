<!DOCTYPE html>
<html lang="en">
  <head>
    <title>
        NLP #2 | TextRepresentation - Lab.Koreanbear|한국곰연구소
      </title>
        <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
    <meta name="renderer" content="webkit">
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    
    <meta name="theme-color" content="#000000" />
    
    <meta http-equiv="window-target" content="_top" />
    
    
    <meta name="description" content="Summary Text Representation (Embedding) : When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to &amp;ldquo;vectorize&amp;rdquo; the text) before feeding it to the model. Methods Sparse Representation : One-hot encoding, Document Term Matrix, etc. Dense Representation : Word2Vec, Glove, FastText, etc. Pretraind Word Embedding : ELMo, GPT, BERT 1. Sparse Representations Introduction : Sparse Representation embeds word" />
    <meta name="generator" content="Hugo 0.101.0 with theme pure" />
    <title>NLP #2 | TextRepresentation - Lab.Koreanbear|한국곰연구소</title>
    
    
    <link rel="stylesheet" href="https://koreanbear89.github.io/css/style.min.be627ecd35738958a04c60fe5c31d5410949a5e53a29e404dda965a1eb8160c8.css">
    
    <link rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/9.15.10/styles/github.min.css" async>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css" async>
    <meta property="og:title" content="NLP #2 | TextRepresentation" />
<meta property="og:description" content="Summary Text Representation (Embedding) : When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to &ldquo;vectorize&rdquo; the text) before feeding it to the model. Methods Sparse Representation : One-hot encoding, Document Term Matrix, etc. Dense Representation : Word2Vec, Glove, FastText, etc. Pretraind Word Embedding : ELMo, GPT, BERT 1. Sparse Representations Introduction : Sparse Representation embeds word" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://koreanbear89.github.io/research/5.-natural-language/nlp-2-text-representation/" /><meta property="article:section" content="Research" />
<meta property="article:published_time" content="2021-01-13T09:00:00+00:00" />
<meta property="article:modified_time" content="2021-01-13T09:00:00+00:00" />

<meta itemprop="name" content="NLP #2 | TextRepresentation">
<meta itemprop="description" content="Summary Text Representation (Embedding) : When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to &ldquo;vectorize&rdquo; the text) before feeding it to the model. Methods Sparse Representation : One-hot encoding, Document Term Matrix, etc. Dense Representation : Word2Vec, Glove, FastText, etc. Pretraind Word Embedding : ELMo, GPT, BERT 1. Sparse Representations Introduction : Sparse Representation embeds word"><meta itemprop="datePublished" content="2021-01-13T09:00:00+00:00" />
<meta itemprop="dateModified" content="2021-01-13T09:00:00+00:00" />
<meta itemprop="wordCount" content="1067">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="NLP #2 | TextRepresentation"/>
<meta name="twitter:description" content="Summary Text Representation (Embedding) : When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to &ldquo;vectorize&rdquo; the text) before feeding it to the model. Methods Sparse Representation : One-hot encoding, Document Term Matrix, etc. Dense Representation : Word2Vec, Glove, FastText, etc. Pretraind Word Embedding : ELMo, GPT, BERT 1. Sparse Representations Introduction : Sparse Representation embeds word"/>

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    
    
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" rel="stylesheet">

    
    <!--[if lte IE 9]>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
      <![endif]-->

    <!--[if lt IE 9]>
        <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
      <![endif]-->



  </head>

  
  

  <body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader">
    <div class="slimContent">
      <div class="navbar-header">
        <div class="profile-block text-center">
          
           
          
          <a id="avatar" href="/" target="_self">
            <img class=" img-rotate" src="https://koreanbear89.github.io/fa-igloo.png" width="200" height="200">
          </a>
          <a href="/" target="_self">
            <h2 id="name" class="hidden-xs hidden-sm">Lab.Koreanbear</h2>
          </a>
          
          <h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3>
          <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i>Seoul, Korea</small>
        </div><div class="search" id="search-form-wrap">
    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i
                        class="icon icon-search"></i></button>
            </span>
        </div>
        <div class="ins-search">
            <div class="ins-search-mask"></div>
            <div class="ins-search-container">
                <div class="ins-input-wrapper">
                    <input type="text" class="ins-search-input" placeholder="Type something..."
                        x-webkit-speech />
                    <button type="button" class="close ins-close ins-selectable" data-dismiss="modal"
                        aria-label="Close"><span aria-hidden="true">×</span></button>
                </div>
                <div class="ins-section-wrapper">
                    <div class="ins-section-container"></div>
                </div>
            </div>
        </div>
    </form>
</div>
        <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>


      <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
        <ul class="nav navbar-nav main-nav">
            <li class="menu-item menu-item-home">
                <a href="/">

                    
                    

                    
                    <i class=" fas fa-home"></i>
                  <span class="menu-title">Home</span>
                </a>
            </li>
            <li class="menu-item menu-item-about">
                <a href="/about/">

                    
                    

                    
                    <i class=" fas fa-user-alt"></i>
                  <span class="menu-title">About</span>
                </a>
            </li>
            <li class="menu-item menu-item-mathematics">
                <a href="/mathematics/">

                    
                    

                    
                    <i class=" fas fa-square-root-alt"></i>
                  <span class="menu-title">Mathematics</span>
                </a>
            </li>
            <li class="menu-item menu-item-research">
                <a href="/research/">

                    
                    

                    
                    <i class=" fas fa-book-open"></i>
                  <span class="menu-title">Research</span>
                </a>
            </li>
            <li class="menu-item menu-item-engineering">
                <a href="/engineering/">

                    
                    

                    
                    <i class=" fas fa-cog"></i>
                  <span class="menu-title">Engineering</span>
                </a>
            </li>
        </ul>
      </nav>
    </div>
  </header>

    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    
    <div class="widget-body">



        <ul style="margin-bottom:25px;"  class="category-list"><h5>Mathematics</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-linear-algebra">1. Linear Algebra</a>
                  <span class="category-list-count">6</span></li>
                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-statistics">2. Statistics</a>
                  <span class="category-list-count">11</span></li>
                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-mathematics-for-ml">3. Mathematics for ML</a>
                  <span class="category-list-count">12</span></li>
                  
              
        </ul><hr>



        <ul style="margin-bottom:25px;"  class="category-list"><h5>Research</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-computer-science">1. Computer Science</a>
                  <span class="category-list-count">12</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-machine-learning">2. Machine Learning</a>
                  <span class="category-list-count">12</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-computer-vision">3. Computer Vision</a>
                  <span class="category-list-count">13</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/4.-image-processing">4. Image Processing</a>
                  <span class="category-list-count">4</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/5.-natural-language">5. Natural Language</a>
                  <span class="category-list-count">8</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/6.-recommendation-system">6. Recommendation System</a>
                  <span class="category-list-count">2</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/7.-vision-language">7. Vision Language</a>
                  <span class="category-list-count">1</span></li>

                  
              
        </ul><hr>



        <ul style="margin-bottom:25px;"  class="category-list"><h5>Engineering</h5>
              
              
              
                  
                  
                  <li class="category-list-item"><a href="/categories/1.-cheatsheets">1. CheatSheets</a>
                  <span class="category-list-count">15</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/2.-languages">2. Languages</a>
                  <span class="category-list-count">13</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/3.-system-design">3. System Design</a>
                  <span class="category-list-count">5</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/9.-others">9. Others</a>
                  <span class="category-list-count">9</span></li>

                  
              
                  
                  
                  <li class="category-list-item"><a href="/categories/9.-study">9. Study</a>
                  <span class="category-list-count">20</span></li>

                  
              
        </ul><hr>









    </div>
</div>

  </div>
</aside>

    
    


  
  <div class="sidebar-toc-all">
  <aside class="sidebar sidebar-toc show" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  
    <div class="slimContent">
      <h4 class="toc-title">Contents</h4>
      <nav id="toc" class="js-toc toc" >
      </nav>
    </div>
  </aside>
</div>

<main class="main" role="main"><div class="content">
  <article id="-" class="article article-type-" itemscope
    itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      <h1 itemprop="name">
  <a
    class="article-title"
    href="/research/5.-natural-language/nlp-2-text-representation/"
    >NLP #2 | TextRepresentation</a
  >
</h1>

      <div class="article-meta">
        
<span class="article-date">
  <i class="icon icon-calendar-check"></i>&nbsp;
<a href="https://koreanbear89.github.io/research/5.-natural-language/nlp-2-text-representation/" class="article-date">
  <time datetime="2021-01-13 09:00:00 &#43;0000 UTC" itemprop="datePublished">2021-01-13</time>
</a>
</span>
<span class="article-category">
  <i class="icon icon-folder"></i>&nbsp;
  <a href="/categories/5.-natural-language/"> 5. Natural Language </a>
</span>


        <span class="post-comment"><i class="icon icon-comment"></i>&nbsp;<a href="/research/5.-natural-language/nlp-2-text-representation/#comments"
            class="article-comment-link">Comments</a></span>
      </div>
    </div>
    <div class="article-entry marked-body js-toc-content" itemprop="articleBody">
      <br>
<h2 id="summary">Summary</h2>
<ul>
<li>
<p>Text Representation (Embedding) : When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to &ldquo;vectorize&rdquo; the text) before feeding it to the model.</p>
</li>
<li>
<p>Methods</p>
<ul>
<li>
<p>Sparse Representation : One-hot encoding, Document Term Matrix, etc.</p>
</li>
<li>
<p>Dense Representation : Word2Vec, Glove, FastText, etc.</p>
</li>
<li>
<p>Pretraind Word Embedding : ELMo, GPT, BERT</p>
</li>
</ul>
</li>
</ul>
<h3 id="heading"></h3>
<hr>
<h2 id="1-sparse-representations">1. Sparse Representations</h2>
<ul>
<li>
<p>Introduction : Sparse Representation embeds word as a vector which have a relatively small number of nonzero elements. (most of elements in vectors are zero)</p>
</li>
<li>
<p>Models</p>
<ul>
<li>
<p><strong>Model 1) One-hot Encoding</strong> : a 1 × <em>N</em> matrix (vector) used to distinguish each word in a vocabulary from every other word in the vocabulary.</p>
</li>
<li>
<p><strong>Model 2) Bag of Words (BoW)</strong> : In this model, a text (such as a sentence or a document) is represented as the bag(multiset) of its words, disregarding grammar and even word order but keeping multiplicity.</p>
<ol>
<li>
<p>Give each word a unique integer index first.</p>
</li>
<li>
<p>Create a vector recording the number of occurrences of word tokens in each index.</p>
</li>
</ol>
</li>
<li>
<p><strong>Model 3) Document-Term Matrix</strong> : BoW for set of multiple sentences (documents)</p>
</li>
<li>
<p><strong>Model 4) TF-IDF</strong> : give more score to the terms (token) that occur frequently in this document but not in the others</p>
<ul>
<li>
<p>$TF(d,t)$ : The number of occurrences of a specific term $t$ in a particular document $d$.</p>
</li>
<li>
<p>$DF(t)$ :  The number of documents that a specific term $t$ appeared.</p>
</li>
<li>
<p>$IDF(t)$ : Inverse $DF(t) = log(\frac{n}{1+DF(t)})$ , where $n$ is the number of sentences in corpus</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Limitations</p>
<ul>
<li>
<p>There is no notion of similarity between words =&gt; Word2Vec</p>
</li>
<li>
<p>The dimension of the embedded vector is too high.</p>
</li>
</ul>
</li>
</ul>
<pre><code class="language-python">corpus = [
    &quot;John likes to watch movies. Mary likes movies too.&quot;, # doc1
    &quot;Mary also likes to watch football games.&quot; # doc2
    ]

one_hot_John = [1,0,0,0,0,0,0,0,0,0,0] # {&quot;John&quot;,&quot;likes&quot;,&quot;to&quot;,&quot;watch&quot;,&quot;movies&quot;,&quot;Mary&quot;,&quot;too&quot;,&quot;also&quot;,&quot;football&quot;,&quot;games&quot;}
BoW_sentence1 = {&quot;John&quot;:1,&quot;likes&quot;:2,&quot;to&quot;:1,&quot;watch&quot;:1,&quot;movies&quot;:2,&quot;Mary&quot;:1,&quot;too&quot;:1}
DTM = [[1,2,1,1,2,1,1,0,0,0],[0,1,1,1,0,1,0,1,1,1]]

# Create DTM with sklearn.CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer
count_vectorizer = CountVectorizer().fit(corpus)
DTM = count_vectorizer.transform([corpus[0]])
print(DTM.toarray()) # [[1,2,1,1,2,1,1,0,0,0]]

# Create TF-IDF with sklearn TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer().fit(corpus)
tfidf = tfidf_vectorizer.transform([corpus[0]])
print(tfidf.toarray()) # [0.32 0.46 0.23 0.23 0.64 0.23 0.32 0.00 0.00 0.00]
</code></pre>
<br>
<!--

- **DTM** (Document-Term Matrix) : 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것을 말합니다. 쉽게 생각하면 각 문서에 대한 BoW를 하나의 행렬로 만든 것

- **TF-IDF** (Term Frequency Inverse Doc Frequency) : 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법입니다. 사용 방법은 우선 DTM을 만든 후, 각 element에 IDF를 곱해 가중치를 부여합니다

  1. TF : 특정 문서에서 특정단어의 등장횟수로 DTM과 동일

  2. IDF : '특정 단어가 등장한 문서의 수 $(t)$ ' 의 역수, 다만 총 문서의 수 $(n)$ 가 커질수록 IDF가 기하급수로 커지기에 log를 취해줌 , 각 문서에서 몇번씩 등장했는지는 관심 없고 

     ​    $$IDF = log(\frac{n}{1+df(t)})$$

- **Class-based TF-IDF**

  - The goal of the class-based TF-IDF is to supply all documents within a single class with the same **class vector**
  - If documents are not individuals but part of a larger collective, then it might be interesting to actually regard them as such by joining all docs in a class together

```python
  doc1 = "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다."
  doc2 = "소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다."
  doc3 = "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다."

  # (1) 각 단어에 고유한 정수 인덱스를 부여하면
  ('정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17)  

  # (2) 각 인덱스의 위치에 단어 토큰의 등장 횟루를 기록
  BoW1 >> [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]  
  BoW2 >> [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1]  
  BoW3 >> [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]  

  # (3) DTM
  DTM >> [[1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],  
          [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1],  
          [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1] ]
```

    ~~~python
    from sklearn.feature_extraction.text import CountVectorizer # Convert a collection of text documents to a matrix of token counts

    vectorizer = CountVectorizer(
        min_df=0.1, # 10% 아래의 문서에서 출현한 단어 제외
        max_df=0.9, # 90% 이상의 문서에서 출현한 단어 제외
        ngram_range=(1,1),
        lowercase=True,
        tokenizer=lambda x:x.split())

    corpus.iter_sent=False
    X = vectorizer.fit_transform(corpus)
    print(X.shape) 
    ~~~

### 

-->
<hr>
<h2 id="2-word2vec--efficient-estimation-of-word-representations-in-vector-space-2013httpsarxivorgabs13013781">2. <a href="https://arxiv.org/abs/1301.3781">Word2Vec : Efficient Estimation of Word Representations in Vector Space (2013)</a></h2>
<ul>
<li>
<p>Introduction :</p>
<ul>
<li>
<p>Current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary</p>
</li>
<li>
<p>The most successful concept is to use distributed representations of word</p>
</li>
<li>
<p>With the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity</p>
<p>$$
vector(\text{King}) - vector(\text{Man}) + vector(\text{Woman}) = vector(\text{Queen})
$$</p>
</li>
</ul>
</li>
<li>
<p>Methods :</p>
<ul>
<li>
<p>Continuous Bag of Words Model : predicting the current word based on the context</p>
<ul>
<li>
<p>(1) Put context words into a simple neural net in the form of a one-hot vector.</p>
</li>
<li>
<p>(2) Train the model to predict what the central word is.</p>
</li>
</ul>
</li>
<li>
<p>Continuous Skip-Gram Model : maximize classification of a word based on another word in the same sentence</p>
<ul>
<li>
<p>(1) Use each current word as an input to a classifier</p>
</li>
<li>
<p>(2) Predict words within a certain range before and after the current word.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<center>
<img width=40% src='https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG'>
<img width=40% src='https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG'>
<p> CBOW(T) , Skip-Gram (B) </p>
</center>
<br>
<hr>
<h2 id="3-elmo--deep-contextualized-word-representations-2018httpsarxivorgabs180205365">3. <a href="https://arxiv.org/abs/1802.05365">ELMo : Deep contextualized word representations (2018)</a></h2>
<ul>
<li>
<p><strong>Introduction</strong> :</p>
<ul>
<li>
<p>Existing embedding models consider the surrounding context only in training step.</p>
</li>
<li>
<p>That is, after training step is finished, the word and embedding vector match 1:1 and do not change.</p>
</li>
<li>
<p>Therefore, although the bank of &ldquo;River Bank&rdquo; and &ldquo;Bank Account&rdquo; have completely different meanings, they are embedded to the same vector (polysemy).</p>
</li>
</ul>
</li>
<li>
<p><strong>Method</strong>: ELMo (Embeddings from Language Model)</p>
<ol>
<li>
<p>Contextualized Word Embedding : ELMo reflect context not only during the training process but also when making predictions (embedding).</p>
</li>
<li>
<p>Bidirectional-LSTM : A final embedding vector considering context is created by forward-LSTM and backward LSTM</p>
<ul>
<li>
<p>Forward LSTM : predicts (n+1)th word by looking at n words</p>
</li>
<li>
<p>Backward LSTM : predicts the (n-1)th word by looking at n words.</p>
</li>
</ul>
</li>
</ol>
</li>
<li>
<p><strong>Conclusion</strong> : biLM layers efficiently encode different types of syntactic and semantic information about words in-context</p>
</li>
</ul>
<br>
<hr>
<h2 id="4-bert--pre-training-of-deep-bidirectional-transformers-for-language-understandion-google-2018httpsarxivorgabs181004805">4. <a href="https://arxiv.org/abs/1810.04805">BERT : Pre-training of Deep Bidirectional Transformers for Language Understandion (Google, 2018)</a></h2>
<ul>
<li>
<p><strong>Introduction</strong> : introduce a new language representation model called BERT, that is designed to pretrain deep bi-directional  representations from unlabeled text by jointly conditioning on both left and right context in all layers.</p>
</li>
<li>
<p><strong>Method</strong>:</p>
<ol>
<li>
<p>Input Representation = Token Embeddings + Segment Embeddings + Position Embeddings</p>
<ul>
<li>
<p>Token Embeddings : [CLS] token aggregate sequence representation for classification task and [SEP] token simply diffenciate the sentences</p>
</li>
<li>
<p>Segment Embeddings : details in figure2</p>
</li>
<li>
<p>Positional Embeddings : details in transformer (Attention is all you need)</p>
</li>
</ul>
</li>
<li>
<p>Architecture : BERT’s model architecture is a multi-layer bidirectional Transformer encoder (Only encoder part)</p>
</li>
<li>
<p>Pretrain :  pre-train BERT using two unsupervised tasks</p>
<ul>
<li>
<p>Task #1  Masked LM : simply mask some percentage(15%) of the input tokens at random and then predict those masked tokens</p>
</li>
<li>
<p>Task #2  Next Sentence Prediction : train a model that understands sentence relationships, by a binarized next sentence prediction task.</p>
</li>
</ul>
</li>
<li>
<p>Finetune :  For each sub-task, simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end</p>
</li>
</ol>
</li>
<li>
<p><strong>Conclusion</strong> : generalizing these findings (unsupervised pre-training can improve down-stream NLP tasks) to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.</p>
</li>
</ul>
<br>
<hr>
<h2 id="5-multilingual-sentencebert--making-monolingual-sentence-embeddings-multilingual-using-knowledge-distillationhttpsarxivorgabs200409813">5. <a href="https://arxiv.org/abs/2004.09813">Multilingual SentenceBert : Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation</a></h2>
<ul>
<li>
<p><strong>Introduction</strong> :</p>
<ul>
<li>
<p>We present an easy and efficient method to extend existing sentence embedding models to new languages</p>
</li>
<li>
<p>based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence</p>
</li>
</ul>
</li>
<li>
<p><strong>Method</strong> :</p>
<ul>
<li>
<p>Multilingual knowledge distillation : use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model.</p>
</li>
<li>
<p>Training : With $s_i$, a sentence in one of the source languages and $t_i$ a sentence in one of the target languages, we minimize the MSE</p>
<p>$$ \frac{1}{\beta} \sum [ (M(s_j)-M&rsquo;(s_j))^2 + (M(s_j)-M&rsquo;(t_j))^2 ] $$</p>
</li>
<li>
<p>Architecture : mainly use an English SBERT model as teacher model $M$ and use XLM-RoBERTa (XLM-R) as student model $M'$</p>
</li>
</ul>
</li>
<li>
<p><strong>Conclusion :</strong></p>
<ul>
<li>
<p>presented a method to make monolingual sentence embeddings multilingual, with aligned vector spaces between the languages</p>
</li>
<li>
<p>We demonstrate the effectiveness of our approach for 50+ languages from various language families.</p>
</li>
<li>
<p>Code to extend sentence embeddings models to more than 400 languages is publicly available.</p>
</li>
</ul>
</li>
</ul>

    </div>
    <div class="article-footer">

    </div>
  </article>

</div><nav class="bar bar-footer clearfix" data-stick-bottom>
    <div class="bar-inner">
        <ul class="pager pull-right">
            
            
            
            
        </ul>
        <div class="bar-right">
        </div>
    </div>
</nav>


</main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
<ul class="social-links">
    <li><a href="https://github.com/koreanbear89" target="_blank" title="github" data-toggle=tooltip data-placement=top >
            <i class="icon icon-github"></i></a></li>
    <li><a href="https://www.linkedin.com/in/hanwoong-kim-95b5a7130/" target="_blank" title="linkedin" data-toggle=tooltip data-placement=top >
            <i class="icon icon-linkedin"></i></a></li>
    <li><a href="https://koreanbear89.github.io/index.xml" target="_blank" title="rss" data-toggle=tooltip data-placement=top >
            <i class="icon icon-rss"></i></a></li>
</ul>
  
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>
<script>
    window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/highlight.min.js"></script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/languages/python.min.js" defer></script>
<script type="text/javascript" src="https://cdn.staticfile.org/highlight.js/9.15.10/languages/javascript.min.js" defer></script><script>
    hljs.configure({
        tabReplace: '    ', 
        classPrefix: ''     
        
    })
    hljs.initHighlightingOnLoad();
</script>
<script src="https://koreanbear89.github.io/js/application.min.c181e6b0c036798c7731cfb85b41b44c80689fd48fee546b73d449386ce6ccfb.js"></script>
<script src="https://koreanbear89.github.io/js/plugin.min.46930345227de54c034f39c80841463c3b879632feb482e9f2734d4b616ae3be.js"></script>

<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            ROOT_URL: 'https:\/\/koreanbear89.github.io\/',
            CONTENT_URL: 'https:\/\/koreanbear89.github.io\/\/searchindex.json ',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script type="text/javascript" src="https://koreanbear89.github.io/js/insight.min.07c7d73e2ec5e960e55f8b98577ada0bd78b36e7ef9d6dd65b85d4b1d443e7c1a1ca7cd1d98e7105db5fd72583cbc6fbb5e386062d3bf3c2568588fbb38c0c06.js" defer></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
<script>
    tocbot.init({
        
        tocSelector: '.js-toc',
        
        contentSelector: '.js-toc-content',
        
        headingSelector: 'h1, h2, h3',
        
        hasInnerContainers: true,
    });
</script>



  </body>
</html>
