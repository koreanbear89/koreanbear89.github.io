<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2. Languages on Lab.Koreanbear|한국곰연구소</title>
    <link>https://koreanbear89.github.io/categories/2.-languages/</link>
    <description>Recent content in 2. Languages on Lab.Koreanbear|한국곰연구소</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 09 Jun 2021 09:00:13 +0000</lastBuildDate>
    
        <atom:link href="https://koreanbear89.github.io/categories/2.-languages/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Mathematics for ML #1 | Introduction Part.I </title>
      <link>https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml01-introduction/</link>
      <pubDate>Tue, 18 Jan 2022 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml01-introduction/</guid>
      <description>1. Introduction and Motivation Machine learning is about designing algorithms that automatically extract valuable information from data. There are three concepts that are at the core of machine learning : data, a model, and learning. Data : Since machine learning is inherently data driven, data is at the core of machine learning. Model : would describe a function that maps inputs to real-valued outputs. Learning : can be understood as a way to automatically find patterns and structure in data by optimizing the parameters of the model 1.</description>
    </item>
    
    <item>
      <title>MLCV #1 | Image Classification</title>
      <link>https://koreanbear89.github.io/research/3.-computer-vision/cv01-image-classification/</link>
      <pubDate>Sat, 02 Jul 2016 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/3.-computer-vision/cv01-image-classification/</guid>
      <description>Introduction Tasks: Image Classification : The task of classifying an image according to its visual content. Image Representation : focus on the way to encode visual contents into vectors (embedding, encoding) 1. AlexNet (2012) Introduction : CNNs have been prohibitively expensive to apply in large scale to high resolution images. Method : Training on Multiple GPUs def AlexNet(x): out = MP(relu(conv11x11(x))) out = MP(relu(conv5x5(out))) out = relu(conv3x3(out)) out = relu(conv3x3(out))</description>
    </item>
    
    <item>
      <title>NLP #2 | TextRepresentation</title>
      <link>https://koreanbear89.github.io/research/5.-natural-language/nlp-2-text-representation/</link>
      <pubDate>Wed, 13 Jan 2021 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/5.-natural-language/nlp-2-text-representation/</guid>
      <description>Summary Text Representation (Embedding) : When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to &amp;ldquo;vectorize&amp;rdquo; the text) before feeding it to the model. Methods Sparse Representation : One-hot encoding, Document Term Matrix, etc. Dense Representation : Word2Vec, Glove, FastText, etc. Pretraind Word Embedding : ELMo, GPT, BERT 1. Sparse Representations Introduction : Sparse Representation embeds word</description>
    </item>
    
    <item>
      <title>MLCV #2 | Object Detection</title>
      <link>https://koreanbear89.github.io/research/3.-computer-vision/cv02-object-detection/</link>
      <pubDate>Fri, 22 Jul 2016 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/3.-computer-vision/cv02-object-detection/</guid>
      <description>Introduction Tasks Object Detection : a task of finding the different objects in an image and classifying them Salient Object Detection : a task based on a visual attention mechanism, in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or RGB images. Metrics: AP or mAP is generally used as the primary metrics metric.click here for details Others Non Maximum Suppression</description>
    </item>
    
    <item>
      <title>NLP #3 | Language Modeling </title>
      <link>https://koreanbear89.github.io/research/5.-natural-language/nlp-3-language-modeling/</link>
      <pubDate>Mon, 13 Apr 2020 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/5.-natural-language/nlp-3-language-modeling/</guid>
      <description>Summary Language Modeling : is the task of predicting the next word or character in a document that can be used in downstream tasks like:
Machine Translation : By comparing two sentences with a language model, and return the more natural sentence
Spell Correction : Correct spelling by choosing a more natural vocabulary
Speech Recognition : Correct the recognition result with more natural words
1. Seq2Seq Learning with Neural Networks (2014) Introduction : DNNs work well but they cannot be used to map sequences to sequences.</description>
    </item>
    
    <item>
      <title>MLCV #3 | Semantic Segmentation</title>
      <link>https://koreanbear89.github.io/research/3.-computer-vision/cv03-image-segmentation/</link>
      <pubDate>Sun, 23 Jul 2017 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/3.-computer-vision/cv03-image-segmentation/</guid>
      <description>Introduction Tasks: Image Segmentation : The process of assigning a label to every pixel in the image. Semantic Segmentation : treats multiple objects of the same class as a single entity. Instance Segmentation : treats multiple objects of the same class as distinct individual objects. 1. FCN (2015) Introduction : The first end-to-end pixel-wise prediction model based only on convolutional layers. Method: Feature Extraction : using convolution layers like conventional</description>
    </item>
    
    <item>
      <title>MLCV #4 | Image Synthesis</title>
      <link>https://koreanbear89.github.io/research/3.-computer-vision/cv04-image-synthesis/</link>
      <pubDate>Tue, 25 Jul 2017 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/3.-computer-vision/cv04-image-synthesis/</guid>
      <description>0. Introduction Tasks : Image Synthesis : The task of creating new images from some form of image description. 1. GAN (2014) Introduction : A new framework for estimating generative models via an adversarial process
Method: simultaneously train two models : a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$.</description>
    </item>
    
    <item>
      <title>NLP #5 | Text Segmentation</title>
      <link>https://koreanbear89.github.io/research/5.-natural-language/nlp-5-text-segmentation/</link>
      <pubDate>Thu, 18 Nov 2021 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/5.-natural-language/nlp-5-text-segmentation/</guid>
      <description>Summary Text Segmentation : is the process of dividing written text into meaningful units, such as words, sentences, or topics.
To improve the results of Information Retrieval system and help users to find relevant passages faster keywords : Text Segmentation, Document Segmentation, Discourse Segmentation Methods
Lexical-Cohesion based methods : The first basic insight is that people talk about different topics in different ways: they use different words.
Discriminative approach : calculate similarity between neighboring sentences Hearst.</description>
    </item>
    
    <item>
      <title>MLCV #5 | Image Style Transfer</title>
      <link>https://koreanbear89.github.io/research/3.-computer-vision/cv05-image-style-transfer/</link>
      <pubDate>Thu, 26 Apr 2018 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/3.-computer-vision/cv05-image-style-transfer/</guid>
      <description>0. Introduction Tasks : Image Style Transfer : The task of migrating a style from one image (Style Image) to another (Content Image). 1. Image Style Transfer using CNNs (2016) Introduction : Introduce a algorithm that can separate and recombine the image content and style of natural images. Method : Extract feature maps $F_l$ from each input image $I_{content} $ and $I_{style}$ using pretrained networks at $l_{th}$ layer. Then, optimize</description>
    </item>
    
    <item>
      <title>Mathematics for ML #6 | Probabilirty and Distributions</title>
      <link>https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml06-probability-and-distributions/</link>
      <pubDate>Mon, 13 Jun 2022 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml06-probability-and-distributions/</guid>
      <description>6. Probability and Distributions Probability, loosely speaking, concerns the study of uncertainty Probability can be thought of as the fraction of times an event occurs, as a degree of belief about an event. We then would like to use this probability to measure the chance of something occurring in an experiment In ML, we often quantify uncertainty in the data, uncertainty in the machine learning model, and uncertainty in the predictions produced by the model.</description>
    </item>
    
    <item>
      <title>NLP #6 | Text Summarization</title>
      <link>https://koreanbear89.github.io/research/5.-natural-language/nlp-6-text-summarization/</link>
      <pubDate>Mon, 28 Jun 2021 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/5.-natural-language/nlp-6-text-summarization/</guid>
      <description>Summary Text Summarization : is a technique to shorten long texts such that the summary has all the important points of the actual document.
By Summarization Approache
Extraction-based Summarization: The extractive approach involves picking up the most important phrases and lines from the documents. It then combines all the important lines to create the summary. So, in this case, every line and word of the summary actually belongs to the original document which is summarized.</description>
    </item>
    
    <item>
      <title>MLCV #6 | Image Retrieval</title>
      <link>https://koreanbear89.github.io/research/3.-computer-vision/cv06-image-retrieval/</link>
      <pubDate>Thu, 14 Jun 2018 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/3.-computer-vision/cv06-image-retrieval/</guid>
      <description>Introduction Tasks: Image Retrieval : aims to find similar images to a query image among an image dataset. Tech Trend : Conventional Methods : relying on local descriptor matching (scale invariant features - local image descriptors - reranking with spatial verifications) using FC layers : after several conv layers as global descriptors [A Babenko et al, A Gordo et al.] using global pooling methods : from the activations of conv</description>
    </item>
    
    <item>
      <title>MLCV #7 | Action Classification</title>
      <link>https://koreanbear89.github.io/research/3.-computer-vision/cv07-video-classification/</link>
      <pubDate>Sat, 03 Nov 2018 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/3.-computer-vision/cv07-video-classification/</guid>
      <description>Introduction Tasks: Action Classification : The task classfying an action in video sequences according to its spatio-temporal content. Benchmark Set UCF-101 : is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. HMDB-51 Kinetics : has 400 human action classes with more than 400 examples for each class, each from a unique YouTube video. Methods CNN + RNNs 3D Convolutional Networks ResNeXt-101 :</description>
    </item>
    
    <item>
      <title>Mathematics for ML #8 | Introduction Part.II</title>
      <link>https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml08-when-models-meet-data/</link>
      <pubDate>Sat, 13 Aug 2022 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml08-when-models-meet-data/</guid>
      <description>8. When Models Meet Data In the first part of the book, we introduced the mathematics that form the foundations of many machine learning methods
The second part of the book introduces four pillars of machine learning:
Regression (Chapter 9) Dimensionality reduction (Chapter 10) Density estimation (Chapter 11) Classification (Chapter 12) 8.1 Data, Models, and Learning Three major components of a machine learning system: data, models, and learning. Good models : should perform well on unseen data.</description>
    </item>
    
    <item>
      <title>MLCV #8 | Pose Estimation</title>
      <link>https://koreanbear89.github.io/research/3.-computer-vision/cv08-pose-estimation/</link>
      <pubDate>Wed, 07 Aug 2019 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/3.-computer-vision/cv08-pose-estimation/</guid>
      <description>Introduction Tasks : Pose Estimation : The task aims to detect the locations of human anatomical keypoints (e.g., elbow, wrist, etc) 1. Deep Pose (2014) Introduction : The first major paper that applied Deep Learning to Human pose estimation Method : DNN-based regression : Alexnet backend (7 layers) with an extra final layer that outputs 2k joint coordinates (where $k$ is the number of joints). Cascade of pose regressors :</description>
    </item>
    
    <item>
      <title>Mathematics for ML #9 | Linear Regression</title>
      <link>https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml09-linear-regression/</link>
      <pubDate>Tue, 13 Sep 2022 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml09-linear-regression/</guid>
      <description>9. Linear Regression In the following, we will apply the mathematical concepts from previous chapters, to solve linear regression (curve fitting) problems.
In regression, we aim to find a function $f$ that maps inputs $x∈R^D$ to corresponding function values $f(x)∈R.$
We are given a set of training inputs $x_n$ and corresponding noisy observations $y_n=f(x_n) + \epsilon$,
where $\epsilon$ is an i.i.d random variable that describes measurement and observation noise =&amp;gt; simply zero-mean Gaussian noise (not further in this chapter) Then the task is to infer the function $f$ that generated the data and generalizes well to function values at new input locations.</description>
    </item>
    
    <item>
      <title>MLCV #9 | 3D Object Detection</title>
      <link>https://koreanbear89.github.io/research/3.-computer-vision/cv09-3d-object-detection/</link>
      <pubDate>Tue, 06 Oct 2020 09:00:13 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/research/3.-computer-vision/cv09-3d-object-detection/</guid>
      <description>Introduction Tasks : 3D object detection : classifies the object category and estimates oriented 3D bounding boxes of physical objects from 3D sensor data. Applications : By extending prediction to 3D, one can capture an object’s size, position and orientation in the world, leading to a variety of applications in robotics, self-driving vehicles, image retrieval, and augmented reality Benchmarks KITTI (car, cyclist, pedestrian</description>
    </item>
    
    <item>
      <title>Mathematcs for ML #10 | Dimensionality Reduction with PCA</title>
      <link>https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml10-dimensionality-reduction-and-pca/</link>
      <pubDate>Thu, 13 Oct 2022 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/mathematics/3.-mathematics-for-ml/mml10-dimensionality-reduction-and-pca/</guid>
      <description>10. Dimensionality Reduction with PCA Introduction : Working directly with high-dimensional data comes with some difficulties It is hard to analyze, interpretation is difficult, visualization is nearly impossible, and storage of the data vectors can be expensive In this chapter, we derive PCA from first principles, drawing on our understanding of basis and basis change (Sections 2.6.1 and 2.7.2), projections (Section 3.8), eigenvalues (Section 4.2), Gaussian distributions (Section 6.5), and constrained optimization (Section 7.</description>
    </item>
    
    <item>
      <title>Effective Python | #5 Classes and Interfaces</title>
      <link>https://koreanbear89.github.io/engineering/9.-study/effective-python-05/</link>
      <pubDate>Mon, 13 Mar 2023 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/engineering/9.-study/effective-python-05/</guid>
      <description>Introduction As an oop language, Python supports a full range of features, such as inheritance, polymorphism, and encapsulation
Python’s classes and inheritance make it easy to express a program’s intended behaviors with objects.
They allow you to improve and expand functionality over time.
They provide flexibility in an environment of changing requirements.
Item 37. Compose Classes Instead of Nesting Many Levels of Build-in Types Avoid making dictionaries with values that are dictionaries, long tuples, or complex nestings of other build-in types.</description>
    </item>
    
    <item>
      <title>Effective Python | #4 Comprehensions and Generators</title>
      <link>https://koreanbear89.github.io/engineering/9.-study/effective-python-04/</link>
      <pubDate>Thu, 16 Feb 2023 09:00:00 +0000</pubDate>
      
      <guid>https://koreanbear89.github.io/engineering/9.-study/effective-python-04/</guid>
      <description>Introduction Python provides a special syntax, called comprehensions, for succinctly iterating through these types (list, dict, set) and creating derivative data structures
This style of processing is extended to functions with generators, which enable a stream of values to be incrementally returned by a function.
Item 27. Use Comprehensions Instead of map and filter Comprehensions support multiple levels of loops and multiple conditions per loop level.
Comprehensions with more than two control subexpressions are very difficult to read and should be avoided.</description>
    </item>
    
  </channel>
</rss>
